{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Pricing Optimization with TD3\n",
    "### An Interactive Reinforcement Learning Solution\n",
    "*Clear objective and visual hierarchy.* This notebook uses the TD3 algorithm to find optimal pricing strategies, presented in an interactive format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "*Collapsible configuration section with interactive display.* Configure paths, hyperparameters, and system settings here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 CSS Styling\n",
    " *Adds some basic styling for visual elements.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    " <style>\n",
    " .config-box {\n",
    "     border: 1px solid #e0e0e0;\n",
    "     border-left: 5px solid #007bff;\n",
    "     border-radius: 4px;\n",
    "     padding: 15px;\n",
    "     margin: 15px 0;\n",
    "     background-color: #f8f9fa;\n",
    "     font-family: monospace;\n",
    " }\n",
    " .config-box h4 {\n",
    "     margin-top: 0;\n",
    "     color: #0056b3;\n",
    "     border-bottom: 1px solid #ccc;\n",
    "     padding-bottom: 5px;\n",
    " }\n",
    " .output-box {\n",
    "     border: 1px solid #ccc;\n",
    "     border-radius: 4px;\n",
    "     padding: 10px;\n",
    "     margin: 10px 0;\n",
    "     background-color: #fff;\n",
    " }\n",
    " </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Core Parameters Definition\n",
    "*Define the `Config` dataclass holding all parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the Airline Pricing TD3 experiment\"\"\"\n",
    "    # --- Data paths ---\n",
    "    data_paths: Dict[str, str] = field(default_factory=lambda: {\n",
    "        'historical_data': \"./data/cleaned_dataset.csv\",\n",
    "        'fuel_data': \"./data/petroluem_prod_consumption.csv\", \n",
    "        'aircraft_specs': \"./data/airplane_prices_dataset.csv\",\n",
    "        'weather_data_detailed': \"./data/weather_data.csv\"\n",
    "    })\n",
    "    output_dir: str = \"./output_td3_interactive\" # Separate output dir\n",
    "\n",
    "    # --- Experiment parameters ---\n",
    "    seed: int = 42\n",
    "    n_episodes: int = 200\n",
    "    batch_size: int = 100\n",
    "    buffer_size: int = 100000\n",
    "\n",
    "    # --- TD3 Specific Hyperparameters ---\n",
    "    actor_lr: float = 1e-4\n",
    "    critic_lr: float = 1e-3\n",
    "    gamma: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    policy_noise: float = 0.2\n",
    "    noise_clip: float = 0.5\n",
    "    policy_freq: int = 2\n",
    "    exploration_noise: float = 0.1\n",
    "\n",
    "    # --- Environment parameters ---\n",
    "    seats_capacity: int = 150\n",
    "    simulation_length_days: int = 90\n",
    "    price_range: Tuple[float, float] = (1000.0, 50000.0) # Placeholder\n",
    "\n",
    "    # --- Agent Network Architecture ---\n",
    "    actor_hidden_dims: list = field(default_factory=lambda: [256, 128])\n",
    "    critic_hidden_dims: list = field(default_factory=lambda: [256, 128])\n",
    "\n",
    "    # --- Training Control ---\n",
    "    start_timesteps: int = 5000 # Random exploration steps\n",
    "    validation_freq: int = 20\n",
    "    patience: int = 15\n",
    "    use_gpu: bool = True\n",
    "    max_action_value: float = 1.0 # Corresponds to Tanh output [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Instantiate & Display Configuration\n",
    " *Interactive configuration display.* Creates the config object and shows its current values in a formatted box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pprint # For nicely formatting the config display\n",
    "\n",
    "config = Config() # Create the actual config instance\n",
    "\n",
    "# Create HTML representation\n",
    "config_html = (\n",
    "    f'<div class=\"config-box\">'\n",
    "    f'<h4>Active Configuration:</h4>'\n",
    "    f'<pre>{pprint.pformat(config.__dict__, indent=2)}</pre>'\n",
    "    f'</div>'\n",
    ")\n",
    "display(HTML(config_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Import Libraries & System Setup\n",
    "*Import necessary libraries and initialize seeds, device, logging.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "from collections import deque\n",
    "import random\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter as writer\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, Dropdown, Button, FloatSlider, IntSlider, Layout, VBox, HBox, FloatLogSlider\n",
    "import holidays\n",
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "# Filter simple warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# --- Plot Style ---\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (10, 5) # Slightly smaller default plots\n",
    "\n",
    "# --- Random Seeds ---\n",
    "def set_seeds(seed_value: int):\n",
    "    random.seed(seed_value); np.random.seed(seed_value); torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed_value)\n",
    "set_seeds(config.seed)\n",
    "logging.info(f\"Seeds set to {config.seed}\")\n",
    "\n",
    "# --- Device Configuration ---\n",
    "if config.use_gpu and torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "else: device = torch.device(\"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# --- Output Directory ---\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "config.model_path_actor = os.path.join(config.output_dir, \"td3_actor_best.pth\")\n",
    "config.model_path_critic = os.path.join(config.output_dir, \"td3_critic_best.pth\")\n",
    "logging.info(f\"Output Dir: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline\n",
    "---\n",
    "*Interactive data exploration.* Load, process, and interactively explore the flight data.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 DataProcessor Class\n",
    " *Handles data loading, cleaning, merging, feature engineering, and scaling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Handles the loading, preprocessing, feature engineering, and scaling\n",
    "    of various data sources for the airline pricing environment.\n",
    "\n",
    "    Key Steps:\n",
    "    1. Loads raw data (historical flights, aircraft specs, weather).\n",
    "    2. Fetches holidays dynamically.\n",
    "    3. Merges external data onto the historical flight data.\n",
    "    4. Engineers time-based and statistical features.\n",
    "    5. Handles missing values using defined strategies.\n",
    "    6. Fits encoders and scalers during the `fit` phase using training data.\n",
    "    7. Applies learned transformations during the `transform` phase.\n",
    "    8. Provides utilities for price scaling.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor with configuration and sets up scalers/encoders.\n",
    "\n",
    "        Args:\n",
    "            config: A dataclass or object containing configuration parameters,\n",
    "                    including data paths and processing settings.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.paths = config.data_paths # Dictionary of data file paths\n",
    "\n",
    "        # --- Scalers ---\n",
    "        # Initialize scalers for numerical features. They will be fitted later.\n",
    "        # Price (Fare) Scaler: Maps fares to [0, 1] range, matching Actor's Tanh output after shifting.\n",
    "        self.price_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # Days Left Scaler: Maps days left to [0, 1] range.\n",
    "        self.days_left_scaler = MinMaxScaler()\n",
    "        # Standard Scalers: Standardize features to have mean 0 and std dev 1.\n",
    "        self.duration_scaler = StandardScaler()       # For 'Duration_in_hours'\n",
    "        self.temp_scaler = StandardScaler()           # For 'Weather_Temp_C'\n",
    "        self.humidity_scaler = StandardScaler()       # For 'Weather_Humidity'\n",
    "        self.age_scaler = StandardScaler()            # For 'Aircraft_Age'\n",
    "        self.fuel_eff_scaler = StandardScaler()       # For 'Fuel Consumption (L/hour)'\n",
    "        self.fare_ma7_scaler = StandardScaler()       # For 'Price_MA7' (engineered feature)\n",
    "        self.fare_lag1_scaler = StandardScaler()      # For 'Price_Lag1' (engineered feature)\n",
    "\n",
    "        # --- Encoders ---\n",
    "        # Initialize encoders for categorical features.\n",
    "        # Label Encoders: Assign a unique integer to each category.\n",
    "        self.route_encoder = LabelEncoder()           # For 'Route' (e.g., 'Delhi-Mumbai')\n",
    "        self.airline_encoder = LabelEncoder()         # For 'Airline' name\n",
    "        self.class_encoder = LabelEncoder()           # For 'Class' (e.g., 'Economy')\n",
    "        self.source_encoder = LabelEncoder()          # For 'Source' city\n",
    "        self.destination_encoder = LabelEncoder()     # For 'Destination' city\n",
    "        self.aircraft_model_encoder = LabelEncoder()  # For 'AircraftType' / 'Model'\n",
    "        self.weather_cond_encoder = LabelEncoder()    # For 'Weather_Condition' text\n",
    "        self.season_encoder = LabelEncoder()          # For generated 'Season'\n",
    "\n",
    "        # Ordinal Encoders: Assign integers based on a predefined order.\n",
    "        # Handle unknown values by mapping them to -1.\n",
    "        self.stops_encoder = OrdinalEncoder(\n",
    "            categories=[['non-stop', '1-stop', '2-stops', '3-stops', '4-stops']], # Explicit order\n",
    "            handle_unknown='use_encoded_value', unknown_value=-1\n",
    "        )\n",
    "        self.journey_day_encoder = OrdinalEncoder(\n",
    "            categories=[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']],\n",
    "            handle_unknown='use_encoded_value', unknown_value=-1\n",
    "        )\n",
    "        self.dep_time_encoder = OrdinalEncoder(\n",
    "            categories=[['Before 6 AM', '6 AM - 12 PM', '12 PM - 6 PM', 'After 6 PM']],\n",
    "            handle_unknown='use_encoded_value', unknown_value=-1\n",
    "        )\n",
    "        self.arr_time_encoder = OrdinalEncoder(\n",
    "             categories=[['Before 6 AM', '6 AM - 12 PM', '12 PM - 6 PM', 'After 6 PM']],\n",
    "             handle_unknown='use_encoded_value', unknown_value=-1\n",
    "        )\n",
    "\n",
    "        # --- Fitted Parameters & State ---\n",
    "        # These store values learned during the 'fit' phase or other necessary info.\n",
    "        self.fuel_price_mean = 2.5  # Fallback value as real price data is unavailable\n",
    "        self.historical_fare_stats: Dict[Tuple[str, str], Dict[str, float]] = {} # Stores {'Route-Airline': {'mean': X, 'median': Y}}\n",
    "        self.global_fare_mean: float = 0.0 # Fallback historical mean fare across all data\n",
    "        self.is_fitted: bool = False # Flag to track if `fit` has been called\n",
    "\n",
    "        # --- Data for Environment ---\n",
    "        # Store mappings needed by the simulation environment (e.g., for cost calculation).\n",
    "        self.aircraft_fuel_rates: Dict[str, float] = {} # Maps Aircraft Model -> Fuel Consumption Rate\n",
    "        self.maintenance_costs: Dict[str, float] = {} # Maps Aircraft Model -> Hourly Maintenance Cost\n",
    "\n",
    "    def _load_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads dataframes specified in the config's data_paths.\n",
    "        Handles file not found errors for optional files.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary where keys are data names (e.g., 'historical_data')\n",
    "            and values are the loaded pandas DataFrames.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        logging.info(\"Loading data files specified in configuration...\")\n",
    "        for name, path in self.paths.items():\n",
    "            # Skip holiday data path if present, as it's fetched dynamically\n",
    "            if name == 'holiday_data':\n",
    "                continue\n",
    "            try:\n",
    "                # Check if the file exists\n",
    "                if os.path.exists(path):\n",
    "                    data[name] = pd.read_csv(path)\n",
    "                    logging.info(f\"  Successfully loaded '{name}' from '{path}' (Shape: {data[name].shape})\")\n",
    "                else:\n",
    "                    # Historical data is essential, raise error if missing\n",
    "                    if name == 'historical_data':\n",
    "                         logging.error(f\"CRITICAL ERROR: Required historical data file not found at {path}.\")\n",
    "                         raise FileNotFoundError(f\"Required file missing: {path}\")\n",
    "                    # Other files might be optional\n",
    "                    else:\n",
    "                         logging.warning(f\"Optional file not found: {path}. Skipping '{name}'.\")\n",
    "                         data[name] = pd.DataFrame() # Assign empty DataFrame as placeholder\n",
    "            except Exception as e:\n",
    "                # Catch other potential errors during loading (e.g., parsing errors)\n",
    "                logging.error(f\"Error loading '{name}' from {path}: {e}\")\n",
    "                data[name] = pd.DataFrame() # Assign empty DataFrame on error\n",
    "        return data\n",
    "\n",
    "    def _fetch_holidays(self, years: list[int]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetches Indian national holidays for the specified year range using the `holidays` library.\n",
    "\n",
    "        Args:\n",
    "            years: A list of years for which to fetch holidays.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with 'Date' and 'HolidayName' columns,\n",
    "            or an empty DataFrame if fetching fails or no holidays are found.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Fetching Indian national holidays for years: {years}...\")\n",
    "        try:\n",
    "            # Use the holidays library for India ('IN')\n",
    "            in_holidays = holidays.country_holidays('IN', years=years)\n",
    "            # Check if the result is empty\n",
    "            if not in_holidays:\n",
    "                logging.warning(f\"No holidays found for years {years}.\")\n",
    "                return pd.DataFrame({'Date': pd.to_datetime([]), 'HolidayName': []}) # Ensure correct dtypes\n",
    "\n",
    "            # Convert the dictionary of holidays to a list of dicts\n",
    "            holiday_list = [{'Date': date, 'HolidayName': name}\n",
    "                            for date, name in sorted(in_holidays.items())]\n",
    "            # Create DataFrame and ensure Date column is datetime\n",
    "            holiday_df = pd.DataFrame(holiday_list)\n",
    "            holiday_df['Date'] = pd.to_datetime(holiday_df['Date'])\n",
    "            logging.info(f\"  Successfully fetched {len(holiday_df)} holiday entries.\")\n",
    "            return holiday_df\n",
    "        except Exception as e:\n",
    "            # Handle potential errors during the fetch process\n",
    "            logging.error(f\"Error fetching holidays using 'holidays' library: {e}\")\n",
    "            return pd.DataFrame({'Date': pd.to_datetime([]), 'HolidayName': []}) # Return empty DF\n",
    "\n",
    "    def fit(self, data: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Fits the encoders, scalers, and calculates imputation values using the provided training data.\n",
    "        This method should only be called once on the training dataset.\n",
    "\n",
    "        Args:\n",
    "            data: A dictionary containing the raw dataframes loaded by `_load_data`.\n",
    "        \"\"\"\n",
    "        logging.info(\"Starting DataProcessor fitting phase (learning transformations)...\")\n",
    "\n",
    "        # --- Get Primary Data ---\n",
    "        hist_data = data.get('historical_data') # Expects 'cleaned_dataset.csv'\n",
    "        aircraft_specs = data.get('aircraft_specs', pd.DataFrame())\n",
    "        weather_data = data.get('weather_data_detailed', pd.DataFrame())\n",
    "\n",
    "        if hist_data is None or hist_data.empty:\n",
    "            raise ValueError(\"Cannot fit: Historical data ('cleaned_dataset.csv') is missing or empty.\")\n",
    "\n",
    "        # --- Initial Cleaning & Preparation ---\n",
    "        logging.info(\"Performing initial cleaning and preparation...\")\n",
    "        # Rename 'Fare' to 'Price' for internal consistency\n",
    "        hist_data.rename(columns={'Fare': 'Price'}, inplace=True, errors='ignore')\n",
    "        if 'Price' not in hist_data.columns: raise ValueError(\"Missing 'Price' (originally 'Fare') column.\")\n",
    "        # Convert 'Date' column to datetime objects, coercing errors to NaT\n",
    "        hist_data['Date'] = pd.to_datetime(hist_data['Date'], errors='coerce')\n",
    "        # Drop rows where essential columns (Date, Source, Dest, Price) are missing\n",
    "        initial_rows = len(hist_data)\n",
    "        hist_data.dropna(subset=['Date', 'Source', 'Destination', 'Price'], inplace=True)\n",
    "        if initial_rows > len(hist_data): logging.info(f\"  Dropped {initial_rows - len(hist_data)} rows with missing essential values.\")\n",
    "        # Create the 'Route' feature by combining Source and Destination\n",
    "        hist_data['Route'] = hist_data['Source'] + '-' + hist_data['Destination']\n",
    "\n",
    "        # Prepare weather data dates\n",
    "        if not weather_data.empty and 'datetime' in weather_data.columns:\n",
    "            weather_data['datetime'] = pd.to_datetime(weather_data['datetime'], errors='coerce')\n",
    "            weather_data.dropna(subset=['datetime'], inplace=True)\n",
    "        else:\n",
    "             weather_data = pd.DataFrame() # Ensure empty DF if unusable\n",
    "\n",
    "        # --- Fetch Holidays ---\n",
    "        # Determine year range from historical data to fetch relevant holidays\n",
    "        min_year = hist_data['Date'].dt.year.min()\n",
    "        max_year = hist_data['Date'].dt.year.max()\n",
    "        holiday_df = self._fetch_holidays(years=list(range(min_year, max_year + 1)))\n",
    "\n",
    "        # --- Prepare Data for Fitting Transformations ---\n",
    "        # Create a copy to avoid modifying the original raw data\n",
    "        fit_data = hist_data.copy()\n",
    "        # Merge external sources (holidays, aircraft, weather)\n",
    "        fit_data = self._merge_external_data(fit_data, holiday_df, aircraft_specs, weather_data)\n",
    "        # Engineer features (like Season, Price MA/Lag) that might be needed for scaling/encoding\n",
    "        fit_data = self._engineer_features(fit_data)\n",
    "        # Handle missing values *before* fitting encoders/scalers\n",
    "        fit_data = self._handle_missing_values(fit_data, is_fitting=True) # is_fitting=True calculates imputation values\n",
    "\n",
    "        # --- Calculate Historical Fare Statistics ---\n",
    "        # Compute mean/median fare per route/airline for later use (e.g., demand heuristic)\n",
    "        logging.info(\"Calculating historical fare statistics per route/airline...\")\n",
    "        try:\n",
    "            self.historical_fare_stats = fit_data.groupby(['Route', 'Airline'])['Price'].agg(['mean', 'median']).to_dict('index')\n",
    "            self.global_fare_mean = fit_data['Price'].mean() # Calculate global fallback mean\n",
    "            logging.info(f\"  Calculated stats for {len(self.historical_fare_stats)} route/airline pairs. Global mean fare: {self.global_fare_mean:.2f}\")\n",
    "        except KeyError:\n",
    "             logging.error(\"  Could not group by 'Route' or 'Airline' - columns might be missing after merge/impute.\")\n",
    "             self.global_fare_mean = fit_data['Price'].mean() if 'Price' in fit_data.columns else 0.0\n",
    "\n",
    "\n",
    "        # --- Fit Encoders ---\n",
    "        # Learn the mapping from categories to integers for all categorical features\n",
    "        self._fit_categorical_encoders(fit_data) # Pass the prepared data\n",
    "\n",
    "        # --- Fit Scalers ---\n",
    "        # Learn the scaling parameters (min/max or mean/std) for all numerical features\n",
    "        self._fit_numerical_scalers(fit_data) # Pass the prepared data\n",
    "\n",
    "        # Set the flag indicating fitting is complete\n",
    "        self.is_fitted = True\n",
    "        logging.info(\"DataProcessor fitting phase completed successfully.\")\n",
    "        return self # Return self for potential chaining\n",
    "\n",
    "    def transform(self, data: Dict[str, pd.DataFrame], is_validation=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Applies the learned transformations (imputation, encoding, scaling)\n",
    "        to the provided data.\n",
    "\n",
    "        Args:\n",
    "            data: A dictionary containing the raw dataframes (similar structure to input for `fit`).\n",
    "            is_validation: Flag indicating if this is for validation/test data (currently unused, but good practice).\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with all preprocessing steps applied.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"DataProcessor must be fitted using .fit() before calling .transform().\")\n",
    "        logging.info(f\"Starting DataProcessor transformation phase (Validation Mode: {is_validation})...\")\n",
    "\n",
    "        # --- Get Primary Data ---\n",
    "        hist_data = data.get('historical_data')\n",
    "        aircraft_specs = data.get('aircraft_specs', pd.DataFrame())\n",
    "        weather_data = data.get('weather_data_detailed', pd.DataFrame())\n",
    "\n",
    "        if hist_data is None or hist_data.empty:\n",
    "             logging.error(\"Cannot transform: Historical data is missing or empty.\")\n",
    "             return pd.DataFrame()\n",
    "\n",
    "        # --- Initial Cleaning & Preparation ---\n",
    "        # Perform the same initial steps as in `fit` to ensure consistency\n",
    "        df = hist_data.copy()\n",
    "        df.rename(columns={'Fare': 'Price'}, inplace=True, errors='ignore')\n",
    "        if 'Price' not in df.columns: raise ValueError(\"Missing 'Price' (originally 'Fare') column.\")\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df.dropna(subset=['Date', 'Source', 'Destination', 'Price'], inplace=True)\n",
    "        df['Route'] = df['Source'] + '-' + df['Destination']\n",
    "\n",
    "        # Prepare weather data dates\n",
    "        if not weather_data.empty and 'datetime' in weather_data.columns:\n",
    "            weather_data['datetime'] = pd.to_datetime(weather_data['datetime'], errors='coerce')\n",
    "            weather_data.dropna(subset=['datetime'], inplace=True)\n",
    "        else: weather_data = pd.DataFrame()\n",
    "\n",
    "        # --- Fetch Holidays (for the date range in *this* data chunk) ---\n",
    "        if not df.empty:\n",
    "             min_year = df['Date'].dt.year.min()\n",
    "             max_year = df['Date'].dt.year.max()\n",
    "             holiday_df = self._fetch_holidays(years=list(range(min_year, max_year + 1)))\n",
    "        else: holiday_df = pd.DataFrame() # Handle empty input df\n",
    "\n",
    "\n",
    "        # --- Apply Transformations in Order ---\n",
    "        # 1. Merge external data\n",
    "        df = self._merge_external_data(df, holiday_df, aircraft_specs, weather_data)\n",
    "        # 2. Engineer features\n",
    "        df = self._engineer_features(df)\n",
    "        # 3. Handle missing values (using stored imputation values from `fit`)\n",
    "        df = self._handle_missing_values(df, is_fitting=False)\n",
    "        # 4. Apply learned encoders\n",
    "        df = self._apply_encoders(df)\n",
    "        # 5. Apply learned scalers\n",
    "        df = self._apply_scalers(df)\n",
    "\n",
    "        logging.info(\"Data transformation phase completed.\")\n",
    "\n",
    "        # Final check for any remaining NaNs after all steps\n",
    "        if df.isnull().any().any():\n",
    "            logging.warning(f\"NaNs detected in final transformed data. Review imputation steps.\")\n",
    "            # Consider dropping or force-filling remaining NaNs based on strategy\n",
    "            # df.fillna(0, inplace=True) # Example: Force fill\n",
    "\n",
    "        return df.reset_index(drop=True) # Return with clean index\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Internal Helper Methods (_merge, _engineer, _handle_missing, _fit*, _apply*)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def _merge_external_data(self, base_df: pd.DataFrame, holiday_df: pd.DataFrame,\n",
    "                             aircraft_df: pd.DataFrame, weather_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Merges holiday, aircraft specs, and weather data onto the base flight data.\"\"\"\n",
    "        logging.debug(\"Merging external data sources...\")\n",
    "        df = base_df.copy()\n",
    "        # Use normalized date (date part only) for merging day-level data\n",
    "        df['Date_Norm'] = df['Date'].dt.normalize()\n",
    "\n",
    "        # --- Merge Holidays ---\n",
    "        if not holiday_df.empty:\n",
    "            holiday_df['Date_Norm'] = holiday_df['Date'].dt.normalize()\n",
    "            # Merge based on date only for national holidays\n",
    "            df = pd.merge(df, holiday_df[['Date_Norm', 'HolidayName']], on='Date_Norm', how='left')\n",
    "            df['IsHoliday'] = df['HolidayName'].notna().astype(int)\n",
    "            df.drop('HolidayName', axis=1, inplace=True, errors='ignore')\n",
    "            logging.debug(f\"  Merged holiday data. Found {df['IsHoliday'].sum()} holiday flags.\")\n",
    "        else:\n",
    "            df['IsHoliday'] = 0 # Default if no holiday data\n",
    "\n",
    "        # --- Merge Aircraft Specs ---\n",
    "        # Assumption: 'AircraftType' in base_df maps to 'Model' in aircraft_df\n",
    "        key_hist = 'AircraftType'\n",
    "        key_spec = 'Model'\n",
    "        # Add placeholder columns first to ensure they exist even if merge fails\n",
    "        df['Age'] = np.nan\n",
    "        df['Fuel Consumption (L/hour)'] = np.nan\n",
    "        df['Hourly Maintenance Cost ($)'] = np.nan\n",
    "\n",
    "        if not aircraft_df.empty and key_spec in aircraft_df.columns:\n",
    "            # Pre-process specs (only needs to happen once conceptually, but safe to repeat)\n",
    "            if 'Production Year' in aircraft_df.columns:\n",
    "                 current_year = datetime.now().year\n",
    "                 # Ensure Production Year is numeric before subtracting\n",
    "                 prod_year_num = pd.to_numeric(aircraft_df['Production Year'], errors='coerce')\n",
    "                 aircraft_df['Age'] = current_year - prod_year_num\n",
    "                 # Handle potential future dates or errors, ensure non-negative age\n",
    "                 aircraft_df['Age'] = aircraft_df['Age'].apply(lambda x: max(0, x) if pd.notnull(x) else np.nan)\n",
    "            # Store mappings if this is the fitting phase (based on is_fitted flag)\n",
    "            # (Mappings are now stored in self.aircraft_fuel_rates etc directly in __init__ or fit)\n",
    "\n",
    "            # Perform merge only if the key exists in the historical data\n",
    "            if key_hist in df.columns:\n",
    "                 spec_cols_to_merge = [key_spec, 'Age', 'Fuel Consumption (L/hour)', 'Hourly Maintenance Cost ($)']\n",
    "                 # Select only columns that actually exist in aircraft_df\n",
    "                 spec_cols_exist = [col for col in spec_cols_to_merge if col in aircraft_df.columns]\n",
    "\n",
    "                 # Perform the merge safely, handling potential duplicate columns if keys are same\n",
    "                 if key_hist == key_spec:\n",
    "                     # Merge and drop duplicate key column from right DF if names are identical\n",
    "                     df_merged = pd.merge(df, aircraft_df[spec_cols_exist], on=key_hist, how='left', suffixes=('', '_spec'))\n",
    "                 else:\n",
    "                      df_merged = pd.merge(df, aircraft_df[spec_cols_exist], left_on=key_hist, right_on=key_spec, how='left', suffixes=('', '_spec'))\n",
    "                      # Drop the key from the right dataframe if names differ\n",
    "                      if key_spec in df_merged.columns: df_merged.drop(key_spec, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "                 # Overwrite original columns with merged data where available\n",
    "                 for col in ['Age', 'Fuel Consumption (L/hour)', 'Hourly Maintenance Cost ($)']:\n",
    "                      spec_col = col + '_spec'\n",
    "                      if spec_col in df_merged.columns:\n",
    "                          # Update original column only where spec data is not NaN\n",
    "                          df[col] = df_merged[spec_col].combine_first(df[col])\n",
    "                          # Drop the temporary spec column\n",
    "                          # df.drop(spec_col, axis=1, inplace=True) # Removed, handled later if needed\n",
    "\n",
    "                 logging.debug(f\"  Merged aircraft specs using '{key_hist}' -> '{key_spec}'.\")\n",
    "            else:\n",
    "                 logging.warning(f\"  Cannot merge aircraft specs: Key '{key_hist}' not found in base data.\")\n",
    "        else:\n",
    "            logging.warning(\"  Skipping aircraft specs merge: Specs data missing, empty, or lacks '{key_spec}' column.\")\n",
    "\n",
    "\n",
    "        # --- Merge Weather Data ---\n",
    "        origin_col = 'Source' # City column in flight data to match weather data\n",
    "        # Add placeholder columns first\n",
    "        df['Weather_Temp_C'] = np.nan\n",
    "        df['Weather_Humidity'] = np.nan\n",
    "        df['Weather_Condition'] = 'Unknown'\n",
    "\n",
    "        if not weather_df.empty and all(c in weather_df.columns for c in ['city', 'datetime', 'temp_c', 'humidity', 'condition']) and origin_col in df.columns:\n",
    "            # Aggregate weather to daily level (mean temp/humidity, mode condition)\n",
    "            weather_daily = weather_df.groupby(\n",
    "                ['city', pd.Grouper(key='datetime', freq='D')]\n",
    "            ).agg(\n",
    "                Weather_Temp_C=('temp_c', 'mean'),\n",
    "                Weather_Humidity=('humidity', 'mean'),\n",
    "                Weather_Condition=('condition', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown') # Handle empty modes\n",
    "            ).reset_index()\n",
    "            # Rename columns to match merge keys\n",
    "            weather_daily = weather_daily.rename(columns={'city': origin_col, 'datetime': 'Date_Norm'})\n",
    "            weather_daily['Date_Norm'] = weather_daily['Date_Norm'].dt.normalize() # Ensure date part only\n",
    "\n",
    "            # Perform the merge\n",
    "            df = pd.merge(df, weather_daily, on=['Date_Norm', origin_col], how='left', suffixes=('', '_weather'))\n",
    "\n",
    "             # Overwrite placeholder columns with merged weather data\n",
    "            if 'Weather_Temp_C_weather' in df.columns:\n",
    "                df['Weather_Temp_C'] = df['Weather_Temp_C_weather'].combine_first(df['Weather_Temp_C'])\n",
    "                df.drop('Weather_Temp_C_weather', axis=1, inplace=True)\n",
    "            if 'Weather_Humidity_weather' in df.columns:\n",
    "                df['Weather_Humidity'] = df['Weather_Humidity_weather'].combine_first(df['Weather_Humidity'])\n",
    "                df.drop('Weather_Humidity_weather', axis=1, inplace=True)\n",
    "            if 'Weather_Condition_weather' in df.columns:\n",
    "                # Fill NaNs in original with merged, then fill remaining NaNs with 'Unknown'\n",
    "                df['Weather_Condition'] = df['Weather_Condition_weather'].combine_first(df['Weather_Condition']).fillna('Unknown')\n",
    "                df.drop('Weather_Condition_weather', axis=1, inplace=True)\n",
    "\n",
    "            logging.debug(f\"  Merged daily aggregated weather data on '{origin_col}'.\")\n",
    "        else:\n",
    "            if not weather_df.empty: logging.warning(\"  Skipping weather merge: Required columns missing or base df lacks '{origin_col}'.\")\n",
    "\n",
    "\n",
    "        # --- Add Fallback Fuel Price ---\n",
    "        # This adds a constant value as real fuel price data was not usable/available\n",
    "        df['FuelPrice'] = self.fuel_price_mean\n",
    "\n",
    "        # Clean up helper columns\n",
    "        df.drop(['Date_Norm'], axis=1, inplace=True, errors='ignore')\n",
    "        # Drop any other temporary merge columns (like '_spec') if they still exist\n",
    "        df.drop([col for col in df.columns if '_spec' in col], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _get_season(self, month: int) -> str:\n",
    "        \"\"\"Maps month number to meteorological season (Northern Hemisphere).\"\"\"\n",
    "        if month in [12, 1, 2]: return 'Winter'\n",
    "        if month in [3, 4, 5]: return 'Spring'\n",
    "        if month in [6, 7, 8]: return 'Summer'\n",
    "        if month in [9, 10, 11]: return 'Fall'\n",
    "        return 'Unknown' # Handle potential NaNs or invalid months\n",
    "\n",
    "    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates time-based features and calculates rolling/lag features.\"\"\"\n",
    "        logging.debug(\"Engineering time-based features (DayOfWeek, Season, etc.)...\")\n",
    "        # --- Time Features ---\n",
    "        df['DayOfWeek'] = df['Date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int) # ISO week number\n",
    "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "        df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int) # Flag for Saturday/Sunday\n",
    "        df['Season'] = df['Month'].apply(self._get_season) # Map month to season\n",
    "\n",
    "        # --- Rolling Average / Lag Features ---\n",
    "        # Requires data to be sorted correctly for meaningful results\n",
    "        logging.debug(\"Calculating rolling average (MA7) and lag (Lag1) for Price...\")\n",
    "        df = df.sort_values(['Route', 'Airline', 'Date'], ascending=True) # Ensure correct order within groups\n",
    "\n",
    "        if 'Price' in df.columns:\n",
    "            # Calculate 7-day rolling mean price per Route-Airline group\n",
    "            df['Price_MA7'] = df.groupby(['Route', 'Airline'])['Price'].transform(\n",
    "                lambda x: x.rolling(window=7, min_periods=1).mean() # Need at least 1 period\n",
    "            )\n",
    "            # Calculate price from the previous day for the same Route-Airline\n",
    "            df['Price_Lag1'] = df.groupby(['Route', 'Airline'])['Price'].transform(\n",
    "                lambda x: x.shift(1) # Shift by 1 day\n",
    "            )\n",
    "        else:\n",
    "            # Add NaN columns if Price is missing, though Price is essential\n",
    "            logging.warning(\"  'Price' column not found for MA/Lag calculation.\")\n",
    "            df['Price_MA7'] = np.nan\n",
    "            df['Price_Lag1'] = np.nan\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _handle_missing_values(self, df: pd.DataFrame, is_fitting=False) -> pd.DataFrame:\n",
    "        \"\"\"Imputes missing values using stored medians (for transform) or calculates them (for fit).\"\"\"\n",
    "        logging.debug(f\"Handling missing values (is_fitting={is_fitting})...\")\n",
    "\n",
    "        # Columns to impute and their strategies (numeric: median, categorical: 'Unknown')\n",
    "        numeric_cols = [\n",
    "            'Duration_in_hours', 'Days_left', 'Price', 'Price_MA7', 'Price_Lag1',\n",
    "            'Weather_Temp_C', 'Weather_Humidity', 'Age',\n",
    "            'Fuel Consumption (L/hour)', 'Hourly Maintenance Cost ($)'\n",
    "        ]\n",
    "        categorical_cols = [\n",
    "            'Airline', 'Flight_code', 'Class', 'Source', 'Departure', 'Total_stops',\n",
    "            'Arrival', 'Destination', 'Journey_day', 'Weather_Condition', 'Season',\n",
    "            'AircraftType' # Impute this if missing, might be needed for merge/encoding\n",
    "        ]\n",
    "\n",
    "        # Impute numeric columns with median\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                median_attribute_name = f\"{col}_median\" # Attribute name to store/retrieve median\n",
    "                if is_fitting:\n",
    "                    # Calculate median from current data if fitting\n",
    "                    median_val = df[col].median()\n",
    "                    # Store the calculated median on the processor instance\n",
    "                    setattr(self, median_attribute_name, median_val)\n",
    "                    if pd.isna(median_val): logging.warning(f\"  Median for '{col}' is NaN during fit.\")\n",
    "                else:\n",
    "                    # Retrieve the stored median if transforming, default to 0 if not found\n",
    "                    median_val = getattr(self, median_attribute_name, 0)\n",
    "\n",
    "                # Perform imputation using the determined median value\n",
    "                # Ensure median_val itself is not NaN before filling\n",
    "                fill_value = median_val if pd.notna(median_val) else 0\n",
    "                df[col] = df[col].fillna(fill_value)\n",
    "            else:\n",
    "                logging.debug(f\"  Column '{col}' not found for numeric imputation.\")\n",
    "\n",
    "        # Impute categorical columns with 'Unknown'\n",
    "        for col in categorical_cols:\n",
    "             if col in df.columns:\n",
    "                 df[col] = df[col].fillna('Unknown') # Use a consistent placeholder string\n",
    "             else:\n",
    "                 logging.debug(f\"  Column '{col}' not found for categorical imputation.\")\n",
    "\n",
    "        # Final check for any remaining NaNs\n",
    "        remaining_nans = df.isnull().sum()\n",
    "        remaining_nans = remaining_nans[remaining_nans > 0]\n",
    "        if not remaining_nans.empty:\n",
    "            logging.warning(f\"  NaNs still remain after imputation in columns: {remaining_nans.index.tolist()}\")\n",
    "            # Consider logging the count: {remaining_nans.to_dict()}\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _fit_categorical_encoders(self, df: pd.DataFrame):\n",
    "        \"\"\"Fits LabelEncoder and OrdinalEncoder instances on the data.\"\"\"\n",
    "        logging.debug(\"Fitting categorical encoders...\")\n",
    "\n",
    "        # Dictionary mapping feature names to their corresponding LabelEncoder instances\n",
    "        label_encoders_map = {\n",
    "            'Route': self.route_encoder, 'Airline': self.airline_encoder, 'Class': self.class_encoder,\n",
    "            'Source': self.source_encoder, 'Destination': self.destination_encoder, 'Season': self.season_encoder,\n",
    "            'Weather_Condition': self.weather_cond_encoder, 'AircraftType': self.aircraft_model_encoder\n",
    "        }\n",
    "\n",
    "        # Fit each LabelEncoder\n",
    "        for name, encoder in label_encoders_map.items():\n",
    "            series = df.get(name)\n",
    "            if series is not None and not series.empty:\n",
    "                 try:\n",
    "                     # Include 'Unknown' explicitly in case it only appears in test/validation\n",
    "                     unique_values = np.append(series.astype(str).unique(), 'Unknown')\n",
    "                     encoder.fit(unique_values)\n",
    "                     logging.debug(f\"  Fitted LabelEncoder for '{name}' with {len(encoder.classes_)} classes.\")\n",
    "                 except Exception as e:\n",
    "                     logging.error(f\"  Error fitting LabelEncoder for '{name}': {e}\")\n",
    "            else:\n",
    "                 logging.warning(f\"  Cannot fit LabelEncoder for '{name}', data series is missing or empty.\")\n",
    "\n",
    "        # --- Explicitly Fit Ordinal Encoders ---\n",
    "        # Although categories are predefined, calling fit ensures consistency and attribute access.\n",
    "        ordinal_encoders_map = {\n",
    "            'Total_stops': self.stops_encoder,\n",
    "            'Journey_day': self.journey_day_encoder,\n",
    "            'Departure': self.dep_time_encoder,\n",
    "            'Arrival': self.arr_time_encoder\n",
    "        }\n",
    "\n",
    "        for name, encoder in ordinal_encoders_map.items():\n",
    "            series = df.get(name)\n",
    "            if series is not None and not series.empty:\n",
    "                try:\n",
    "                    # Fit requires a 2D array, so reshape the series\n",
    "                    encoder.fit(series.astype(str).values.reshape(-1, 1))\n",
    "                    # Now access categories_ safely after fitting\n",
    "                    logging.debug(f\"  Fitted OrdinalEncoder for '{name}'. Categories used: {encoder.categories_}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"  Error fitting OrdinalEncoder for '{name}': {e}\")\n",
    "            else:\n",
    "                logging.warning(f\"  Cannot fit OrdinalEncoder for '{name}', data series is missing or empty.\")\n",
    "                                \n",
    "        # Log their categories for confirmation.\n",
    "        logging.debug(f\"  Using predefined OrdinalEncoder for Stops: {self.stops_encoder.categories_}\")\n",
    "        logging.debug(f\"  Using predefined OrdinalEncoder for Journey Day: {self.journey_day_encoder.categories_}\")\n",
    "        logging.debug(f\"  Using predefined OrdinalEncoder for Departure Time: {self.dep_time_encoder.categories_}\")\n",
    "        logging.debug(f\"  Using predefined OrdinalEncoder for Arrival Time: {self.arr_time_encoder.categories_}\")\n",
    "\n",
    "    def _apply_encoders(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Applies the fitted encoders to transform categorical columns.\"\"\"\n",
    "        logging.debug(\"Applying categorical encoders...\")\n",
    "\n",
    "        # Map feature names to their fitted encoders and the desired output column name\n",
    "        encoders_to_apply = {\n",
    "            'Route': (self.route_encoder, 'Route_encoded'), 'Airline': (self.airline_encoder, 'Airline_encoded'),\n",
    "            'Class': (self.class_encoder, 'Class_encoded'), 'Source': (self.source_encoder, 'Source_encoded'),\n",
    "            'Destination': (self.destination_encoder, 'Destination_encoded'), 'Season': (self.season_encoder, 'Season_encoded'),\n",
    "            'Weather_Condition': (self.weather_cond_encoder, 'WeatherCond_encoded'), # Shortened name\n",
    "            'AircraftType': (self.aircraft_model_encoder, 'AircraftModel_encoded'), # Using AircraftType as source\n",
    "            'Total_stops': (self.stops_encoder, 'Stops_encoded'), # Shortened name\n",
    "            'Journey_day': (self.journey_day_encoder, 'JourneyDay_encoded'), # Shortened name\n",
    "            'Departure': (self.dep_time_encoder, 'Departure_encoded'),\n",
    "            'Arrival': (self.arr_time_encoder, 'Arrival_encoded')\n",
    "        }\n",
    "\n",
    "        # Apply each encoder\n",
    "        for source_col, (encoder, target_col) in encoders_to_apply.items():\n",
    "            series = df.get(source_col)\n",
    "            # Use helper functions for safe transformation\n",
    "            if isinstance(encoder, LabelEncoder):\n",
    "                df[target_col] = self._safe_transform_le(encoder, series, source_col)\n",
    "            elif isinstance(encoder, OrdinalEncoder):\n",
    "                df[target_col] = self._safe_transform_ord(encoder, series, source_col)\n",
    "            else:\n",
    "                 logging.warning(f\"Unknown encoder type for column '{source_col}'. Skipping.\")\n",
    "                 df[target_col] = -1 # Assign default error value\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _safe_transform_le(self, encoder: LabelEncoder, series: Optional[pd.Series], name: str) -> np.ndarray:\n",
    "        \"\"\"Safely transforms a Series using a fitted LabelEncoder, handling unknowns.\"\"\"\n",
    "        default_value = -1 # Value for unknown categories\n",
    "        # Return default if series is missing or encoder not fitted\n",
    "        if series is None: return np.full(len(series) if series is not None else 0, default_value, dtype=int) # Need length for shape\n",
    "        if not hasattr(encoder, 'classes_'):\n",
    "            logging.warning(f\"LabelEncoder '{name}' not fitted. Returning {default_value}.\")\n",
    "            return np.full(len(series), default_value, dtype=int)\n",
    "\n",
    "        series_str = series.astype(str)\n",
    "        # Create a mask for values known by the encoder\n",
    "        known_mask = series_str.isin(encoder.classes_)\n",
    "        # Initialize output array with the default value\n",
    "        encoded_values = np.full(series_str.shape, default_value, dtype=int)\n",
    "\n",
    "        try:\n",
    "            # Transform only the known values\n",
    "            if known_mask.any():\n",
    "                encoded_values[known_mask] = encoder.transform(series_str[known_mask])\n",
    "            # Log if unknowns were encountered\n",
    "            if (~known_mask).any():\n",
    "                 logging.debug(f\"  Unknown categories encountered in '{name}': {series_str[~known_mask].unique()}\")\n",
    "            return encoded_values\n",
    "        except Exception as e:\n",
    "            logging.error(f\"  Error applying LabelEncoder for '{name}': {e}\")\n",
    "            return np.full(series_str.shape, default_value, dtype=int) # Return default on error\n",
    "\n",
    "    def _safe_transform_ord(self, encoder: OrdinalEncoder, series: Optional[pd.Series], name: str) -> np.ndarray:\n",
    "        \"\"\"Safely transforms a Series using a fitted OrdinalEncoder.\"\"\"\n",
    "        default_value = -1 # Value for unknown categories (should match encoder's setting)\n",
    "        if series is None: return np.full(len(series) if series is not None else 0, default_value, dtype=int)\n",
    "        if not hasattr(encoder, 'categories_'):\n",
    "            logging.warning(f\"OrdinalEncoder '{name}' not fitted. Returning {default_value}.\")\n",
    "            return np.full(len(series), default_value, dtype=int)\n",
    "\n",
    "        try:\n",
    "            # OrdinalEncoder with handle_unknown='use_encoded_value' handles unknowns internally\n",
    "            encoded_values = encoder.transform(series.astype(str).values.reshape(-1, 1)).flatten()\n",
    "            return encoded_values.astype(int)\n",
    "        except Exception as e:\n",
    "            # This might catch errors if a value is truly unexpected and not handled\n",
    "            logging.error(f\"  Error applying OrdinalEncoder for '{name}': {e}\")\n",
    "            return np.full(len(series), default_value, dtype=int)\n",
    "\n",
    "    def _fit_numerical_scalers(self, df: pd.DataFrame):\n",
    "        \"\"\"Fits StandardScaler and MinMaxScaler instances on numerical columns.\"\"\"\n",
    "        logging.debug(\"Fitting numerical scalers...\")\n",
    "\n",
    "        # Map features to their corresponding scaler instances\n",
    "        scalers_map = {\n",
    "            'Price': self.price_scaler, 'Days_left': self.days_left_scaler,\n",
    "            'Duration_in_hours': self.duration_scaler, 'Weather_Temp_C': self.temp_scaler,\n",
    "            'Weather_Humidity': self.humidity_scaler, 'Age': self.age_scaler,\n",
    "            'Fuel Consumption (L/hour)': self.fuel_eff_scaler,\n",
    "            'Price_MA7': self.fare_ma7_scaler, 'Price_Lag1': self.fare_lag1_scaler\n",
    "        }\n",
    "\n",
    "        # Fit each scaler\n",
    "        for name, scaler in scalers_map.items():\n",
    "            series = df.get(name)\n",
    "            # Check if series exists, is numeric, and has non-NaN values\n",
    "            if series is not None and pd.api.types.is_numeric_dtype(series):\n",
    "                series_cleaned = series.dropna()\n",
    "                if not series_cleaned.empty:\n",
    "                    try:\n",
    "                        # Fit scaler on the non-NaN values, reshaped for sklearn input\n",
    "                        scaler.fit(series_cleaned.values.reshape(-1, 1))\n",
    "                        logging.debug(f\"  Fitted {scaler.__class__.__name__} for '{name}'.\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"  Error fitting scaler for '{name}': {e}\")\n",
    "                else:\n",
    "                     logging.warning(f\"  Cannot fit scaler for '{name}', all values are NaN.\")\n",
    "            else:\n",
    "                 logging.warning(f\"  Cannot fit scaler for '{name}', column missing, empty, or non-numeric.\")\n",
    "\n",
    "        # --- Update Config Price Range ---\n",
    "        # After fitting the price_scaler, update the config's price_range\n",
    "        # This ensures the environment uses the actual observed min/max fares.\n",
    "        if hasattr(self.price_scaler, 'data_min_') and hasattr(self.price_scaler, 'data_max_'):\n",
    "             # Ensure min/max are valid numbers before updating\n",
    "             min_price = float(self.price_scaler.data_min_[0])\n",
    "             max_price = float(self.price_scaler.data_max_[0])\n",
    "             if pd.notna(min_price) and pd.notna(max_price) and min_price < max_price:\n",
    "                 self.config.price_range = (min_price, max_price)\n",
    "                 logging.info(f\"  Configuration price_range updated based on fitted Fare data: {self.config.price_range}\")\n",
    "             else:\n",
    "                 logging.warning(\"  Could not update config price_range, fitted scaler min/max invalid.\")\n",
    "\n",
    "    def _apply_scalers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Applies the fitted scalers to transform numerical columns.\"\"\"\n",
    "        logging.debug(\"Applying numerical scalers...\")\n",
    "\n",
    "        # Map original feature names to scaler instances and target scaled column names\n",
    "        scalers_to_apply = {\n",
    "            'Price': (self.price_scaler, 'Price_scaled'),\n",
    "            'Days_left': (self.days_left_scaler, 'Days_Left_scaled'), # Renamed target\n",
    "            'Duration_in_hours': (self.duration_scaler, 'Duration_scaled'), # Renamed target\n",
    "            'Weather_Temp_C': (self.temp_scaler, 'Weather_Temp_scaled'), # Renamed target\n",
    "            'Weather_Humidity': (self.humidity_scaler, 'Weather_Humidity_scaled'), # Renamed target\n",
    "            'Age': (self.age_scaler, 'Age_scaled'),\n",
    "            'Fuel Consumption (L/hour)': (self.fuel_eff_scaler, 'Fuel_Efficiency_scaled'), # Renamed target\n",
    "            'Price_MA7': (self.fare_ma7_scaler, 'Price_MA7_scaled'),\n",
    "            'Price_Lag1': (self.fare_lag1_scaler, 'Price_Lag1_scaled')\n",
    "        }\n",
    "\n",
    "        # Apply each scaler\n",
    "        for source_col, (scaler, target_col) in scalers_to_apply.items():\n",
    "            series = df.get(source_col)\n",
    "            # Use helper for safe scaling\n",
    "            df[target_col] = self._safe_scale(scaler, series, source_col)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _safe_scale(self, scaler, series: Optional[pd.Series], name: str) -> np.ndarray:\n",
    "        \"\"\"Safely scales a Series using a fitted scaler, handling NaNs and fitting status.\"\"\"\n",
    "        # Return default (array of zeros) if series is missing or non-numeric\n",
    "        if series is None or not pd.api.types.is_numeric_dtype(series):\n",
    "             return np.zeros(len(series) if series is not None else 0, dtype=float)\n",
    "\n",
    "        # Check if the scaler instance is fitted (has learned parameters)\n",
    "        is_fitted = hasattr(scaler, 'scale_') or hasattr(scaler, 'data_min_')\n",
    "        if not is_fitted:\n",
    "            logging.warning(f\"Scaler for '{name}' is not fitted. Returning array of zeros.\")\n",
    "            return np.zeros(len(series), dtype=float)\n",
    "\n",
    "        try:\n",
    "            # Impute NaNs *before* scaling. Use mean for StandardScaler, min for MinMaxScaler as simple fallbacks.\n",
    "            if isinstance(scaler, StandardScaler) and hasattr(scaler, 'mean_'):\n",
    "                impute_value = scaler.mean_[0] if pd.notna(scaler.mean_[0]) else 0.0\n",
    "            elif isinstance(scaler, MinMaxScaler) and hasattr(scaler, 'data_min_'):\n",
    "                 impute_value = scaler.data_min_[0] if pd.notna(scaler.data_min_[0]) else 0.0\n",
    "            else: impute_value = 0.0 # Generic fallback\n",
    "\n",
    "            series_filled = series.fillna(impute_value)\n",
    "            # Reshape for sklearn and transform\n",
    "            scaled_values = scaler.transform(series_filled.values.reshape(-1, 1)).flatten()\n",
    "            return scaled_values\n",
    "        except Exception as e:\n",
    "            logging.error(f\"  Error applying scaler for '{name}': {e}\")\n",
    "            # Return array of zeros on error\n",
    "            return np.zeros(len(series), dtype=float)\n",
    "\n",
    "    # --- Public Price Scaling Utilities ---\n",
    "\n",
    "    def scale_prices(self, prices: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Scales an array of prices (fares) using the fitted price_scaler (MinMaxScaler to [0, 1]).\n",
    "\n",
    "        Args:\n",
    "            prices: A NumPy array of prices.\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array of scaled prices (0-1 range).\n",
    "        \"\"\"\n",
    "        if not self.is_fitted or not hasattr(self.price_scaler, 'scale_'):\n",
    "            raise RuntimeError(\"Price scaler has not been fitted. Call fit() first.\")\n",
    "        # Use the safe scaling helper method\n",
    "        # Need to convert numpy array to Series temporarily for the helper\n",
    "        scaled_prices_array = self._safe_scale(self.price_scaler, pd.Series(prices), \"Price (Utility Scale)\")\n",
    "        return scaled_prices_array\n",
    "\n",
    "    def inverse_scale_prices(self, scaled_prices_01: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Inverse transforms an array of scaled prices (in [0, 1] range) back to the original fare range.\n",
    "\n",
    "        Args:\n",
    "            scaled_prices_01: A NumPy array of prices scaled between 0 and 1.\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array of prices in the original fare scale.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted or not hasattr(self.price_scaler, 'scale_'):\n",
    "             raise RuntimeError(\"Price scaler has not been fitted. Call fit() first.\")\n",
    "\n",
    "        # Ensure input is a NumPy array and has the correct shape for inverse_transform\n",
    "        scaled_prices = np.array(scaled_prices_01).reshape(-1, 1)\n",
    "\n",
    "        # Clip scaled prices to the expected [0, 1] range before inverse transforming\n",
    "        # This prevents errors if the input action (e.g., from actor network noise) slightly exceeds bounds.\n",
    "        scaled_prices_clipped = np.clip(scaled_prices, 0.0, 1.0)\n",
    "\n",
    "        try:\n",
    "            # Perform the inverse transformation\n",
    "            original_prices = self.price_scaler.inverse_transform(scaled_prices_clipped)\n",
    "            # Return as a flattened 1D array\n",
    "            return original_prices.flatten()\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during inverse price scaling: {e}\")\n",
    "             # Fallback strategy: return the minimum price from the config range\n",
    "             return np.full(scaled_prices.shape[0], self.config.price_range[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load, Fit, Transform Data\n",
    " *Execute the data processing pipeline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor(config)\n",
    "raw_data = data_processor._load_data()\n",
    "\n",
    "# Fit\n",
    "try:\n",
    "    data_processor.fit(raw_data)\n",
    "    print(f\"\\n DataProcessor Fitted. Price Range: {config.price_range}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Data Fitting Failed: {e}\", exc_info=True); raise SystemExit(\"Stop: Fitting Error\") from e\n",
    "\n",
    "# Transform\n",
    "processed_data = data_processor.transform(raw_data)\n",
    "\n",
    "if processed_data.empty:\n",
    "    raise SystemExit(\" Processed data is empty! Check processing steps.\")\n",
    "else:\n",
    "    print(f\" Data Processed. Shape: {processed_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Interactive Data Exploration\n",
    " *Explore distributions and data samples interactively.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sample of Processed Data\n",
    "display(HTML(\"<h4>Processed Data Sample:</h4>\"))\n",
    "display(processed_data.sample(5))\n",
    "\n",
    "# Interactive Distribution Plot\n",
    "numeric_cols = processed_data.select_dtypes(include=np.number).columns.tolist()\n",
    "# Filter out obviously encoded columns for better default view\n",
    "numeric_cols_plot = [c for c in numeric_cols if not c.endswith('_encoded') and c not in ['IsHoliday', 'IsWeekend']]\n",
    "\n",
    "@interact\n",
    "def show_distributions(column=Dropdown(options=numeric_cols_plot, value='Price')):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(processed_data[column].dropna(), kde=True, bins=50) # dropna for safety\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reinforcement Learning Environment\n",
    "---\n",
    "*Visual environment diagram (placeholder).* Define the environment dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Environment Diagram (Conceptual)\n",
    "*Illustrates the Agent-Environment interaction loop.* "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%{init: {'theme': 'neutral', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
    "sequenceDiagram\n",
    "    participant TD3 Agent\n",
    "    participant Pricing Environment\n",
    "    \n",
    "    Note left of TD3 Agent: Twin Delayed DDPG Architecture<br/>- Actor-Critic Networks<br/>- Target Networks<br/>- Experience Replay\n",
    "    Note right of Pricing Environment: Components from Code:<br/>- State: 45+ Features<br/>- Demand Heuristic<br/>- Profit Calculation\n",
    "    \n",
    "    loop Each Simulation Day\n",
    "        TD3 Agent->>Pricing Environment: Continuous Actions<br/>(Normalized Prices: [-1,1]^n)\n",
    "        Pricing Environment->>Pricing Environment: \n",
    "            Convert to Actual Prices (8k-25k)<br/>\n",
    "            Calculate Demand per Flight:<br/>\n",
    "            - Price vs Hist Avg<br/>\n",
    "            - Days Left Urgency<br/>\n",
    "            - Class/Stop Adjustments<br/>\n",
    "            - Random Noise\n",
    "        \n",
    "        Pricing Environment->>Pricing Environment: \n",
    "            Calculate Profit:<br/>\n",
    "            [(Price - Cost)  Sold]<br/>\n",
    "            Cost = Fuel + Maintenance<br/>\n",
    "            Update Seats Left\n",
    "        \n",
    "        Pricing Environment-->>TD3 Agent: \n",
    "            New State:<br/>\n",
    "            - Temporal Features<br/>\n",
    "            - Market Conditions<br/>\n",
    "            - Flight Status<br/>\n",
    "            Reward: Daily Profit<br/>\n",
    "            Done: End of Period\n",
    "            \n",
    "        TD3 Agent->>TD3 Agent: \n",
    "            Store Experience<br/>\n",
    "            Update Networks:<br/>\n",
    "            - Delayed Policy Updates<br/>\n",
    "            - Target Smoothing\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Environment Class (`AirlinePricingEnv`)\n",
    " *Defines the simulation logic.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirlinePricingEnv:\n",
    "    \"\"\"\n",
    "    Simulates the airline pricing environment for Reinforcement Learning.\n",
    "\n",
    "    The environment takes actions representing desired price levels (scaled),\n",
    "    simulates demand and costs based on heuristics, calculates profit,\n",
    "    and provides the next state and reward to the agent.\n",
    "\n",
    "    Key Attributes:\n",
    "        data (pd.DataFrame): Preprocessed historical flight data.\n",
    "        data_processor (DataProcessor): Instance used for data transformations (esp. price scaling).\n",
    "        config (Config): Configuration object with environment parameters.\n",
    "        device (torch.device): CPU or GPU device for tensor operations.\n",
    "        n_flights (int): Number of unique Route-Airline combinations being managed.\n",
    "        state_size (int): Dimension of the state vector provided to the agent.\n",
    "        action_size (int): Dimension of the action vector (equals n_flights).\n",
    "        current_prices (np.ndarray): Current prices set for each flight.\n",
    "        seats_sold (np.ndarray): Number of seats sold for each flight so far in the episode.\n",
    "        current_date (pd.Timestamp): Current simulation day.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processed_data: pd.DataFrame, data_processor: DataProcessor, config: Config, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initializes the Airline Pricing Environment.\n",
    "\n",
    "        Args:\n",
    "            processed_data: DataFrame containing preprocessed features (output from DataProcessor.transform).\n",
    "            data_processor: Fitted DataProcessor instance (needed for price scaling & stats).\n",
    "            config: Configuration object containing simulation parameters (capacity, length, etc.).\n",
    "            device: The torch device (CPU or GPU) to use for state tensors.\n",
    "        \"\"\"\n",
    "        if processed_data.empty:\n",
    "            raise ValueError(\"Processed data cannot be empty for environment initialization.\")\n",
    "\n",
    "        # --- Store Core Components ---\n",
    "        # Sort data for potentially easier lookups later, though data_by_date is primary\n",
    "        self.data = processed_data.sort_values(by=['Route', 'Airline', 'Date']).reset_index(drop=True)\n",
    "        self.data_processor = data_processor\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        logging.info(\"Initializing AirlinePricingEnv...\")\n",
    "\n",
    "        # --- Environment Parameters ---\n",
    "        self.seats_capacity = self.config.seats_capacity\n",
    "        self.simulation_length_days = self.config.simulation_length_days\n",
    "        # Price range is crucial and should be set by DataProcessor during fit\n",
    "        self.price_range = self.config.price_range\n",
    "        # Pre-calculate price range delta for efficient action scaling\n",
    "        self.price_delta = self.price_range[1] - self.price_range[0]\n",
    "        if self.price_delta <= 0:\n",
    "             logging.warning(f\"Invalid price range detected ({self.price_range}). Check data processor fitting.\")\n",
    "             # Provide a fallback range if needed\n",
    "             # self.price_range = (1000.0, 50000.0)\n",
    "             # self.price_delta = self.price_range[1] - self.price_range[0]\n",
    "        logging.info(f\"  Environment using Price Range: [{self.price_range[0]:.2f}, {self.price_range[1]:.2f}] (Delta: {self.price_delta:.2f})\")\n",
    "\n",
    "        # --- Identify Unique Flights ---\n",
    "        # A \"flight\" in this context is a unique Route-Airline combination\n",
    "        if 'Route' not in self.data.columns or 'Airline' not in self.data.columns:\n",
    "            raise ValueError(\"Processed data must contain 'Route' and 'Airline' columns.\")\n",
    "        # Get unique pairs and create a mapping for easy indexing\n",
    "        self.flights = self.data[['Route', 'Airline']].drop_duplicates().to_records(index=False)\n",
    "        self.n_flights = len(self.flights)\n",
    "        if self.n_flights == 0: raise ValueError(\"No unique flights (Route-Airline pairs) found in the processed data.\")\n",
    "        # Create a dictionary: {(Route, Airline): index}\n",
    "        self.flight_map = {(route, airline): i for i, (route, airline) in enumerate(self.flights)}\n",
    "        logging.info(f\"  Found {self.n_flights} unique flights (Route-Airline pairs).\")\n",
    "\n",
    "        # --- Simulation Time Tracking ---\n",
    "        self.start_date = self.data['Date'].min()\n",
    "        # Calculate end date based on simulation length\n",
    "        self.end_date = self.start_date + pd.Timedelta(days=self.simulation_length_days - 1)\n",
    "        logging.info(f\"  Simulation period: {self.start_date.date()} to {self.end_date.date()} ({self.simulation_length_days} days)\")\n",
    "        self.current_step: int = 0          # Counter for steps within an episode\n",
    "        self.current_date: pd.Timestamp = self.start_date # Current simulation day\n",
    "\n",
    "        # --- Dynamic Environment State ---\n",
    "        # Track seats sold for each flight within the current episode\n",
    "        self.seats_sold = np.zeros(self.n_flights, dtype=int)\n",
    "        # Track the price set by the agent for each flight on the *current* day\n",
    "        # Initialize prices using the global historical mean fare calculated by DataProcessor\n",
    "        initial_price = self.data_processor.global_fare_mean\n",
    "        # Ensure initial price is within the allowed range\n",
    "        initial_price = np.clip(initial_price if pd.notna(initial_price) else self.price_range[0],\n",
    "                                self.price_range[0], self.price_range[1])\n",
    "        self.current_prices = np.full(self.n_flights, fill_value=initial_price, dtype=float)\n",
    "\n",
    "        # --- Precomputed Data Structures for Efficiency ---\n",
    "        # Group data by normalized date for quick lookups in the step function\n",
    "        self.data_by_date: Dict[pd.Timestamp, pd.DataFrame] = {\n",
    "            date.normalize(): group\n",
    "            for date, group in self.data.groupby(self.data['Date'].dt.normalize())\n",
    "        }\n",
    "        # Store historical fare statistics (mean/median per route/airline) from DataProcessor\n",
    "        self.hist_fare_stats = self.data_processor.historical_fare_stats\n",
    "        # Store global fallback mean fare\n",
    "        self.global_fare_mean = self.data_processor.global_fare_mean\n",
    "\n",
    "        # --- Determine State and Action Dimensions ---\n",
    "        # State size depends on the features defined in _determine_state_size\n",
    "        self.state_size: int = self._determine_state_size()\n",
    "        # Action size is the number of flights, as the agent sets one price per flight\n",
    "        self.action_size: int = self.n_flights\n",
    "\n",
    "        logging.info(f\"  Environment State Size: {self.state_size}\")\n",
    "        logging.info(f\"  Environment Action Size: {self.action_size} (one price per flight)\")\n",
    "        if self.state_size == 0:\n",
    "             raise ValueError(\"State size calculation failed. Check required features and data processing.\")\n",
    "\n",
    "    def _determine_state_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Determines the size of the state vector based on explicitly defined features.\n",
    "        Checks if required features exist in the processed data.\n",
    "\n",
    "        Returns:\n",
    "            The calculated total dimension of the state vector.\n",
    "        \"\"\"\n",
    "        logging.debug(\"Determining state vector size and features...\")\n",
    "        # Define features EXPLICITLY included in the state vector\n",
    "        # Part 1: Features shared across all flights for a given day\n",
    "        shared_features = [\n",
    "            'DayOfWeek', 'Month', 'WeekOfYear', 'DayOfYear', 'IsWeekend', 'IsHoliday',\n",
    "            'FuelPrice' # Using the constant fallback value\n",
    "        ]\n",
    "        # Part 2: Features specific to each flight\n",
    "        flight_features = [\n",
    "            # Internal dynamic state (calculated within the environment)\n",
    "            'Seats_Left_scaled',      # Proportion of seats remaining (0-1)\n",
    "            'Current_Price_scaled',   # Agent's previous price, scaled (0-1) by historical range\n",
    "            # Data-driven features (from processed_data)\n",
    "            'Days_Left_scaled',         # Days until departure, scaled (0-1)\n",
    "            'Duration_scaled',        # Flight duration, standardized\n",
    "            'Price_MA7_scaled',       # 7-day rolling avg historical price, standardized\n",
    "            'Price_Lag1_scaled',      # Previous day's historical price, standardized\n",
    "            # Encoded categorical features\n",
    "            'Route_encoded',          # Integer ID for route\n",
    "            'Airline_encoded',        # Integer ID for airline\n",
    "            'Class_encoded',          # Integer ID for class\n",
    "            'Stops_encoded',          # Integer ID for number of stops (ordinal)\n",
    "            'Source_encoded',         # Integer ID for origin city\n",
    "            'Destination_encoded',    # Integer ID for destination city\n",
    "            'Season_encoded',         # Integer ID for season\n",
    "            'JourneyDay_encoded',     # Integer ID for day name (ordinal)\n",
    "            'Departure_encoded',      # Integer ID for departure time block (ordinal)\n",
    "            'Arrival_encoded',        # Integer ID for arrival time block (ordinal)\n",
    "            # Weather features (merged and scaled/encoded)\n",
    "            'Weather_Temp_scaled',    # Avg daily temp at origin, standardized\n",
    "            'Weather_Humidity_scaled',# Avg daily humidity at origin, standardized\n",
    "            'WeatherCond_encoded',    # Integer ID for weather condition text\n",
    "            # Aircraft features (merged and scaled/encoded)\n",
    "            'Aircraft_Age_scaled',    # Aircraft age, standardized\n",
    "            'Fuel_Efficiency_scaled', # Fuel consumption rate, standardized\n",
    "            'AircraftModel_encoded'   # Integer ID for aircraft model/type\n",
    "        ]\n",
    "\n",
    "        # --- Verification ---\n",
    "        # Check if all required data-driven features are present in the processed DataFrame\n",
    "        all_required_data_cols = [f for f in shared_features + flight_features\n",
    "                                  if f not in ['Seats_Left_scaled', 'Current_Price_scaled', 'FuelPrice']] # Exclude internally generated state\n",
    "        missing_cols = [col for col in all_required_data_cols if col not in self.data.columns]\n",
    "        if missing_cols:\n",
    "            logging.error(f\"CRITICAL ERROR: State features missing from processed data columns: {missing_cols}\")\n",
    "            logging.error(f\"Available columns are: {self.data.columns.tolist()}\")\n",
    "            return 0 # Indicate failure\n",
    "\n",
    "        num_shared = len(shared_features)\n",
    "        num_flight_specific = len(flight_features)\n",
    "        determined_size = num_shared + self.n_flights * num_flight_specific\n",
    "\n",
    "        # Store feature names lists for use in state construction methods\n",
    "        self._shared_feature_names = shared_features\n",
    "        self._flight_feature_names = flight_features\n",
    "        # Separate internal vs data-driven features for clarity in _get_state_for_flight\n",
    "        self._flight_internal_state_features = ['Seats_Left_scaled', 'Current_Price_scaled']\n",
    "        self._flight_data_features = [f for f in flight_features if f not in self._flight_internal_state_features]\n",
    "        logging.debug(f\"  State size determined: {determined_size} ({num_shared} shared + {self.n_flights} flights * {num_flight_specific} flight-specific)\")\n",
    "        return determined_size\n",
    "\n",
    "    def _get_shared_state(self, date: pd.Timestamp) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the shared part of the state vector for a given date.\n",
    "\n",
    "        Args:\n",
    "            date: The current simulation timestamp.\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array containing the shared state features.\n",
    "        \"\"\"\n",
    "        norm_date = date.normalize() # Use normalized date for lookup\n",
    "        day_data = self.data_by_date.get(norm_date) # Retrieve pre-grouped data for the day\n",
    "\n",
    "        # Initialize state vector with zeros\n",
    "        shared_state_values = np.zeros(len(self._shared_feature_names), dtype=np.float32)\n",
    "\n",
    "        # Get index of FuelPrice to set the fallback value correctly\n",
    "        try: fuel_idx = self._shared_feature_names.index('FuelPrice')\n",
    "        except ValueError: fuel_idx = -1 # Should not happen if defined above\n",
    "\n",
    "        # Handle cases where data for the specific date might be missing\n",
    "        if day_data is None or day_data.empty:\n",
    "            logging.log(logging.DEBUG - 1, f\"No historical data found for date {norm_date}. Using defaults for shared state.\")\n",
    "            # Set the fallback fuel price if the feature is included\n",
    "            if fuel_idx != -1: shared_state_values[fuel_idx] = self.data_processor.fuel_price_mean\n",
    "            return shared_state_values\n",
    "\n",
    "        # Use the first row of the day's data for shared features (they should be constant for the day)\n",
    "        row = day_data.iloc[0]\n",
    "        # Populate the state vector based on defined feature names\n",
    "        for i, fname in enumerate(self._shared_feature_names):\n",
    "            if fname == 'FuelPrice':\n",
    "                shared_state_values[i] = self.data_processor.fuel_price_mean # Use constant fallback\n",
    "            else:\n",
    "                # Use .get(fname, 0) for safety, defaulting to 0 if a feature is unexpectedly missing\n",
    "                shared_state_values[i] = row.get(fname, 0.0)\n",
    "\n",
    "        return shared_state_values\n",
    "\n",
    "    def _get_state_for_flight(self, flight_index: int, date: pd.Timestamp) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the flight-specific part of the state vector for a single flight on a given date.\n",
    "\n",
    "        Args:\n",
    "            flight_index: The index of the flight (in self.flights).\n",
    "            date: The current simulation timestamp.\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array containing the flight-specific state features.\n",
    "        \"\"\"\n",
    "        # Get route and airline identifier for this flight index\n",
    "        route, airline = self.flights[flight_index]\n",
    "        # Initialize the state vector part with zeros\n",
    "        flight_state_values = np.zeros(len(self._flight_feature_names), dtype=np.float32)\n",
    "\n",
    "        # --- 1. Get Internal (Dynamic) State Features ---\n",
    "        # Seats Left: Calculate remaining seats and scale by capacity\n",
    "        seats_left = self.seats_capacity - self.seats_sold[flight_index]\n",
    "        seats_left_scaled = seats_left / self.seats_capacity if self.seats_capacity > 0 else 0.0\n",
    "        # Current Price: Use the price set in the *previous* step, scaled 0-1\n",
    "        current_price_scaled_01 = self.data_processor.scale_prices(np.array([self.current_prices[flight_index]]))[0]\n",
    "\n",
    "        # Populate the internal state features in the vector\n",
    "        try:\n",
    "            # Find indices based on predefined names\n",
    "            sl_idx = self._flight_feature_names.index('Seats_Left_scaled')\n",
    "            cp_idx = self._flight_feature_names.index('Current_Price_scaled')\n",
    "            flight_state_values[sl_idx] = seats_left_scaled\n",
    "            flight_state_values[cp_idx] = current_price_scaled_01\n",
    "        except ValueError as e:\n",
    "            # This indicates a mismatch between defined features and the index lookup\n",
    "            logging.error(f\"State feature name mismatch error: {e}. Check _determine_state_size.\")\n",
    "\n",
    "        # --- 2. Get Data-Driven State Features ---\n",
    "        norm_date = date.normalize() # Use normalized date for lookup\n",
    "        day_data = self.data_by_date.get(norm_date) # Get data for the current day\n",
    "\n",
    "        # If no data exists for the day, return state with zeros for data features\n",
    "        if day_data is None:\n",
    "            logging.log(logging.DEBUG - 1, f\"No historical data for date {norm_date}, flight {flight_index}. Using zeros for data features.\")\n",
    "            return flight_state_values\n",
    "\n",
    "        # Find the specific row(s) for this flight (Route-Airline) on this date\n",
    "        flight_data_rows = day_data[(day_data['Route'] == route) & (day_data['Airline'] == airline)]\n",
    "\n",
    "        # If no specific data for this flight on this day, return state with zeros\n",
    "        if flight_data_rows.empty:\n",
    "            logging.log(logging.DEBUG - 1, f\"No specific historical data for flight {route}-{airline} on {norm_date}. Using zeros for data features.\")\n",
    "            return flight_state_values\n",
    "\n",
    "        # Use the first matching row if multiple exist (shouldn't happen often with daily data)\n",
    "        row = flight_data_rows.iloc[0]\n",
    "\n",
    "        # Populate the remaining features using data from the matched row\n",
    "        for fname in self._flight_data_features: # Iterate through data-driven feature names\n",
    "             try:\n",
    "                  # Find the index in the full flight feature list\n",
    "                  f_idx = self._flight_feature_names.index(fname)\n",
    "                  # Get value from row, defaulting to 0.0 if column missing from row (shouldn't happen after processing)\n",
    "                  flight_state_values[f_idx] = row.get(fname, 0.0)\n",
    "             except ValueError:\n",
    "                 # This feature name wasn't found in the main list - indicates definition error\n",
    "                 logging.error(f\"Feature '{fname}' not found in _flight_feature_names list.\")\n",
    "                 pass # Continue, but state will be incomplete/incorrect\n",
    "\n",
    "        return flight_state_values\n",
    "\n",
    "    def _get_state_vector(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Constructs the full, flattened state vector for the current simulation date\n",
    "        by combining shared and all flight-specific states. Performs validation.\n",
    "\n",
    "        Returns:\n",
    "            A PyTorch tensor representing the complete state.\n",
    "        \"\"\"\n",
    "        date = self.current_date # Use the environment's current date\n",
    "\n",
    "        # Get the shared part of the state\n",
    "        shared_state = self._get_shared_state(date)\n",
    "        # Get the specific state part for each flight\n",
    "        flight_states_list = [self._get_state_for_flight(i, date) for i in range(self.n_flights)]\n",
    "\n",
    "        # --- Concatenation ---\n",
    "        # Ensure components are NumPy arrays before combining\n",
    "        shared_state_np = np.asarray(shared_state, dtype=np.float32)\n",
    "        # Flatten the list of flight state arrays into a single 1D array\n",
    "        flight_states_np_flat = np.asarray(flight_states_list, dtype=np.float32).flatten()\n",
    "\n",
    "        # Concatenate shared state + flattened flight states\n",
    "        full_state = np.concatenate([shared_state_np, flight_states_np_flat])\n",
    "\n",
    "        # --- Validation and Sanitization ---\n",
    "        # 1. Size Check: Ensure constructed state size matches expected size\n",
    "        if len(full_state) != self.state_size:\n",
    "            logging.warning(f\"Constructed state size mismatch! Expected {self.state_size}, got {len(full_state)}. Padding/Truncating.\")\n",
    "            # Create a correctly sized zero array and copy data into it\n",
    "            correct_state = np.zeros(self.state_size, dtype=np.float32)\n",
    "            limit = min(len(full_state), self.state_size)\n",
    "            correct_state[:limit] = full_state[:limit]\n",
    "            full_state = correct_state\n",
    "\n",
    "        # 2. NaN/Infinity Check: Replace any invalid numerical values\n",
    "        if np.isnan(full_state).any() or np.isinf(full_state).any():\n",
    "            logging.warning(f\"NaN/Inf detected in state vector for date {date}. Imputing with 0/large values.\")\n",
    "            full_state = np.nan_to_num(full_state, nan=0.0, posinf=1e6, neginf=-1e6) # Replace NaN with 0, Inf with large numbers\n",
    "\n",
    "        # Convert the final NumPy array to a PyTorch tensor on the specified device\n",
    "        state_tensor = torch.tensor(full_state, dtype=torch.float32, device=self.device)\n",
    "        return state_tensor\n",
    "\n",
    "    def reset(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state for the start of a new episode.\n",
    "\n",
    "        Returns:\n",
    "            The initial state vector as a PyTorch tensor.\n",
    "        \"\"\"\n",
    "        logging.debug(\"Resetting environment state for new episode...\")\n",
    "        # Reset step counter and simulation date\n",
    "        self.current_step = 0\n",
    "        self.current_date = self.start_date # Reset to the beginning of the simulation period\n",
    "\n",
    "        # Reset dynamic state arrays\n",
    "        self.seats_sold.fill(0) # Reset seats sold for all flights\n",
    "        # Reset current prices to the initial calculated value (e.g., global mean fare)\n",
    "        initial_price = self.global_fare_mean\n",
    "        initial_price = np.clip(initial_price if pd.notna(initial_price) else self.price_range[0],\n",
    "                                self.price_range[0], self.price_range[1])\n",
    "        self.current_prices.fill(initial_price)\n",
    "\n",
    "        logging.debug(f\"Environment reset to date {self.current_date.date()}.\")\n",
    "        # Return the state vector corresponding to the reset state\n",
    "        return self._get_state_vector()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Demand and Cost Heuristics (Internal Simulation Logic)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def _calculate_demand(self, flight_index: int, price: float) -> int:\n",
    "        \"\"\"\n",
    "        (HEURISTIC MODEL) Calculates estimated demand for a specific flight at a given price.\n",
    "        This function uses simplified rules and does not rely on real historical demand data.\n",
    "\n",
    "        Args:\n",
    "            flight_index: Index of the flight.\n",
    "            price: The price set for the flight for the current day.\n",
    "\n",
    "        Returns:\n",
    "            An estimated integer demand value.\n",
    "        \"\"\"\n",
    "        route, airline = self.flights[flight_index]\n",
    "        flight_key = (route, airline) # Key for looking up historical stats\n",
    "        date = self.current_date\n",
    "        norm_date = date.normalize()\n",
    "\n",
    "        # --- 1. Base Potential ---\n",
    "        # Arbitrary starting point for demand potential\n",
    "        base_potential = 50.0 # Example: Start with a base of 50 interested customers\n",
    "\n",
    "        # --- 2. Get Contextual Data ---\n",
    "        # Retrieve historical data row for this flight on this day, if available\n",
    "        day_data = self.data_by_date.get(norm_date)\n",
    "        flight_row = None\n",
    "        if day_data is not None:\n",
    "             rows = day_data[(day_data['Route'] == route) & (day_data['Airline'] == airline)]\n",
    "             if not rows.empty: flight_row = rows.iloc[0]\n",
    "\n",
    "        # --- 3. Apply Demand Factors (Multipliers) ---\n",
    "        # Price Effect: Demand decreases relative to historical average price\n",
    "        hist_stats = self.hist_fare_stats.get(flight_key) # Get precomputed mean/median\n",
    "        # Use mean if available, else fallback to global mean\n",
    "        hist_avg = hist_stats['mean'] if hist_stats and pd.notna(hist_stats.get('mean')) else self.global_fare_mean\n",
    "        hist_avg = self.global_fare_mean if hist_avg <= 0 else hist_avg # Ensure positive reference price\n",
    "        price_ratio = price / hist_avg # How does current price compare to average?\n",
    "        price_sensitivity = 0.7 # Parameter controlling price effect steepness\n",
    "        # Exponential decay-like effect: demand drops as price exceeds historical avg\n",
    "        price_effect = 1.0 / (1.0 + price_sensitivity * max(0.0, price_ratio - 1.0))\n",
    "\n",
    "        # Days Left Effect: Demand increases closer to departure (urgency)\n",
    "        # Get days_left from the data row or estimate from current step\n",
    "        if flight_row is not None and 'Days_left' in flight_row:\n",
    "             days_left = flight_row['Days_left']\n",
    "        else: days_left = max(1, self.simulation_length_days - self.current_step) # Estimate if missing\n",
    "        days_left_urgency = 1.2 # Maximum multiplier (e.g., 20% higher demand on day 0)\n",
    "        # Non-linear effect: demand increases faster closer to the end\n",
    "        days_left_effect = 1.0 + (days_left_urgency - 1.0) * (1.0 - (days_left / self.simulation_length_days))**2\n",
    "\n",
    "        # Class/Stops Effect: Simple adjustments based on flight characteristics\n",
    "        class_effect = 1.0\n",
    "        stops_effect = 1.0\n",
    "        if flight_row is not None:\n",
    "             # Assume Business class has lower volume demand (adjust multiplier as needed)\n",
    "             if flight_row.get('Class') == 'Business': class_effect = 0.5\n",
    "             # Assume demand decreases with more stops\n",
    "             stops = flight_row.get('Total_stops', 'non-stop')\n",
    "             if stops == '1-stop': stops_effect = 0.7\n",
    "             elif stops == '2-stops': stops_effect = 0.4\n",
    "             # Add more rules for '3-stops' etc. if needed\n",
    "\n",
    "        # --- 4. Combine Factors ---\n",
    "        estimated_demand = base_potential * price_effect * days_left_effect * class_effect * stops_effect\n",
    "\n",
    "        # --- 5. Add Stochasticity ---\n",
    "        # Introduce random noise to make demand less predictable\n",
    "        noise_factor = np.random.normal(loc=1.0, scale=0.25) # +/- 25% variation around the estimate\n",
    "        final_demand = estimated_demand * noise_factor\n",
    "\n",
    "        # --- 6. Constraints ---\n",
    "        # Ensure demand is non-negative and an integer\n",
    "        final_demand = max(0, round(final_demand))\n",
    "\n",
    "        # Optional: Log the calculation details for debugging\n",
    "        logging.log(logging.DEBUG - 1, # Use lower debug level\n",
    "                    f\"Demand Calc ({route}-{airline}, {date.date()}, P:{price:.0f}): \"\n",
    "                    f\"Base={base_potential:.1f}, P_Eff={price_effect:.2f}, Days_Eff={days_left_effect:.2f}, \"\n",
    "                    f\"Cls_Eff={class_effect:.1f}, Stp_Eff={stops_effect:.1f}, Noise={noise_factor:.2f} -> Demand={final_demand}\")\n",
    "\n",
    "        return int(final_demand)\n",
    "\n",
    "\n",
    "    def _calculate_operational_cost(self, flight_index: int) -> float:\n",
    "        \"\"\"\n",
    "        (HEURISTIC MODEL) Calculates the approximate operational cost *per seat* for a flight.\n",
    "        Uses merged aircraft specs and fallback values.\n",
    "\n",
    "        Args:\n",
    "            flight_index: Index of the flight.\n",
    "\n",
    "        Returns:\n",
    "            Estimated cost per seat for the flight.\n",
    "        \"\"\"\n",
    "        route, airline = self.flights[flight_index]\n",
    "        date = self.current_date\n",
    "        norm_date = date.normalize()\n",
    "\n",
    "        # --- Get Flight Context (Aircraft Type, Duration) ---\n",
    "        day_data = self.data_by_date.get(norm_date)\n",
    "        aircraft_type = 'Unknown' # Default if not found\n",
    "        duration_hours = 3.0      # Default duration if not found\n",
    "        if day_data is not None:\n",
    "             rows = day_data[(day_data['Route'] == route) & (day_data['Airline'] == airline)]\n",
    "             if not rows.empty:\n",
    "                 row = rows.iloc[0]\n",
    "                 # Get AircraftType if available (depends on successful merge in DataProcessor)\n",
    "                 aircraft_type = row.get('AircraftType', 'Unknown')\n",
    "                 # Get duration, ensuring it's positive\n",
    "                 duration_hours = row.get('Duration_in_hours', 3.0)\n",
    "                 duration_hours = max(0.1, duration_hours) # Prevent zero or negative duration\n",
    "\n",
    "        # --- Cost Components ---\n",
    "        # Fuel Cost = Fuel Price * Fuel Rate * Duration\n",
    "        fuel_price = self.data_processor.fuel_price_mean # Using fallback value\n",
    "        # Get fuel rate from stored dictionary, use default if model unknown\n",
    "        fuel_rate_l_per_hr = self.data_processor.aircraft_fuel_rates.get(aircraft_type, 3000) # Example default\n",
    "        total_fuel_cost = fuel_price * fuel_rate_l_per_hr * duration_hours\n",
    "\n",
    "        # Maintenance Cost = Hourly Rate * Duration\n",
    "        maint_rate_usd_per_hr = self.data_processor.maintenance_costs.get(aircraft_type, 1000) # Example default\n",
    "        total_maint_cost = maint_rate_usd_per_hr * duration_hours\n",
    "\n",
    "        # Other Fixed Costs (Placeholder for crew, landing fees, etc.)\n",
    "        other_fixed_costs = 5000 # Example fixed cost per flight\n",
    "\n",
    "        # --- Total Cost Calculation ---\n",
    "        total_flight_cost = total_fuel_cost + total_maint_cost + other_fixed_costs\n",
    "\n",
    "        # --- Cost Per Seat ---\n",
    "        # Divide total cost by capacity (handle potential division by zero)\n",
    "        cost_per_seat = total_flight_cost / self.seats_capacity if self.seats_capacity > 0 else total_flight_cost\n",
    "        # Ensure cost is non-negative\n",
    "        cost_per_seat = max(0.0, cost_per_seat)\n",
    "\n",
    "        # --- Cost Capping (Sanity Check) ---\n",
    "        # Prevent unrealistic scenarios where cost > high price. Cap at e.g., 150% of min possible price.\n",
    "        max_reasonable_cost = self.price_range[0] * 1.5\n",
    "        cost_per_seat = min(cost_per_seat, max_reasonable_cost)\n",
    "\n",
    "        logging.log(logging.DEBUG - 1, # Use lower debug level\n",
    "                    f\"Cost Calc ({route}-{airline}, AC:{aircraft_type}, Dur:{duration_hours:.1f}h): \"\n",
    "                    f\"Fuel({fuel_rate_l_per_hr:.0f}L/h*${fuel_price:.1f}/L)={(fuel_price * fuel_rate_l_per_hr * duration_hours):.0f}, \"\n",
    "                    f\"Maint(${maint_rate_usd_per_hr:.0f}/h)={(maint_rate_usd_per_hr * duration_hours):.0f}, \"\n",
    "                    f\"Fixed={other_fixed} -> Total={total_flight_cost:.0f}, PerSeat={cost_per_seat:.2f}\")\n",
    "\n",
    "        return cost_per_seat\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Main Environment Interaction Method\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def step(self, actions_neg1_to_1: np.ndarray) -> Tuple[torch.Tensor, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment based on the agent's actions.\n",
    "        Accepts actions scaled between -1 and 1 (from Actor's Tanh output).\n",
    "\n",
    "        Args:\n",
    "            actions_neg1_to_1: A NumPy array of actions (scaled prices), one for each flight,\n",
    "                                in the range [-1, 1].\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - next_state (torch.Tensor): The state vector for the next day.\n",
    "            - reward (float): The total profit accumulated across all flights for the current day.\n",
    "            - done (bool): Boolean indicating if the simulation episode has ended.\n",
    "            - info (Dict): Dictionary containing auxiliary information (e.g., profit/demand per flight).\n",
    "        \"\"\"\n",
    "        # --- Action Validation ---\n",
    "        if len(actions_neg1_to_1) != self.n_flights:\n",
    "             logging.error(f\"Action size mismatch in step: Expected {self.n_flights}, got {len(actions_neg1_to_1)}.\")\n",
    "             # Fallback: Use default action (0 -> middle price) for missing/extra actions\n",
    "             corrected_actions = np.zeros(self.n_flights)\n",
    "             limit = min(len(actions_neg1_to_1), self.n_flights)\n",
    "             corrected_actions[:limit] = actions_neg1_to_1[:limit]\n",
    "             actions_neg1_to_1 = corrected_actions\n",
    "\n",
    "        # --- State Initialization for the Step ---\n",
    "        total_profit_today = 0.0\n",
    "        # Info dictionary to store step details (useful for logging/analysis)\n",
    "        step_info = {'profits': {}, 'demands': {}, 'seats_sold': {}, 'prices': {}, 'costs': {}}\n",
    "\n",
    "        # --- Scale Actions to Prices ---\n",
    "        # Convert actions from [-1, 1] range to the actual price range [min_price, max_price]\n",
    "        # Formula: price = min_price + (action + 1) * 0.5 * (max_price - min_price)\n",
    "        prices_today = self.price_range[0] + (actions_neg1_to_1 + 1.0) * 0.5 * self.price_delta\n",
    "\n",
    "        # Clip prices *after* scaling to ensure they strictly stay within the defined bounds\n",
    "        prices_today = np.clip(prices_today, self.price_range[0], self.price_range[1])\n",
    "\n",
    "        # --- Simulate Each Flight for the Day ---\n",
    "        for i in range(self.n_flights):\n",
    "            route, airline = self.flights[i]\n",
    "            flight_key = (route, airline) # Unique identifier for dictionaries\n",
    "            price = prices_today[i]\n",
    "\n",
    "            # Store the actual price set for this flight (for state calculation next step)\n",
    "            self.current_prices[i] = price\n",
    "\n",
    "            # --- Simulate Market Response ---\n",
    "            # Calculate demand based on the set price using the heuristic model\n",
    "            demand = self._calculate_demand(flight_index=i, price=price)\n",
    "            # Calculate the operational cost per seat for this flight\n",
    "            cost_per_seat = self._calculate_operational_cost(flight_index=i)\n",
    "\n",
    "            # --- Calculate Sales ---\n",
    "            # Determine available seats for this flight\n",
    "            available_seats = self.seats_capacity - self.seats_sold[i]\n",
    "            # Seats sold is the minimum of demand and available seats (cannot sell more than available)\n",
    "            # Also ensure non-negative values.\n",
    "            sold_today = min(max(0, demand), max(0, available_seats))\n",
    "\n",
    "            # --- Calculate Profit ---\n",
    "            # Profit for this flight = (Revenue per seat - Cost per seat) * Seats sold\n",
    "            profit = (price - cost_per_seat) * sold_today\n",
    "\n",
    "            # --- Update Environment State ---\n",
    "            # Increment seats sold for this flight\n",
    "            self.seats_sold[i] += sold_today\n",
    "            # Add this flight's profit to the daily total\n",
    "            total_profit_today += profit\n",
    "\n",
    "            # --- Store Information ---\n",
    "            # Record details for this flight in the info dictionary\n",
    "            step_info['profits'][flight_key] = profit\n",
    "            step_info['demands'][flight_key] = demand\n",
    "            step_info['seats_sold'][flight_key] = sold_today\n",
    "            step_info['prices'][flight_key] = price\n",
    "            step_info['costs'][flight_key] = cost_per_seat\n",
    "\n",
    "        # --- Advance Simulation Time ---\n",
    "        self.current_step += 1\n",
    "        self.current_date += pd.Timedelta(days=1)\n",
    "        # Check if the episode termination condition is met\n",
    "        done = self.current_date > self.end_date\n",
    "\n",
    "        # --- Get Next State ---\n",
    "        # Calculate the state vector for the *next* timestep\n",
    "        next_state_vector = self._get_state_vector()\n",
    "\n",
    "        if done:\n",
    "            logging.debug(f\"Episode finished at step {self.current_step}.\")\n",
    "\n",
    "        # --- Define Reward ---\n",
    "        # The reward for the agent is the total profit achieved across all flights on this day\n",
    "        reward = total_profit_today\n",
    "\n",
    "        return next_state_vector, reward, done, step_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initialize Environment\n",
    " *Create an instance of the environment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = None\n",
    "if 'processed_data' in locals() and not processed_data.empty:\n",
    "    try:\n",
    "        env = AirlinePricingEnv(processed_data, data_processor, config, device)\n",
    "        print(f\"\\n Environment initialized successfully. State size: {env.state_size}, Action size: {env.action_size}\")\n",
    "    except Exception as e: print(f\" Error initializing environment: {e}\"); logging.error(\"Env init failed.\", exc_info=True)\n",
    "else: print(\" Env init failed: Processed data missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TD3 Agent Definition\n",
    " ---\n",
    " *Define the Actor, Critic networks, Replay Buffer, and the main TD3 Agent logic.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Actor Network (`Actor`)\n",
    "**Maps** a given state observation to a deterministic action (scaled prices in `[-1, 1]`). This represents the agent's current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %% [markdown]\n",
    "# ### 5.1 Actor Network (`Actor`) - with Comments\n",
    "# **Maps** a given state observation to a deterministic action (scaled prices in `[-1, 1]`). This represents the agent's current policy.\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Often used for activations\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor Network for TD3.\n",
    "\n",
    "    Takes a state representation as input and outputs a deterministic action\n",
    "    (or vector of actions) bounded within a specified range (typically [-1, 1]\n",
    "    using tanh activation). This network learns the policy function (s).\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int], max_action: float):\n",
    "        \"\"\"\n",
    "        Initializes the Actor network layers.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimensionality of the input state space.\n",
    "            action_dim (int): Dimensionality of the output action space (number of continuous actions).\n",
    "                               In this case, action_dim = n_flights.\n",
    "            hidden_dims (List[int]): A list defining the number of neurons in each hidden layer.\n",
    "                                     Example: [256, 128] creates two hidden layers.\n",
    "            max_action (float): The maximum absolute value of the action output. Used to scale\n",
    "                                the output of the final tanh activation layer. Should typically be 1.0\n",
    "                                if the environment expects actions in [-1, 1].\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__() # Initialize the parent nn.Module class\n",
    "\n",
    "        # Store max_action for scaling the output\n",
    "        self.max_action = max_action\n",
    "        logging.debug(f\"Initializing Actor: StateDim={state_dim}, ActionDim={action_dim}, Hidden={hidden_dims}, MaxAction={max_action}\")\n",
    "\n",
    "        # --- Build Network Layers ---\n",
    "        layers = []\n",
    "        # Define the input dimension for the first layer\n",
    "        input_dim = state_dim\n",
    "\n",
    "        # Dynamically create hidden layers based on the hidden_dims list\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # Linear layer: maps from input_dim to hidden_dim\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            # Activation function (ReLU is common)\n",
    "            layers.append(nn.ReLU())\n",
    "            # Update input_dim for the next layer\n",
    "            input_dim = hidden_dim\n",
    "            logging.debug(f\"  Added Actor hidden layer: Linear({layers[-2].in_features}, {layers[-2].out_features}), ReLU\")\n",
    "\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        # Final linear layer maps the last hidden layer's output to the action dimension\n",
    "        layers.append(nn.Linear(input_dim, action_dim))\n",
    "        # Tanh activation function: squashes the output to the range [-1, 1]\n",
    "        # This is standard practice for continuous action spaces in algorithms like TD3/DDPG.\n",
    "        layers.append(nn.Tanh())\n",
    "        logging.debug(f\"  Added Actor output layer: Linear({layers[-2].in_features}, {layers[-2].out_features}), Tanh\")\n",
    "\n",
    "\n",
    "        # Create the sequential model from the defined layers\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "     # weight initialization method\n",
    "    def _init_weights(self, module):\n",
    "         if isinstance(module, nn.Linear):\n",
    "             # Example using Xavier uniform initialization\n",
    "             torch.nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "             if module.bias is not None:\n",
    "                 module.bias.data.fill_(0.01) # Small non-zero bias\n",
    "\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Actor network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): A batch of state observations. Shape: (batch_size, state_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A batch of actions, scaled to the range [-max_action, max_action].\n",
    "                          Shape: (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        # Pass the state through the sequential network\n",
    "        # Output will be in the range [-1, 1] due to the Tanh activation\n",
    "        tanh_output = self.network(state)\n",
    "\n",
    "        # Scale the output from [-1, 1] to [-max_action, max_action]\n",
    "        # This allows flexibility if the environment requires actions outside [-1, 1],\n",
    "        # although typically max_action is set to 1.0.\n",
    "        scaled_action = self.max_action * tanh_output\n",
    "\n",
    "        return scaled_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Critic Network (`Critic`)\n",
    "**Estimates** the action-value function `Q(s, a)` for a given state `s` and action `a`. TD3 uses **two** Critic networks (hence \"Twin\") to mitigate Q-value overestimation. Both Critics have the same architecture but are trained independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Twin Critic Network for TD3.\n",
    "\n",
    "    Contains two separate Q-network estimators (Q1 and Q2). Each network takes\n",
    "    both the state and the action as input and outputs a single scalar value\n",
    "    representing the estimated Q-value Q(s, a). Using the minimum of the two\n",
    "    Q-values during target calculation helps to reduce overestimation bias.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"\n",
    "        Initializes the two Critic networks (Q1 and Q2).\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimensionality of the input state space.\n",
    "            action_dim (int): Dimensionality of the input action space.\n",
    "            hidden_dims (List[int]): A list defining the number of neurons in each hidden layer\n",
    "                                     for *both* Q1 and Q2 networks. Example: [256, 128].\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__() # Initialize the parent nn.Module class\n",
    "        logging.debug(f\"Initializing Critic (Twin): StateDim={state_dim}, ActionDim={action_dim}, Hidden={hidden_dims}\")\n",
    "\n",
    "        # --- Network 1 (Q1) ---\n",
    "        layers1 = []\n",
    "        # The input dimension for the first layer is the sum of state and action dimensions\n",
    "        input_dim1 = state_dim + action_dim\n",
    "\n",
    "        # Build hidden layers for Q1\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers1.append(nn.Linear(input_dim1, hidden_dim))\n",
    "            layers1.append(nn.ReLU()) # Standard activation\n",
    "            input_dim1 = hidden_dim # Update input dim for the next layer\n",
    "            logging.debug(f\"  Added Critic Q1 hidden layer: Linear({layers1[-2].in_features}, {layers1[-2].out_features}), ReLU\")\n",
    "\n",
    "        # Output layer for Q1: Outputs a single scalar Q-value\n",
    "        layers1.append(nn.Linear(input_dim1, 1))\n",
    "        logging.debug(f\"  Added Critic Q1 output layer: Linear({layers1[-1].in_features}, 1)\")\n",
    "        # Create the sequential model for Q1\n",
    "        self.q1_network = nn.Sequential(*layers1)\n",
    "\n",
    "\n",
    "        # --- Network 2 (Q2) ---\n",
    "        # Build Q2 with the *same architecture* but separate weights\n",
    "        layers2 = []\n",
    "        input_dim2 = state_dim + action_dim # Reset input dimension\n",
    "\n",
    "        # Build hidden layers for Q2\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers2.append(nn.Linear(input_dim2, hidden_dim))\n",
    "            layers2.append(nn.ReLU())\n",
    "            input_dim2 = hidden_dim\n",
    "            logging.debug(f\"  Added Critic Q2 hidden layer: Linear({layers2[-2].in_features}, {layers2[-2].out_features}), ReLU\")\n",
    "\n",
    "        # Output layer for Q2: Outputs a single scalar Q-value\n",
    "        layers2.append(nn.Linear(input_dim2, 1))\n",
    "        logging.debug(f\"  Added Critic Q2 output layer: Linear({layers2[-1].in_features}, 1)\")\n",
    "        # Create the sequential model for Q2\n",
    "        self.q2_network = nn.Sequential(*layers2)\n",
    "\n",
    "        # Apply shared or separate weight initialization\n",
    "        self.apply(self._init_weights) # Apply initialization to both networks\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "         if isinstance(module, nn.Linear):\n",
    "             # Example using Kaiming uniform initialization for ReLU\n",
    "             torch.nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5)) # a=math.sqrt(5) is default for LeakyReLU, adjust if needed\n",
    "             if module.bias is not None:\n",
    "                 # Initialize bias based on fan_in\n",
    "                 fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n",
    "                 bound = 1 / math.sqrt(fan_in)\n",
    "                 torch.nn.init.uniform_(module.bias, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Defines the forward pass for both Critic networks.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): A batch of state observations. Shape: (batch_size, state_dim)\n",
    "            action (torch.Tensor): A batch of actions corresponding to the states.\n",
    "                                   Shape: (batch_size, action_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the Q-value estimates\n",
    "                                               from both critics: (q1, q2).\n",
    "                                               Each tensor has shape: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Concatenate state and action tensors along the feature dimension (dim=1)\n",
    "        # This creates the combined input [s, a] for the Q-networks.\n",
    "        state_action_input = torch.cat([state, action], dim=1)\n",
    "\n",
    "        # Pass the combined input through each Q-network independently\n",
    "        q1 = self.q1_network(state_action_input)\n",
    "        q2 = self.q2_network(state_action_input)\n",
    "\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Helper method to get the Q-value estimate only from the first Critic (Q1).\n",
    "        Used in the Actor loss calculation in TD3.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): A batch of state observations.\n",
    "            action (torch.Tensor): A batch of actions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The Q-value estimates from Critic 1. Shape: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Concatenate state and action\n",
    "        state_action_input = torch.cat([state, action], dim=1)\n",
    "        # Pass through only the Q1 network\n",
    "        q1 = self.q1_network(state_action_input)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Replay Buffer (`TD3Agent`)\n",
    "**Combines** the Actor, Critics, target networks, replay buffer, and implements the core TD3 learning algorithm logic, including delayed policy updates, target policy smoothing, and clipped double-Q learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Agent:\n",
    "    \"\"\"\n",
    "    Twin Delayed Deep Deterministic Policy Gradient (TD3) Agent.\n",
    "\n",
    "    Implements the TD3 algorithm, which builds upon DDPG by introducing:\n",
    "    1. Clipped Double Q-Learning: Uses two Critic networks and takes the minimum\n",
    "       target Q-value to reduce overestimation bias.\n",
    "    2. Delayed Policy Updates: Updates the Actor less frequently than the Critics\n",
    "       to allow Q-value estimates to stabilize.\n",
    "    3. Target Policy Smoothing: Adds noise to the target Actor's actions during\n",
    "       target Q-value calculation to smooth the value landscape.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, max_action: float, config: Config, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initializes the TD3 Agent, including networks, optimizers, and replay buffer.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimensionality of the state space.\n",
    "            action_dim (int): Dimensionality of the action space.\n",
    "            max_action (float): The maximum absolute value for actions output by the Actor's tanh.\n",
    "            config (Config): Configuration object containing hyperparameters.\n",
    "            device (torch.device): The device (CPU or GPU) for tensor operations.\n",
    "        \"\"\"\n",
    "        if state_dim <= 0: raise ValueError(f\"Invalid state_dim: {state_dim}\")\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "        # Store max_action, used for clipping noise and action selection bounds\n",
    "        self.max_action = max_action\n",
    "        # Store relevant hyperparameters from config\n",
    "        self.gamma = config.gamma          # Discount factor\n",
    "        self.tau = config.tau            # Soft update factor for target networks\n",
    "        self.policy_noise = config.policy_noise # Std dev for target policy smoothing noise\n",
    "        self.noise_clip = config.noise_clip      # Clipping range for target policy noise\n",
    "        self.policy_freq = config.policy_freq    # Frequency of delayed policy updates\n",
    "        self.batch_size = config.batch_size      # Training batch size\n",
    "\n",
    "        logging.info(\"Initializing TD3 Agent components...\")\n",
    "\n",
    "        # --- Initialize Networks ---\n",
    "        # Actor Network (outputs actions)\n",
    "        self.actor = Actor(state_dim, action_dim, config.actor_hidden_dims, max_action).to(device)\n",
    "        # Target Actor Network (a slow-moving average of the main actor)\n",
    "        self.actor_target = copy.deepcopy(self.actor) # Initialize target same as main actor\n",
    "        # Optimizer for the Actor network\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)\n",
    "        logging.info(\"  Actor networks (main & target) and Adam optimizer created.\")\n",
    "\n",
    "        # Critic Networks (Twin Critics Q1, Q2 estimate Q(s,a))\n",
    "        self.critic = Critic(state_dim, action_dim, config.critic_hidden_dims).to(device)\n",
    "        # Target Critic Network (slow-moving average)\n",
    "        self.critic_target = copy.deepcopy(self.critic) # Initialize target same as main critic\n",
    "        # Optimizer for *both* Critic networks (parameters are optimized together)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)\n",
    "        logging.info(\"  Critic networks (main & target) and Adam optimizer created.\")\n",
    "\n",
    "        # --- Initialize Replay Buffer ---\n",
    "        self.replay_buffer = ReplayBuffer(state_dim, action_dim, config.buffer_size, device)\n",
    "        logging.info(\"  Replay buffer created.\")\n",
    "\n",
    "        # --- Training Counter ---\n",
    "        # Keep track of total training iterations for delayed policy update schedule\n",
    "        self.total_it: int = 0\n",
    "\n",
    "        logging.info(\"TD3 Agent initialized successfully.\")\n",
    "\n",
    "\n",
    "    def select_action(self, state: np.ndarray, exploration_noise: float = 0.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state using the Actor network.\n",
    "        Adds Gaussian noise for exploration during training.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The current state observation.\n",
    "            exploration_noise (float): Standard deviation of Gaussian noise to add\n",
    "                                       for exploration. Set to 0 for deterministic evaluation.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The selected action, clipped to the range [-max_action, max_action].\n",
    "        \"\"\"\n",
    "        # 1. Prepare state tensor\n",
    "        # Convert NumPy state to PyTorch tensor on the correct device\n",
    "        # Add batch dimension (unsqueeze(0)) as the network expects batches\n",
    "        state_tensor = torch.tensor(state.reshape(1, -1), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # 2. Get action from Actor network\n",
    "        self.actor.eval() # Set actor to evaluation mode (disables dropout if used)\n",
    "        with torch.no_grad(): # Disable gradient calculation for inference\n",
    "            action = self.actor(state_tensor).cpu().numpy().flatten() # Get action, move to CPU, flatten\n",
    "        self.actor.train() # Set actor back to training mode\n",
    "\n",
    "        # 3. Add exploration noise (if specified)\n",
    "        if exploration_noise > 0:\n",
    "            # Sample noise from a Gaussian distribution\n",
    "            noise = np.random.normal(0, self.max_action * exploration_noise, size=self.action_dim)\n",
    "            # Add noise to the deterministic action\n",
    "            action = action + noise\n",
    "            logging.log(logging.DEBUG - 1, f\"Action with noise {exploration_noise:.2f}: {action}\")\n",
    "\n",
    "\n",
    "        # 4. Clip action to valid range\n",
    "        # Ensure the final action (with or without noise) stays within [-max_action, max_action]\n",
    "        clipped_action = np.clip(action, -self.max_action, self.max_action)\n",
    "\n",
    "        return clipped_action\n",
    "\n",
    "\n",
    "    def train(self) -> Tuple[Optional[float], Optional[float]]:\n",
    "        \"\"\"\n",
    "        Performs a single TD3 training update step.\n",
    "        Samples a batch from the replay buffer, calculates losses, and updates\n",
    "        Actor and Critic networks.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Optional[float], Optional[float]]: A tuple containing the critic loss\n",
    "            and actor loss for this step (actor loss might be None if it wasn't updated\n",
    "            due to policy delay). Returns (None, None) if buffer size is insufficient.\n",
    "        \"\"\"\n",
    "        # Increment the total iteration counter\n",
    "        self.total_it += 1\n",
    "\n",
    "        # 1. Check if buffer has enough samples for a batch\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "             logging.debug(f\"Skipping training step {self.total_it}: Buffer size ({self.replay_buffer.size}) < Batch size ({self.batch_size})\")\n",
    "             return None, None # Not enough samples to form a batch\n",
    "\n",
    "        # 2. Sample a mini-batch from the replay buffer\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        state = batch['state']\n",
    "        action = batch['action'] # Action actually taken, stored in buffer\n",
    "        reward = batch['reward']\n",
    "        next_state = batch['next_state']\n",
    "        done = batch['done'] # Done flags (as floats 0.0 or 1.0)\n",
    "\n",
    "        # --- Critic Loss Calculation and Update ---\n",
    "        with torch.no_grad(): # Operations inside this block don't track gradients\n",
    "            # 3. Select next action using the *target* Actor network: a' = pi_target(s')\n",
    "            next_action = self.actor_target(next_state)\n",
    "\n",
    "            # 4. Apply Target Policy Smoothing: Add clipped noise to the target action\n",
    "            # Sample noise from Gaussian distribution, clamp it, add to action\n",
    "            policy_noise_tensor = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            # Add noise and clip the resulting action to the valid range [-max_action, max_action]\n",
    "            next_action = (next_action + policy_noise_tensor).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # 5. Compute the target Q-value using Clipped Double-Q Learning:\n",
    "            # Get Q-values for the noisy next action from *both* target Critic networks\n",
    "            target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
    "            # Take the minimum of the two target Q-values to mitigate overestimation\n",
    "            target_q_min = torch.min(target_q1, target_q2)\n",
    "            # Calculate the final TD target: y = r + gamma * min(Q1_target, Q2_target) * (1 - done)\n",
    "            # (1 - done) ensures the target is just 'r' if the state was terminal\n",
    "            td_target = reward + (1.0 - done) * self.gamma * target_q_min\n",
    "\n",
    "        # 6. Get current Q estimates from the main Critic networks for the batch's (s, a) pairs\n",
    "        current_q1, current_q2 = self.critic(state, action)\n",
    "\n",
    "        # 7. Compute the Critic loss: Mean Squared Error between current Q estimates and the TD target\n",
    "        # Sum the MSE losses for both critics\n",
    "        critic_loss = F.mse_loss(current_q1, td_target) + F.mse_loss(current_q2, td_target)\n",
    "        critic_loss_val = critic_loss.item() # Store scalar value for logging\n",
    "        logging.log(logging.DEBUG - 1, f\"Iter {self.total_it}: Critic Loss={critic_loss_val:.4f}\")\n",
    "\n",
    "\n",
    "        # 8. Optimize the Critic networks\n",
    "        self.critic_optimizer.zero_grad() # Reset gradients\n",
    "        critic_loss.backward() # Compute gradients\n",
    "        # Optional: Clip critic gradients if needed\n",
    "        # nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
    "        self.critic_optimizer.step() # Update critic weights\n",
    "\n",
    "        # --- Delayed Actor Loss Calculation and Update ---\n",
    "        actor_loss_val = None # Initialize actor loss as None for this step\n",
    "        # Update the Actor and target networks only every 'policy_freq' iterations\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # 9. Compute Actor loss (Policy Gradient part)\n",
    "            # The Actor aims to output actions that maximize the Q-value estimated by Critic 1.\n",
    "            # Loss is the negative mean Q1 value for the actions the Actor *currently* proposes for the batch states.\n",
    "            actor_proposed_actions = self.actor(state)\n",
    "            q1_for_actor_loss = self.critic.Q1(state, actor_proposed_actions) # Use the Q1() helper method\n",
    "            actor_loss = -q1_for_actor_loss.mean() # Maximize Q1 -> Minimize -Q1\n",
    "            actor_loss_val = actor_loss.item() # Store scalar value\n",
    "            logging.log(logging.DEBUG - 1, f\"Iter {self.total_it}: Actor Loss={actor_loss_val:.4f} (Policy Update)\")\n",
    "\n",
    "\n",
    "            # 10. Optimize the Actor network\n",
    "            self.actor_optimizer.zero_grad() # Reset gradients\n",
    "            actor_loss.backward() # Compute gradients\n",
    "            # Optional: Clip actor gradients if needed\n",
    "            # nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "            self.actor_optimizer.step() # Update actor weights\n",
    "\n",
    "            # 11. Soft update the target networks (Actor and Critic)\n",
    "            self.soft_update_target_networks()\n",
    "\n",
    "        # Return losses for monitoring purposes\n",
    "        return critic_loss_val, actor_loss_val\n",
    "\n",
    "\n",
    "    def soft_update_target_networks(self):\n",
    "        \"\"\"\n",
    "        Performs a soft update of the target network parameters.\n",
    "        target_weights = tau * local_weights + (1 - tau) * target_weights\n",
    "        \"\"\"\n",
    "        logging.log(logging.DEBUG - 2, \"Performing soft target network update...\") # Use lower debug level\n",
    "        # Update Critic target network\n",
    "        for target_param, local_param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "        # Update Actor target network\n",
    "        for target_param, local_param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, actor_path: str, critic_path: str):\n",
    "        \"\"\"Saves the state dictionaries of the Actor, Critic, and their optimizers.\"\"\"\n",
    "        logging.info(f\"Saving TD3 agent models -> Actor: {actor_path}, Critic: {critic_path}\")\n",
    "        try:\n",
    "             # Save Actor state, optimizer state, and training iteration count\n",
    "             torch.save({\n",
    "                 'actor_state_dict': self.actor.state_dict(),\n",
    "                 'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "                 'total_it': self.total_it # Save training progress\n",
    "                 }, actor_path)\n",
    "             # Save Critic state and optimizer state\n",
    "             torch.save({\n",
    "                 'critic_state_dict': self.critic.state_dict(),\n",
    "                 'critic_optimizer_state_dict': self.critic_optimizer.state_dict()\n",
    "                 }, critic_path)\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error saving agent state: {e}\", exc_info=True)\n",
    "\n",
    "    def load(self, actor_path: str, critic_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Loads the agent's state (networks and optimizers) from specified file paths.\n",
    "        Also re-synchronizes the target networks.\n",
    "\n",
    "        Args:\n",
    "            actor_path (str): Path to the saved Actor checkpoint file.\n",
    "            critic_path (str): Path to the saved Critic checkpoint file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if both Actor and Critic loaded successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        loaded_actor = False\n",
    "        loaded_critic = False\n",
    "\n",
    "        # --- Load Actor ---\n",
    "        if not os.path.exists(actor_path):\n",
    "            logging.error(f\"Actor checkpoint file not found: {actor_path}\")\n",
    "        else:\n",
    "            try:\n",
    "                logging.info(f\"Loading Actor state from: {actor_path}\")\n",
    "                checkpoint = torch.load(actor_path, map_location=self.device) # Load to specified device\n",
    "\n",
    "                self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "                self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "                self.total_it = checkpoint.get('total_it', 0) # Load training iterations\n",
    "\n",
    "                # Important: Re-initialize target network from the loaded actor weights\n",
    "                self.actor_target = copy.deepcopy(self.actor)\n",
    "                self.actor.to(self.device) # Ensure loaded model is on device\n",
    "                self.actor_target.to(self.device)\n",
    "\n",
    "                # Move optimizer states to the correct device (important if loading CPU model to GPU or vice-versa)\n",
    "                for state in self.actor_optimizer.state.values():\n",
    "                    for k, v in state.items():\n",
    "                        if isinstance(v, torch.Tensor): state[k] = v.to(self.device)\n",
    "                loaded_actor = True\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading Actor state from {actor_path}: {e}\", exc_info=True)\n",
    "\n",
    "        # --- Load Critic ---\n",
    "        if not os.path.exists(critic_path):\n",
    "            logging.error(f\"Critic checkpoint file not found: {critic_path}\")\n",
    "        else:\n",
    "            try:\n",
    "                logging.info(f\"Loading Critic state from: {critic_path}\")\n",
    "                checkpoint = torch.load(critic_path, map_location=self.device)\n",
    "\n",
    "                self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "                self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "\n",
    "                # Important: Re-initialize target network from the loaded critic weights\n",
    "                self.critic_target = copy.deepcopy(self.critic)\n",
    "                self.critic.to(self.device) # Ensure loaded model is on device\n",
    "                self.critic_target.to(self.device)\n",
    "\n",
    "                 # Move optimizer states to the correct device\n",
    "                for state in self.critic_optimizer.state.values():\n",
    "                    for k, v in state.items():\n",
    "                        if isinstance(v, torch.Tensor): state[k] = v.to(self.device)\n",
    "                loaded_critic = True\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading Critic state from {critic_path}: {e}\", exc_info=True)\n",
    "\n",
    "        # --- Final Checks ---\n",
    "        if loaded_actor and loaded_critic:\n",
    "            logging.info(\"TD3 Agent state loaded successfully.\")\n",
    "            # Set networks to appropriate modes after loading\n",
    "            self.actor.train()\n",
    "            self.critic.train()\n",
    "            self.actor_target.eval()\n",
    "            self.critic_target.eval()\n",
    "            return True\n",
    "        else:\n",
    "            logging.error(\"TD3 Agent loading failed (Actor or Critic or both). Check logs.\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Initialize TD3 Agent\n",
    "*Create the agent instance if the environment is ready.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = None\n",
    "if 'env' in locals() and env is not None and env.state_size > 0:\n",
    "    try:\n",
    "        agent = TD3Agent(state_dim=env.state_size, action_dim=env.action_size, max_action=config.max_action_value, config=config, device=device)\n",
    "        print(\"\\n TD3Agent initialized.\")\n",
    "    except Exception as e: print(f\" Error initializing TD3Agent: {e}\"); logging.error(\"Agent init failed.\", exc_info=True)\n",
    "else: print(\" Cannot initialize agent: Environment not ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent Training\n",
    " ---\n",
    "*Live training dashboard and interactive controls.* Monitor and manage the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Performance Metrics Tracker\n",
    "**Tracks** key performance indicators during training and evaluation, such as rewards, losses, profits, prices, and load factors. Provides methods for logging data and plotting progress. Designed for use with the TD3 agent (tracks separate Actor/Critic losses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "class PerformanceMetrics:\n",
    "    \"\"\"\n",
    "    Tracks and visualizes performance metrics for the RL agent training process.\n",
    "\n",
    "    Stores per-step and per-episode data, calculates summary statistics,\n",
    "    and generates plots to monitor training progress and evaluation results.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config, n_flights: int):\n",
    "        \"\"\"\n",
    "        Initializes the PerformanceMetrics tracker.\n",
    "\n",
    "        Args:\n",
    "            config: The configuration object containing parameters like price range,\n",
    "                    capacity, validation frequency.\n",
    "            n_flights: The number of unique flights managed by the agent/environment.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.n_flights = n_flights\n",
    "        # Store relevant config parameters for calculations and plotting\n",
    "        self.price_range: Tuple[float, float] = config.price_range\n",
    "        self.seats_capacity: int = config.seats_capacity\n",
    "        logging.info(\"Initializing PerformanceMetrics tracker.\")\n",
    "\n",
    "        # --- Data Storage ---\n",
    "        # Use lists to store per-episode summary metrics dynamically\n",
    "        self.episode_rewards: List[float] = []         # Total reward (profit) per episode\n",
    "        self.episode_losses_critic: List[float] = []   # Average critic loss per episode\n",
    "        self.episode_losses_actor: List[float] = []    # Average actor loss per episode (when updated)\n",
    "        self.episode_profits: List[float] = []         # Same as episode_rewards if reward = profit\n",
    "        self.episode_avg_prices: List[float] = []      # Average price set across all flights/steps in episode\n",
    "        self.episode_load_factors: List[float] = []    # Average load factor (% seats sold) per episode\n",
    "        self.episode_steps: List[int] = []             # Number of simulation steps taken in each episode\n",
    "\n",
    "        # Store raw data for each step (can become large, consider sampling or limiting if memory is an issue)\n",
    "        self.all_step_data: List[Dict] = []\n",
    "\n",
    "        # Store results from periodic evaluation runs\n",
    "        self.eval_rewards: List[float] = []\n",
    "    def log_step(self, episode: int, step: int, reward: float,\n",
    "                 critic_loss: Optional[float], actor_loss: Optional[float], info: Dict):\n",
    "        \"\"\"\n",
    "        Logs data for a single environment step within an episode.\n",
    "\n",
    "        Args:\n",
    "            episode (int): The current episode index (0-based).\n",
    "            step (int): The current step index within the episode (0-based).\n",
    "            reward (float): The reward received for this step (daily total profit).\n",
    "            critic_loss (Optional[float]): The critic loss calculated in this step (if any).\n",
    "            actor_loss (Optional[float]): The actor loss calculated in this step (if any, due to policy delay).\n",
    "            info (Dict): Additional information dictionary returned by env.step().\n",
    "        \"\"\"\n",
    "        # Create a dictionary summarizing the key data for this step\n",
    "        step_summary = {\n",
    "            'episode': episode,\n",
    "            'step': step,\n",
    "            'reward': reward,\n",
    "            # Store losses, using NaN if None (makes averaging easier)\n",
    "            'critic_loss': critic_loss if critic_loss is not None else np.nan,\n",
    "            'actor_loss': actor_loss if actor_loss is not None else np.nan,\n",
    "            # Calculate average price set across all flights for this day\n",
    "            'avg_price_day': np.mean(list(info.get('prices', {}).values())) if info.get('prices') else np.nan,\n",
    "            # Calculate total seats sold across all flights for this day\n",
    "            'total_seats_sold_day': sum(info.get('seats_sold', {}).values())\n",
    "        }\n",
    "        # Append the summary to the list of all step data\n",
    "        self.all_step_data.append(step_summary)\n",
    "\n",
    "    def log_episode(self, episode_idx: int, total_steps: int):\n",
    "        \"\"\"\n",
    "        Calculates and logs summary metrics at the end of an episode using the stored step data.\n",
    "\n",
    "        Args:\n",
    "            episode_idx (int): The index of the just-completed episode (0-based).\n",
    "            total_steps (int): The total number of steps taken in this episode.\n",
    "        \"\"\"\n",
    "        # Filter the raw step data to get only the data for the specified episode\n",
    "        # This is more robust than assuming the last N items belong to the episode\n",
    "        episode_data = [d for d in self.all_step_data if d['episode'] == episode_idx]\n",
    "\n",
    "        # If no step data was recorded for this episode (e.g., if logging failed), log a warning and return\n",
    "        if not episode_data:\n",
    "            logging.warning(f\"No step data found for episode {episode_idx + 1}. Cannot log episode metrics.\")\n",
    "            # Append NaN/placeholders to keep list lengths consistent for plotting if needed\n",
    "            self.episode_rewards.append(np.nan); self.episode_losses_critic.append(np.nan); self.episode_losses_actor.append(np.nan)\n",
    "            self.episode_profits.append(np.nan); self.episode_avg_prices.append(np.nan); self.episode_load_factors.append(np.nan)\n",
    "            self.episode_steps.append(total_steps) # Still log the number of steps taken\n",
    "            return\n",
    "\n",
    "        # Convert the list of step dictionaries into a pandas DataFrame for easier aggregation\n",
    "        ep_df = pd.DataFrame(episode_data)\n",
    "\n",
    "        # Calculate episode summary statistics\n",
    "        total_reward = ep_df['reward'].sum()\n",
    "        # Calculate average losses, ignoring NaN values (where loss wasn't computed)\n",
    "        avg_critic_loss = ep_df['critic_loss'].mean()\n",
    "        avg_actor_loss = ep_df['actor_loss'].mean() # Will be NaN if actor never updated\n",
    "        total_profit = total_reward # Assuming reward is defined as profit\n",
    "        avg_daily_price = ep_df['avg_price_day'].mean() # Average of daily average prices\n",
    "        total_seats_sold_episode = ep_df['total_seats_sold_day'].sum() # Sum of seats sold each day\n",
    "\n",
    "        # Calculate total seat capacity offered during the episode\n",
    "        # Capacity = num_flights * seats_per_flight * num_days_in_episode\n",
    "        # Use len(ep_df) which is the actual number of steps/days simulated\n",
    "        total_capacity_episode = self.n_flights * self.seats_capacity * len(ep_df)\n",
    "        # Calculate load factor (proportion of offered seats that were sold)\n",
    "        load_factor = total_seats_sold_episode / total_capacity_episode if total_capacity_episode > 0 else 0.0\n",
    "\n",
    "        # --- Store Episode Summaries ---\n",
    "        # Append calculated metrics to their respective lists\n",
    "        self.episode_rewards.append(total_reward)\n",
    "        # Store 0 if average loss is NaN (e.g., no training steps occurred)\n",
    "        self.episode_losses_critic.append(avg_critic_loss if pd.notna(avg_critic_loss) else 0.0)\n",
    "        self.episode_losses_actor.append(avg_actor_loss if pd.notna(avg_actor_loss) else 0.0)\n",
    "        self.episode_profits.append(total_profit)\n",
    "        self.episode_avg_prices.append(avg_daily_price if pd.notna(avg_daily_price) else 0.0)\n",
    "        self.episode_load_factors.append(load_factor)\n",
    "        self.episode_steps.append(total_steps)\n",
    "\n",
    "        # Log a summary message to the console for immediate feedback\n",
    "        log_msg = (f\"Ep {episode_idx + 1}: Steps={total_steps}, Profit={total_profit:,.0f}, \"\n",
    "                   f\"AvgCritLoss={avg_critic_loss:.4f}, AvgActLoss={avg_actor_loss:.4f}, \"\n",
    "                   f\"AvgPrice={avg_daily_price:.0f}, LoadFactor={load_factor:.1%}\")\n",
    "        logging.info(log_msg)\n",
    "\n",
    "        # Optional: Prune older step data to manage memory usage if `all_step_data` grows too large\n",
    "        # E.g., keep only data for the last N episodes or implement a max size for `all_step_data`\n",
    "\n",
    "    def add_eval_result(self, reward: float):\n",
    "         \"\"\"\n",
    "         Stores the average reward obtained from an evaluation run.\n",
    "\n",
    "         Args:\n",
    "             reward (float): The average reward over the evaluation episodes.\n",
    "         \"\"\"\n",
    "         self.eval_rewards.append(reward)\n",
    "         logging.debug(f\"Added evaluation result: {reward:.2f}\")\n",
    "\n",
    "    def plot_training_progress(self, fig: plt.Figure, axs: np.ndarray, window: int = 10):\n",
    "        \"\"\"\n",
    "        Updates the provided Matplotlib figure and axes with the latest training metrics.\n",
    "        Designed to be called periodically (e.g., end of episode) for live dashboard updates.\n",
    "\n",
    "        Args:\n",
    "            fig (plt.Figure): The Matplotlib figure object for the dashboard.\n",
    "            axs (np.ndarray): NumPy array of Matplotlib axes objects (e.g., from plt.subplots).\n",
    "            window (int): The window size for calculating rolling means (smoothing).\n",
    "        \"\"\"\n",
    "        logging.info(\"Updating training progress dashboard...\")\n",
    "        if not self.episode_rewards:\n",
    "            print(\"No episode data recorded yet to plot.\")\n",
    "            return\n",
    "\n",
    "        # --- Prepare Data for Plotting ---\n",
    "        num_episodes = len(self.episode_rewards)\n",
    "        episodes_axis = np.arange(1, num_episodes + 1) # X-axis for plots (Episode number)\n",
    "\n",
    "        # Helper function for safe rolling mean calculation (handles NaN/Inf)\n",
    "        def rolling_mean(data, w):\n",
    "            series = pd.Series(data).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            return series.rolling(w, min_periods=1).mean() if not series.empty else pd.Series()\n",
    "\n",
    "        # Calculate smoothed versions of the metrics\n",
    "        profits_smooth = rolling_mean(self.episode_profits, window)\n",
    "        # Critic loss handling (get series, drop invalid, then smooth)\n",
    "        critic_loss_series = pd.Series(self.episode_losses_critic).replace([np.inf,-np.inf],np.nan).dropna()\n",
    "        critic_losses_smooth = critic_loss_series.rolling(window, min_periods=1).mean()\n",
    "        # Actor loss handling\n",
    "        actor_loss_series = pd.Series(self.episode_losses_actor).replace([np.inf,-np.inf],np.nan).dropna()\n",
    "        actor_losses_smooth = actor_loss_series.rolling(window, min_periods=1).mean()\n",
    "        # Other metrics\n",
    "        prices_smooth = rolling_mean(self.episode_avg_prices, window)\n",
    "        load_factor_smooth = rolling_mean(self.episode_load_factors, window)\n",
    "        steps_smooth = rolling_mean(self.episode_steps, window)\n",
    "\n",
    "        # --- Clear and Update Plots ---\n",
    "        # Clear previous plots from all axes to prepare for redraw\n",
    "        for row_axs in axs:\n",
    "            for ax in row_axs:\n",
    "                ax.clear()\n",
    "\n",
    "        # --- Plot 1: Total Profit ---\n",
    "        ax = axs[0, 0]\n",
    "        ax.plot(episodes_axis, self.episode_profits, color='lightblue', alpha=0.6, label='Raw Profit')\n",
    "        # Plot smoothed line using its own index + 1 to align with episode number\n",
    "        ax.plot(profits_smooth.index + 1, profits_smooth, color='blue', label=f'Smoothed (w={window})')\n",
    "        ax.set_ylabel('Total Profit per Episode')\n",
    "        ax.set_title('Episode Profit')\n",
    "        ax.legend(fontsize='small'); ax.grid(True)\n",
    "\n",
    "        # --- Plot 2: Critic and Actor Losses ---\n",
    "        ax = axs[0, 1]\n",
    "        # Plot raw critic loss points (use its own index + 1)\n",
    "        ax.plot(critic_loss_series.index + 1, critic_loss_series, color='lightcoral', alpha=0.4, linestyle='', marker='.', markersize=2, label='Raw Critic Loss')\n",
    "        # Plot smoothed critic loss\n",
    "        ax.plot(critic_losses_smooth.index + 1, critic_losses_smooth, color='red', label=f'Smooth Critic (w={window})')\n",
    "        ax.set_ylabel('Critic Loss (Log Scale)', color='red')\n",
    "        ax.set_yscale('log') # Log scale is often useful for losses\n",
    "        ax.tick_params(axis='y', labelcolor='red')\n",
    "        ax.grid(True, which='both', axis='y', linestyle=':', linewidth=0.5)\n",
    "        ax.legend(loc='upper left', fontsize='small')\n",
    "        ax.set_title('Training Losses')\n",
    "\n",
    "        # Create a secondary y-axis for Actor loss\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(actor_loss_series.index + 1, actor_loss_series, color='skyblue', alpha=0.4, linestyle='', marker='.', markersize=2, label='Raw Actor Loss')\n",
    "        ax2.plot(actor_losses_smooth.index + 1, actor_losses_smooth, color='deepskyblue', label=f'Smooth Actor (w={window})')\n",
    "        ax2.set_ylabel('Actor Loss', color='deepskyblue') # Note: Actor loss is typically negative (maximizing -Q)\n",
    "        ax2.tick_params(axis='y', labelcolor='deepskyblue')\n",
    "        ax2.legend(loc='upper right', fontsize='small')\n",
    "        # ax2.grid(True, which='both', axis='y', linestyle=':', linewidth=0.5) # Optional secondary grid\n",
    "\n",
    "        # --- Plot 3: Average Price ---\n",
    "        ax = axs[1, 0]\n",
    "        ax.plot(episodes_axis, self.episode_avg_prices, color='lightgreen', alpha=0.6, label='Raw Avg Price')\n",
    "        ax.plot(prices_smooth.index + 1, prices_smooth, color='green', label=f'Smoothed (w={window})')\n",
    "        # Add lines indicating the min/max price range from config\n",
    "        ax.axhline(self.price_range[0], color='grey', linestyle='--', alpha=0.7, label=f'Min Price ({self.price_range[0]:.0f})')\n",
    "        ax.axhline(self.price_range[1], color='grey', linestyle='--', alpha=0.7, label=f'Max Price ({self.price_range[1]:.0f})')\n",
    "        ax.set_ylabel('Average Price (Fare)')\n",
    "        ax.set_title('Average Daily Price (Fare)')\n",
    "        ax.legend(fontsize='small'); ax.grid(True)\n",
    "\n",
    "        # --- Plot 4: Load Factor ---\n",
    "        ax = axs[1, 1]\n",
    "        ax.plot(episodes_axis, self.episode_load_factors, color='thistle', alpha=0.6, label='Raw Load Factor')\n",
    "        ax.plot(load_factor_smooth.index + 1, load_factor_smooth, color='purple', label=f'Smoothed (w={window})')\n",
    "        ax.set_ylabel('Load Factor')\n",
    "        ax.set_title('Average Load Factor')\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}')) # Format y-axis as percentage\n",
    "        ax.set_ylim(bottom=-0.05, top=1.05) # Set bounds slightly outside 0-1\n",
    "        ax.legend(fontsize='small'); ax.grid(True)\n",
    "\n",
    "        # --- Plot 5: Episode Steps ---\n",
    "        ax = axs[2, 0]\n",
    "        ax.plot(episodes_axis, self.episode_steps, color='navajowhite', alpha=0.6, label='Raw Steps')\n",
    "        ax.plot(steps_smooth.index + 1, steps_smooth, color='orange', label=f'Smoothed (w={window})')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Number of Steps')\n",
    "        ax.set_title('Episode Length')\n",
    "        ax.legend(fontsize='small'); ax.grid(True)\n",
    "\n",
    "        # --- Plot 6: Evaluation Reward ---\n",
    "        ax = axs[2, 1]\n",
    "        if self.eval_rewards:\n",
    "            # X-axis represents the episode number when evaluation was performed\n",
    "            eval_episode_numbers = np.arange(1, len(self.eval_rewards) + 1) * self.config.validation_freq\n",
    "            ax.plot(eval_episode_numbers, self.eval_rewards, marker='o', linestyle='-', color='teal', label='Avg Eval Reward')\n",
    "            # ax.set_xticks(eval_episode_numbers) # Optional: set ticks exactly at eval points\n",
    "        ax.set_xlabel('Episode Number')\n",
    "        ax.set_ylabel('Avg Reward (Profit)')\n",
    "        ax.set_title('Validation Performance')\n",
    "        ax.legend(fontsize='small'); ax.grid(True)\n",
    "\n",
    "        # --- Final Touches ---\n",
    "        # Update the main figure title\n",
    "        fig.suptitle(f'TD3 Training Progress (Episode {num_episodes})', fontsize=14)\n",
    "        # Adjust layout to prevent labels/titles overlapping\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.96]) # Leave space for suptitle\n",
    "\n",
    "        # Redraw the canvas to show the updated plots\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events() # Ensure updates are processed in notebook environment\n",
    "        logging.debug(\"Training dashboard updated.\")\n",
    "\n",
    "    def get_final_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculates and returns a dictionary of final summary performance metrics\n",
    "        after training has finished.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing key summary metrics.\n",
    "        \"\"\"\n",
    "        # Return status if no training data exists\n",
    "        if not self.episode_rewards:\n",
    "             return {\"status\": \"No training data available\"}\n",
    "\n",
    "        # Helper function for safe mean calculation (handles NaN/Inf)\n",
    "        def safe_mean(data: List[float]) -> float:\n",
    "             series = pd.Series(data).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "             return series.mean() if not series.empty else 0.0\n",
    "\n",
    "        # Calculate final metrics using safe mean\n",
    "        avg_profit_last_10 = safe_mean(self.episode_profits[-10:])\n",
    "        # Calculate max profit safely\n",
    "        max_profit = pd.Series(self.episode_profits).replace([np.inf, -np.inf], np.nan).max()\n",
    "        avg_load_factor_last_10 = safe_mean(self.episode_load_factors[-10:])\n",
    "        avg_eval_reward = safe_mean(self.eval_rewards)\n",
    "\n",
    "        # Compile metrics into a dictionary\n",
    "        metrics_summary = {\n",
    "            'total_episodes_run': len(self.episode_rewards),\n",
    "            'avg_profit_last_10_eps': avg_profit_last_10,\n",
    "            'max_profit_episode': max_profit if pd.notna(max_profit) else 0.0,\n",
    "            'avg_load_factor_last_10_eps': avg_load_factor_last_10,\n",
    "            'avg_evaluation_reward': avg_eval_reward\n",
    "            # Add other relevant final stats if needed (e.g., final losses)\n",
    "        }\n",
    "        return metrics_summary\n",
    "if 'env' in locals() and env is not None:\n",
    "        try:\n",
    "             metrics_tracker = PerformanceMetrics(\n",
    "             config=config,\n",
    "             n_flights=env.n_flights\n",
    "             # price_range is taken directly from config inside the class\n",
    "             )\n",
    "             print(\" PerformanceMetrics tracker initialized.\")\n",
    "        except :\n",
    "            print(f\" Failed to initialize PerformanceMetrics tracker: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Live Training Dashboard Setup\n",
    "*Creates the plot area for live updates during training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axes for the dashboard *once*\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize the dashboard plot structure\n",
    "def create_dashboard_figure(figsize=(12, 8)):\n",
    "    plt.ioff() # Turn off interactive mode initially to prevent double plots\n",
    "    fig, axs = plt.subplots(3, 2, figsize=figsize)\n",
    "    fig.canvas.header_visible = False # Hide the default toolbar for cleaner look\n",
    "    fig.suptitle('Training Dashboard (Pending Start...)', fontsize=14)\n",
    "    plt.ion() # Turn interactive mode back on for updates\n",
    "    return fig, axs\n",
    "\n",
    "dashboard_fig, dashboard_axs = create_dashboard_figure()\n",
    "display(dashboard_fig.canvas) # Display the canvas area in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluation Function (`evaluate_model`)\n",
    " **Runs** the trained agent in the environment for a specified number of episodes **without exploration noise** to objectively assess its learned policy's performance (greedy execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(env: AirlinePricingEnv,\n",
    "                   agent: TD3Agent,\n",
    "                   config: Config,\n",
    "                   num_eval_episodes: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the agent's deterministic policy over a specified number of episodes.\n",
    "\n",
    "    Args:\n",
    "        env (AirlinePricingEnv): The instantiated environment to evaluate in.\n",
    "        agent (TD3Agent): The trained TD3 agent.\n",
    "        config (Config): The configuration object (used for simulation length).\n",
    "        num_eval_episodes (int): The number of episodes to run for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: The average total reward (profit) achieved across all evaluation episodes.\n",
    "               Returns -infinity if evaluation cannot proceed due to missing components.\n",
    "    \"\"\"\n",
    "    # --- Pre-Evaluation Checks ---\n",
    "    if not all([env, agent, config]):\n",
    "        logging.error(\"Evaluation cannot start. Environment, Agent, or Config is missing.\")\n",
    "        return -np.inf # Return a value indicating failure\n",
    "\n",
    "    logging.info(f\"Starting evaluation phase for {num_eval_episodes} episode(s)...\")\n",
    "\n",
    "    # --- Set Agent to Evaluation Mode ---\n",
    "    # This is crucial:\n",
    "    # 1. Disables dropout layers (if any).\n",
    "    # 2. Changes behavior of batch normalization layers (uses running stats instead of batch stats).\n",
    "    # 3. Ensures deterministic behavior for evaluation.\n",
    "    agent.actor.eval()\n",
    "\n",
    "    # --- Evaluation Loop ---\n",
    "    total_rewards = [] # List to store the total reward for each evaluation episode\n",
    "\n",
    "    for episode in range(num_eval_episodes):\n",
    "        # --- Episode Initialization ---\n",
    "        # Reset the environment to get the starting state (as NumPy)\n",
    "        state_np = env.reset().cpu().numpy()\n",
    "        episode_reward = 0.0 # Accumulator for the current episode's reward\n",
    "        done = False           # Flag to track episode termination\n",
    "\n",
    "        # Initialize progress bar for the evaluation episode\n",
    "        pbar = tqdm(total=config.simulation_length_days,\n",
    "                    desc=f\"Eval Episode {episode + 1}/{num_eval_episodes}\",\n",
    "                    leave=False) # Keep bar visible until loop finishes\n",
    "\n",
    "        # Disable gradient calculations during evaluation (improves performance and saves memory)\n",
    "        with torch.no_grad():\n",
    "            # --- Step Loop within Evaluation Episode ---\n",
    "            while not done:\n",
    "                # 1. Select Action Deterministically (Greedy Policy)\n",
    "                # Call agent's action selection method with exploration_noise=0.0\n",
    "                action = agent.select_action(state_np, exploration_noise=0.0)\n",
    "\n",
    "                # 2. Step the Environment using the chosen action\n",
    "                # Env expects action in [-1, 1] range\n",
    "                next_state_tensor, reward, done, info = env.step(action)\n",
    "                # Convert next state to NumPy for the next iteration's input\n",
    "                next_state_np = next_state_tensor.cpu().numpy()\n",
    "\n",
    "                # --- Update State and Accumulate Reward ---\n",
    "                state_np = next_state_np  # Transition to the next state\n",
    "                episode_reward += reward  # Add step reward to episode total\n",
    "\n",
    "                # --- Update Progress Bar ---\n",
    "                pbar.update(1)\n",
    "                # Display current accumulated profit for the episode\n",
    "                pbar.set_postfix({'Profit': f'{episode_reward:,.0f}'})\n",
    "\n",
    "                # --- Check for Episode Termination ---\n",
    "                # The 'done' flag comes directly from the environment step\n",
    "                if done:\n",
    "                    pbar.close() # Close the progress bar for this episode\n",
    "                    break        # Exit the inner step loop\n",
    "\n",
    "        # --- End of Evaluation Episode ---\n",
    "        # Store the total reward achieved in this episode\n",
    "        total_rewards.append(episode_reward)\n",
    "        logging.info(f\"  Evaluation Episode {episode + 1} finished. Total Reward: {episode_reward:,.2f}\")\n",
    "\n",
    "    # --- Post-Evaluation ---\n",
    "    # Set the Actor network back to training mode (enables dropout/batch norm training behavior if used)\n",
    "    agent.actor.train()\n",
    "\n",
    "    # Calculate the average reward across all evaluation episodes\n",
    "    average_reward = np.mean(total_rewards) if total_rewards else 0.0 # Handle case of zero episodes\n",
    "    logging.info(f\"Evaluation phase finished. Average Reward over {num_eval_episodes} episodes: {average_reward:,.2f}\")\n",
    "\n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Training Function (`train_model`) \n",
    " **Contains** the core logic for iterating through episodes, interacting with the environment using the TD3 agent, collecting experience, triggering agent learning updates, performing periodic validation, implementing early stopping, and updating the live dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env: AirlinePricingEnv,\n",
    "                agent: TD3Agent,\n",
    "                metrics: PerformanceMetrics,\n",
    "                config: Config,\n",
    "                live_fig: plt.Figure,           # Live dashboard figure object\n",
    "                live_axs: np.ndarray,           # Live dashboard axes array\n",
    "                writer: Optional[writer] = None): # TensorBoard writer\n",
    "    \"\"\"\n",
    "    Main training loop for the TD3 agent in the Airline Pricing Environment.\n",
    "\n",
    "    Args:\n",
    "        env: The instantiated AirlinePricingEnv.\n",
    "        agent: The instantiated TD3Agent.\n",
    "        metrics: The instantiated PerformanceMetrics tracker.\n",
    "        config: The configuration object with hyperparameters.\n",
    "        live_fig: The Matplotlib figure for the live dashboard.\n",
    "        live_axs: The Matplotlib axes array for the live dashboard.\n",
    "        writer: Optional TensorBoard SummaryWriter for logging.\n",
    "    \"\"\"\n",
    "    # --- Pre-Training Checks ---\n",
    "    # Ensure all necessary components are provided and valid\n",
    "    if not all([env, agent, metrics, config, live_fig, live_axs]):\n",
    "        logging.error(\"Training cannot start. Required components (env, agent, metrics, config, fig, axs) are missing or invalid.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Starting TD3 training for {config.n_episodes} episodes...\")\n",
    "    logging.info(f\"  Hyperparameters: Batch={config.batch_size}, ActorLR={config.actor_lr:.1e}, CriticLR={config.critic_lr:.1e}, PolicyFreq={config.policy_freq}\")\n",
    "    logging.info(f\"  Initial random exploration steps: {config.start_timesteps}\")\n",
    "    logging.info(f\"  Validation every {config.validation_freq} episodes, Patience={config.patience}\")\n",
    "\n",
    "    # --- Initialization ---\n",
    "    best_eval_reward = -np.inf     # Track the best average reward achieved during validation\n",
    "    patience_counter = 0         # Counter for early stopping mechanism\n",
    "    global_step_counter = 0      # Track total steps across all episodes for logging/buffer check\n",
    "\n",
    "    # --- Main Training Loop (over episodes) ---\n",
    "    for episode_idx in range(config.n_episodes):\n",
    "        # --- Episode Initialization ---\n",
    "        # Reset the environment to get the initial state (as NumPy array for buffer)\n",
    "        state_np = env.reset().cpu().numpy()\n",
    "        # Reset episode-specific trackers\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        episode_critic_losses = [] # Store critic losses for averaging this episode\n",
    "        episode_actor_losses = []  # Store actor losses for averaging this episode\n",
    "        start_time = time.time()   # Track episode duration\n",
    "\n",
    "        # Initialize progress bar for the current episode\n",
    "        pbar = tqdm(total=config.simulation_length_days,\n",
    "                    desc=f\"Ep {episode_idx + 1}/{config.n_episodes}\",\n",
    "                    leave=False) # leave=False removes bar after completion\n",
    "\n",
    "        # --- Inner Loop (over steps within an episode) ---\n",
    "        while True:\n",
    "            # Increment counters\n",
    "            global_step_counter += 1\n",
    "            episode_steps += 1\n",
    "\n",
    "            # 1. Select Action based on exploration strategy\n",
    "            if global_step_counter < config.start_timesteps:\n",
    "                # Initial phase: Take purely random actions sampled uniformly from [-max_action, max_action]\n",
    "                action = np.random.uniform(-config.max_action_value, config.max_action_value, size=env.action_size)\n",
    "                logging.log(logging.DEBUG - 1, f\"Step {global_step_counter}: Random action (startup)\")\n",
    "            else:\n",
    "                # After initial phase: Use agent's policy + exploration noise\n",
    "                action = agent.select_action(state_np, exploration_noise=config.exploration_noise)\n",
    "                # Note: select_action handles noise addition and clipping internally\n",
    "\n",
    "            # 2. Step the Environment\n",
    "            # Environment accepts action in [-1, 1], performs internal scaling to price\n",
    "            next_state_tensor, reward, done, info = env.step(action)\n",
    "            # Convert next state tensor to NumPy for storing in the replay buffer\n",
    "            next_state_np = next_state_tensor.cpu().numpy()\n",
    "\n",
    "            # Environment interaction completed for this step\n",
    "\n",
    "            # 3. Add Experience to Replay Buffer\n",
    "            # Store the transition: (current_state, action_taken, reward_received, next_state, done_flag)\n",
    "            # Note: 'done' flag here indicates terminal state of the episode\n",
    "            agent.replay_buffer.add(state_np, action, reward, next_state_np, done)\n",
    "\n",
    "            # 4. Agent Learning Step (Train Networks)\n",
    "            critic_loss, actor_loss = None, None # Initialize losses for this step\n",
    "            # Only start training updates after the initial random exploration phase\n",
    "            if global_step_counter >= config.start_timesteps:\n",
    "                # Call the agent's train method, which handles batch sampling and updates\n",
    "                critic_loss, actor_loss = agent.train()\n",
    "                # Store the losses if they were calculated (agent.train returns None if buffer too small)\n",
    "                if critic_loss is not None:\n",
    "                    episode_critic_losses.append(critic_loss)\n",
    "                if actor_loss is not None: # Actor loss might be None due to delayed updates\n",
    "                    episode_actor_losses.append(actor_loss)\n",
    "\n",
    "            # --- Logging & Monitoring ---\n",
    "            # Log step-level data to the PerformanceMetrics tracker\n",
    "            metrics.log_step(episode_idx, episode_steps, reward, critic_loss, actor_loss, info)\n",
    "            # Log step-level data to TensorBoard (optional)\n",
    "            if writer:\n",
    "                 writer.add_scalar('Reward/Step_Reward', reward, global_step_counter)\n",
    "                 if critic_loss is not None: writer.add_scalar('Loss/Critic_Step', critic_loss, global_step_counter)\n",
    "                 if actor_loss is not None: writer.add_scalar('Loss/Actor_Step', actor_loss, global_step_counter) # Log only when updated\n",
    "\n",
    "            # --- State Transition and Progress Bar Update ---\n",
    "            state_np = next_state_np # Move to the next state\n",
    "            episode_reward += reward  # Accumulate reward for the episode\n",
    "            pbar.update(1)            # Increment progress bar\n",
    "\n",
    "            # Update postfix display on the progress bar\n",
    "            pbar_postfix = {'Profit': f'{episode_reward:,.0f}'} # Show accumulated profit\n",
    "            if episode_critic_losses: pbar_postfix['CritL'] = f'{np.mean(episode_critic_losses):.3f}' # Show running avg critic loss\n",
    "            if episode_actor_losses: pbar_postfix['ActL'] = f'{np.mean(episode_actor_losses):.3f}'   # Show running avg actor loss\n",
    "            pbar.set_postfix(pbar_postfix)\n",
    "\n",
    "            # --- Episode Termination Check ---\n",
    "            if done:\n",
    "                pbar.close() # Close the progress bar for this episode\n",
    "                break        # Exit the inner step loop\n",
    "\n",
    "        # --- End of Episode Actions ---\n",
    "        # Log summary metrics for the completed episode\n",
    "        metrics.log_episode(episode_idx, episode_steps)\n",
    "        episode_duration = time.time() - start_time\n",
    "        logging.debug(f\"Episode {episode_idx + 1} finished in {episode_duration:.1f} seconds.\")\n",
    "\n",
    "        # Log episode summary data to TensorBoard (optional)\n",
    "        if writer:\n",
    "            episode_num_tb = episode_idx + 1 # Use 1-based index for TensorBoard display\n",
    "            # Check if metrics were successfully logged for this episode before accessing lists\n",
    "            if episode_idx < len(metrics.episode_profits):\n",
    "                 writer.add_scalar('Reward/Episode_Total_Profit', metrics.episode_profits[episode_idx], episode_num_tb)\n",
    "                 writer.add_scalar('Loss/Critic_Episode_Avg', metrics.episode_losses_critic[episode_idx], episode_num_tb)\n",
    "                 writer.add_scalar('Loss/Actor_Episode_Avg', metrics.episode_losses_actor[episode_idx], episode_num_tb)\n",
    "                 writer.add_scalar('Metrics/Load_Factor', metrics.episode_load_factors[episode_idx], episode_num_tb)\n",
    "                 writer.add_scalar('Timing/Episode_Duration_Sec', episode_duration, episode_num_tb)\n",
    "                 # Add exploration noise parameter if needed\n",
    "                 # writer.add_scalar('Params/Exploration_Noise', config.exploration_noise, episode_num_tb) # Assuming noise is constant for now\n",
    "\n",
    "        # --- Live Plot Update ---\n",
    "        # Call the metrics plotting function, passing the live figure and axes\n",
    "        # This updates the dashboard displayed in the notebook output cell\n",
    "        try:\n",
    "            metrics.plot_training_progress(live_fig, live_axs, window=15) # Adjust smoothing window if desired\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating live dashboard plot: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "        # --- Periodic Validation and Early Stopping ---\n",
    "        current_episode_num = episode_idx + 1 # Use 1-based episode number for checks\n",
    "        # Perform validation check at specified frequency or on the very last episode\n",
    "        if current_episode_num % config.validation_freq == 0 or current_episode_num == config.n_episodes:\n",
    "            logging.info(f\"--- Running Validation after Episode {current_episode_num} ---\")\n",
    "            # Evaluate the agent's current deterministic policy\n",
    "            # Use a fixed number of episodes (e.g., 3-5) for stable evaluation\n",
    "            eval_avg_reward = evaluate_model(env, agent, config, num_eval_episodes=3)\n",
    "            # Store the evaluation result\n",
    "            metrics.add_eval_result(eval_avg_reward)\n",
    "            logging.info(f\"  Validation Average Reward: {eval_avg_reward:,.2f}\")\n",
    "            # Log validation reward to TensorBoard\n",
    "            if writer: writer.add_scalar('Reward/Validation_Average', eval_avg_reward, current_episode_num)\n",
    "\n",
    "            # --- Model Saving (Based on Validation Performance) ---\n",
    "            # Check if the current validation reward is the best seen so far\n",
    "            if eval_avg_reward > best_eval_reward:\n",
    "                logging.info(f\"  New best validation reward! {eval_avg_reward:,.2f} > {best_eval_reward:,.2f}. Saving model...\")\n",
    "                best_eval_reward = eval_avg_reward\n",
    "                # Save both actor and critic models (TD3 requirement)\n",
    "                agent.save(config.model_path_actor, config.model_path_critic)\n",
    "                patience_counter = 0 # Reset patience since we found a better model\n",
    "            else:\n",
    "                # If performance did not improve, increment patience counter\n",
    "                patience_counter += 1\n",
    "                logging.info(f\"  Validation reward did not improve ({eval_avg_reward:,.2f} <= {best_eval_reward:,.2f}). Patience: {patience_counter}/{config.patience}\")\n",
    "\n",
    "            # --- Early Stopping Check ---\n",
    "            # If patience counter reaches the limit, stop training early\n",
    "            if patience_counter >= config.patience:\n",
    "                logging.warning(f\"EARLY STOPPING triggered at episode {current_episode_num} after {config.patience} validations without improvement.\")\n",
    "                break # Exit the main training loop (for episodes)\n",
    "\n",
    "    # --- Post-Training ---\n",
    "    logging.info(\"--- Training Loop Finished ---\")\n",
    "\n",
    "    # Save the final state of the dashboard plot as a static image\n",
    "    try:\n",
    "        final_plot_path = os.path.join(config.output_dir, \"td3_training_final_plot.png\")\n",
    "        live_fig.savefig(final_plot_path)\n",
    "        logging.info(f\"Final training plot saved to: {final_plot_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save final dashboard plot: {e}\")\n",
    "\n",
    "    # Display final summary metrics from the tracker\n",
    "    final_summary = metrics.get_final_metrics()\n",
    "    logging.info(f\"Final Training Summary: {final_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Interactive Training Control\n",
    "*Widgets to set key hyperparameters and initiate the training process.* This allows for easy experimentation without editing code directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Interactive Widgets ---\n",
    "\n",
    "# Button to start the training process\n",
    "button_start_train = Button(\n",
    "    description=\" Start Training\",\n",
    "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to begin training with the current settings',\n",
    "    icon='play',              # FontAwesome icon name\n",
    "    layout=Layout(width='auto', margin='10px 0 0 0') # Add some top margin\n",
    ")\n",
    "\n",
    "# Slider for Actor Learning Rate (Logarithmic steps often better for LR)\n",
    "# Using FloatLogSlider requires ipywidgets >= 7.0.0\n",
    "try:\n",
    "    from ipywidgets import FloatLogSlider\n",
    "    slider_actor_lr = FloatLogSlider(\n",
    "        value=config.actor_lr,   # Default value from config\n",
    "        base=10,                # Logarithmic base\n",
    "        min=-5,                 # Exponent for min value (10^-5)\n",
    "        max=-2,                 # Exponent for max value (10^-2)\n",
    "        step=0.1,               # Step size for the exponent\n",
    "        description='Actor LR:',\n",
    "        readout_format='.1e',   # Display in scientific notation\n",
    "        layout=Layout(width='95%')\n",
    "    )\n",
    "except ImportError:\n",
    "     # Fallback to FloatSlider if FloatLogSlider is not available\n",
    "     slider_actor_lr = FloatSlider(\n",
    "        value=config.actor_lr,\n",
    "        min=1e-5, max=1e-3, step=1e-5,\n",
    "        description='Actor LR:', readout_format='.1e',\n",
    "        layout=Layout(width='95%')\n",
    "     )\n",
    "\n",
    "\n",
    "# Slider for Critic Learning Rate\n",
    "try:\n",
    "    slider_critic_lr = FloatLogSlider(\n",
    "        value=config.critic_lr, base=10, min=-4, max=-2, step=0.1,\n",
    "        description='Critic LR:', readout_format='.1e',\n",
    "        layout=Layout(width='95%')\n",
    "    )\n",
    "except ImportError:\n",
    "     slider_critic_lr = FloatSlider(\n",
    "        value=config.critic_lr, min=1e-4, max=5e-3, step=1e-4,\n",
    "        description='Critic LR:', readout_format='.1e',\n",
    "        layout=Layout(width='95%')\n",
    "     )\n",
    "\n",
    "# Slider for Number of Episodes\n",
    "slider_episodes = IntSlider(\n",
    "    value=config.n_episodes, # Default from config\n",
    "    min=10,                 # Sensible minimum\n",
    "    max=1000,               # Sensible maximum (adjust as needed)\n",
    "    step=10,                # Step size\n",
    "    description='Episodes:',\n",
    "    layout=Layout(width='95%')\n",
    ")\n",
    "\n",
    "# --- Widget Event Handler ---\n",
    "\n",
    "training_running = False # Global flag to prevent starting multiple training runs\n",
    "\n",
    "def on_button_clicked(b: Button):\n",
    "    \"\"\"\n",
    "    Callback function executed when the 'Start Training' button is clicked.\n",
    "    Updates config, re-initializes optimizers if needed, and calls train_model.\n",
    "    \"\"\"\n",
    "    global training_running # Use the global flag\n",
    "\n",
    "    # Prevent starting if already running\n",
    "    if training_running:\n",
    "        print(\" Training is already in progress or has finished for this session.\")\n",
    "        logging.warning(\"Attempted to start training while already running.\")\n",
    "        return\n",
    "\n",
    "    print(\" Training requested...\")\n",
    "    # Set flag and update button appearance to indicate running state\n",
    "    training_running = True\n",
    "    button_start_train.disabled = True\n",
    "    button_start_train.description = \" Training Running...\"\n",
    "    button_start_train.button_style = 'info'\n",
    "    button_start_train.icon = 'spinner'\n",
    "\n",
    "    # --- Update Configuration from Widgets ---\n",
    "    # Read the current values from the sliders and update the global config object\n",
    "    config.actor_lr = slider_actor_lr.value\n",
    "    config.critic_lr = slider_critic_lr.value\n",
    "    config.n_episodes = slider_episodes.value\n",
    "    print(f\"  Updated Config: Episodes={config.n_episodes}, ActorLR={config.actor_lr:.1e}, CriticLR={config.critic_lr:.1e}\")\n",
    "\n",
    "    # --- Re-initialize Optimizers (Crucial!) ---\n",
    "    # If the agent exists, create new optimizer instances with the updated learning rates\n",
    "    # This ensures the training uses the rates selected via the sliders.\n",
    "    if 'agent' in globals() and agent is not None:\n",
    "         try:\n",
    "            agent.actor_optimizer = optim.Adam(agent.actor.parameters(), lr=config.actor_lr)\n",
    "            agent.critic_optimizer = optim.Adam(agent.critic.parameters(), lr=config.critic_lr)\n",
    "            print(f\"  Agent optimizers re-initialized with updated learning rates.\")\n",
    "         except Exception as e:\n",
    "             logging.error(f\"Failed to re-initialize optimizers: {e}\")\n",
    "             # Optionally handle error, e.g., by stopping or using old optimizers\n",
    "\n",
    "\n",
    "    # --- Call the Main Training Function ---\n",
    "    print(f\"  Starting training loop for {config.n_episodes} episodes...\")\n",
    "    # Ensure all necessary components are available before starting\n",
    "    if ('env' in globals() and env is not None and\n",
    "        'agent' in globals() and agent is not None and\n",
    "        'metrics_tracker' in globals() and metrics_tracker is not None and\n",
    "        'dashboard_fig' in globals() and 'dashboard_axs' in globals()): # Check for plot objects\n",
    "         try:\n",
    "            # Pass all required arguments, including the live plot figure and axes\n",
    "            train_model(env=env, agent=agent, metrics=metrics_tracker, config=config,\n",
    "                        live_fig=dashboard_fig, live_axs=dashboard_axs, # Pass dashboard objects\n",
    "                        writer=writer) # Pass TensorBoard writer (might be None)\n",
    "         except Exception as e:\n",
    "            # Catch potential errors during the training loop execution\n",
    "            logging.error(f\"Exception occurred during train_model execution: {e}\", exc_info=True)\n",
    "            print(f\" Training encountered an error: {e}\")\n",
    "         finally:\n",
    "            # --- Post-Training Cleanup (Button State) ---\n",
    "            # Reset button state regardless of whether training finished successfully or errored\n",
    "            button_start_train.disabled = False\n",
    "            button_start_train.description = \" Start Training\"\n",
    "            button_start_train.button_style = 'success'\n",
    "            button_start_train.icon = 'play'\n",
    "            # Decide whether to reset `training_running` flag to allow re-runs\n",
    "            # For safety, keeping it True might prevent accidental re-runs in the same kernel session\n",
    "            # training_running = False # Uncomment to allow re-running training\n",
    "            print(\" Training function execution complete.\")\n",
    "    else:\n",
    "        # If components are missing, report error and reset button\n",
    "        print(\" Cannot start training - one or more required components (Environment, Agent, Metrics Tracker, Dashboard Figure/Axes) are not ready.\")\n",
    "        button_start_train.disabled = False\n",
    "        button_start_train.description = \" Start Training\"\n",
    "        button_start_train.button_style = 'success'\n",
    "        button_start_train.icon = 'play'\n",
    "        training_running = False # Allow trying again if components become ready later\n",
    "\n",
    "\n",
    "# --- Link Button Click Event to Handler ---\n",
    "button_start_train.on_click(on_button_clicked)\n",
    "\n",
    "# --- Display Widgets ---\n",
    "# Use VBox/HBox for better layout control\n",
    "controls_box = VBox([\n",
    "    HTML(\"<h4>Training Controls:</h4>\"),\n",
    "    slider_episodes,\n",
    "    slider_actor_lr,\n",
    "    slider_critic_lr,\n",
    "    button_start_train\n",
    "], layout=Layout(border='1px solid #ccc', padding='10px', margin='10px 0'))\n",
    "\n",
    "display(controls_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Results\n",
    " ---\n",
    "*Interactive result exploration and policy visualization.* Analyze the performance of the trained agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Run Final Evaluation\n",
    " *Load the best performing model saved during training (if it exists) and run the `evaluate_model` function to get a final performance measure.* Logs results and optionally sends final metrics to TensorBoard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Evaluation Trigger Cell ---\n",
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "\n",
    "\n",
    "# Default reward value if evaluation can't run\n",
    "final_eval_reward = -np.inf\n",
    "\n",
    "# Check if environment and agent are properly initialized\n",
    "if 'env' in locals() and env is not None and 'agent' in locals() and agent is not None:\n",
    "\n",
    "    # --- Load Best Saved Model ---\n",
    "    # Attempt to load the actor and critic weights saved during training\n",
    "    # These paths are defined in the config section\n",
    "    model_loaded = agent.load(config.model_path_actor, config.model_path_critic)\n",
    "\n",
    "    # Provide feedback on whether the model was loaded\n",
    "    if model_loaded:\n",
    "        print(\" Successfully loaded best saved Actor and Critic models.\")\n",
    "    else:\n",
    "        print(\" Could not load saved models (or paths invalid). Evaluating the agent's current state.\")\n",
    "\n",
    "    # --- Run Evaluation ---\n",
    "    # Call the evaluation function with the (potentially loaded) agent\n",
    "    final_eval_reward = evaluate_model(env, agent, config, num_eval_episodes=5) # Use 5 episodes for final eval\n",
    "    print(f\"\\n Final Average Evaluation Reward: {final_eval_reward:,.2f}\")\n",
    "\n",
    "    # --- Log Hyperparameters and Final Metrics to TensorBoard ---\n",
    "    # Check if TensorBoard writer and metrics tracker were initialized successfully\n",
    "    if 'writer' in locals() and writer is not None and 'metrics_tracker' in locals() and metrics_tracker is not None:\n",
    "         try:\n",
    "             # Prepare hyperparameters dictionary (flatten complex types like lists/dicts)\n",
    "             hparam_dict_flat = {k:str(v) if isinstance(v,(dict,list)) else v for k,v in config.__dict__.items()}\n",
    "\n",
    "             # Prepare final metrics dictionary\n",
    "             # Start with the final evaluation reward\n",
    "             metric_dict = {'hparam/final_eval_reward': final_eval_reward}\n",
    "             # Get summary metrics from the tracker\n",
    "             final_metrics_summary = metrics_tracker.get_final_metrics()\n",
    "             # Safely add other summary metrics to the dictionary\n",
    "             metric_dict['hparam/avg_profit_last10'] = final_metrics_summary.get('avg_profit_last_10_eps', 0)\n",
    "             metric_dict['hparam/load_factor_last10'] = final_metrics_summary.get('avg_load_factor_last_10_eps', 0)\n",
    "             metric_dict['hparam/max_profit_episode'] = final_metrics_summary.get('max_profit_episode', 0)\n",
    "\n",
    "             # Get run name if defined, otherwise TensorBoard might use default naming\n",
    "             run_name_tb = run_name if 'run_name' in locals() else None\n",
    "             # Log the hyperparameters and final metrics\n",
    "             writer.add_hparams(hparam_dict_flat, metric_dict, run_name=run_name_tb)\n",
    "             print(\"Logged HParams and final metrics to TensorBoard.\")\n",
    "         except Exception as e:\n",
    "              # Catch potential errors during logging\n",
    "              logging.error(f\"Failed to log HParams/Metrics to TensorBoard: {e}\", exc_info=True)\n",
    "    else:\n",
    "        # Explain why HParams weren't logged if applicable\n",
    "        if 'writer' not in locals() or writer is None: print(\"Skipping HParam logging: TensorBoard writer not initialized.\")\n",
    "        if 'metrics_tracker' not in locals() or metrics_tracker is None: print(\"Skipping HParam logging: Metrics tracker not initialized.\")\n",
    "\n",
    "else:\n",
    "    # If env or agent weren't ready\n",
    "    print(\" Skipping final evaluation: Environment or Agent was not initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Interactive Policy Visualization (Example)\n",
    " *Allows selecting a specific route and airline via dropdowns to visualize the pricing strategy learned by the agent over the simulation period.* This requires running a simulation loop using the agent's greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interactive Policy Visualization Cell ---\n",
    "print(\"\\n--- Policy Visualization Tool ---\")\n",
    "\n",
    "if ('env' in locals() and env is not None and\n",
    "    'agent' in locals() and agent is not None and\n",
    "    'processed_data' in locals() and processed_data is not None and not processed_data.empty):\n",
    "\n",
    "    # --- Prepare Widget Options ---\n",
    "    # Get unique routes and airlines from the processed data for dropdowns\n",
    "    try:\n",
    "        route_options = sorted(processed_data['Route'].unique().tolist())\n",
    "        airline_options = sorted(processed_data['Airline'].unique().tolist())\n",
    "        # Check if options are valid\n",
    "        if not route_options or not airline_options:\n",
    "            raise ValueError(\"No routes or airlines found in processed data.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error preparing widget options: {e}. Cannot create visualization tool.\")\n",
    "        route_options = ['N/A'] # Provide dummy options on error\n",
    "        airline_options = ['N/A']\n",
    "\n",
    "    # --- Define Interactive Function ---\n",
    "    # Use the @interact decorator to automatically create widgets\n",
    "    @interact\n",
    "    def show_policy_plot(route=Dropdown(options=route_options, description=\"Route:\"),\n",
    "                         airline=Dropdown(options=airline_options, description=\"Airline:\")):\n",
    "        \"\"\"\n",
    "        This function is called whenever a dropdown selection changes.\n",
    "        It simulates the agent's greedy policy for the selected flight\n",
    "        and plots the resulting price trajectory.\n",
    "        \"\"\"\n",
    "        # Avoid running if dummy options are selected\n",
    "        if route == 'N/A' or airline == 'N/A':\n",
    "            return\n",
    "\n",
    "        print(f\" Simulating greedy policy for: {route} - {airline}...\")\n",
    "        # Double-check agent and env availability inside the function\n",
    "        if not agent or not env:\n",
    "            print(\" Agent or Environment not ready.\")\n",
    "            return\n",
    "\n",
    "        # Find the specific index for this flight combination in the environment's mapping\n",
    "        try:\n",
    "            flight_idx = env.flight_map[(route, airline)]\n",
    "        except KeyError:\n",
    "            print(f\" Error: Combination '{route}' - '{airline}' not found in environment's flight map.\")\n",
    "            return\n",
    "\n",
    "        # --- Simulate Greedy Policy ---\n",
    "        agent.actor.eval() # Set agent to evaluation mode\n",
    "        state_np = env.reset().cpu().numpy() # Reset environment for simulation\n",
    "        prices_over_time = [] # List to store prices for the selected flight\n",
    "        days = []             # List to store corresponding dates\n",
    "        current_sim_date = env.start_date # Start date for x-axis\n",
    "\n",
    "        # Run simulation loop (similar to evaluation, but store specific price)\n",
    "        with torch.no_grad():\n",
    "            for day_step in range(config.simulation_length_days):\n",
    "                # Get the full action vector from the agent (greedy)\n",
    "                action_vector = agent.select_action(state_np, exploration_noise=0.0)\n",
    "\n",
    "                # Extract the specific action for the selected flight index\n",
    "                action_selected_flight = action_vector[flight_idx]\n",
    "\n",
    "                # --- Convert action [-1, 1] to actual price ---\n",
    "                # Use the environment's scaling parameters\n",
    "                price = env.price_range[0] + (action_selected_flight + 1.0) * 0.5 * env.price_delta\n",
    "                price = np.clip(price, env.price_range[0], env.price_range[1]) # Clip final price\n",
    "\n",
    "                # Store the price and date for plotting\n",
    "                prices_over_time.append(price)\n",
    "                days.append(current_sim_date)\n",
    "\n",
    "                # Step the *whole* environment with the full action vector to get the next state\n",
    "                next_state_tensor, _, done, _ = env.step(action_vector)\n",
    "                state_np = next_state_tensor.cpu().numpy() # Update state for next iteration\n",
    "                current_sim_date += pd.Timedelta(days=1) # Increment date\n",
    "\n",
    "                # Stop if the environment signals done (end of simulation period)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        agent.actor.train() # Set agent back to training mode\n",
    "\n",
    "        # --- Plotting Results ---\n",
    "        if prices_over_time:\n",
    "            plt.figure(figsize=(12, 5)) # Create a new figure for the plot\n",
    "            plt.plot(days, prices_over_time, marker='.', linestyle='-', markersize=4, label=f'{airline} Price')\n",
    "            plt.title(f\"Agent's Learned Pricing Policy: {route} ({airline})\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(f\"Price ({config.data_paths['historical_data'].split('/')[-1].split('.')[0].capitalize()})\") # Use currency/unit if known\n",
    "            # Set y-axis limits slightly wider than the price range for better visibility\n",
    "            plt.ylim(bottom=env.price_range[0] * 0.95, top=env.price_range[1] * 1.05)\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "            plt.xticks(rotation=30, ha='right') # Rotate date labels for readability\n",
    "            plt.legend()\n",
    "            plt.tight_layout() # Adjust layout\n",
    "            plt.show() # Display the plot\n",
    "        else:\n",
    "            print(\" No price data was generated during the simulation for this flight.\")\n",
    "\n",
    "else:\n",
    "    # Message if essential components are missing\n",
    "    print(\" Skipping Policy Visualization: Environment, Agent, or Processed Data not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    " ---\n",
    " *Final steps, like closing the TensorBoard writer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Close TensorBoard Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'writer' in locals() and writer:\n",
    "    try: writer.close(); print(\"\\nTensorBoard writer closed.\")\n",
    "    except Exception as e: print(f\"Error closing TB writer: {e}\")\n",
    "\n",
    "print(\"\\n--- Notebook Execution Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
