{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import datetime\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, Dict, Any  # Import necessary types\n",
    "\n",
    "# Type hinting and docstrings in Config\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the airline pricing environment and agent.\"\"\"\n",
    "    seed: int = 42\n",
    "    \"\"\"Random seed for reproducibility.\"\"\"\n",
    "    price_scaler_range: Tuple[float, float] = (0.1, 0.9)\n",
    "    \"\"\"Range for the price scaler.\"\"\"\n",
    "    outlier_std_threshold: int = 3\n",
    "    \"\"\"Number of standard deviations for outlier removal.\"\"\"\n",
    "    max_days_ahead: int = 90\n",
    "    \"\"\"Maximum days ahead for booking.\"\"\"\n",
    "    simulation_length_days: int = 365\n",
    "    \"\"\"Length of the simulation in days.\"\"\"\n",
    "    seats_capacity: int = 150\n",
    "    \"\"\"Capacity of the aircraft.\"\"\"\n",
    "    action_days_ahead: int = 1\n",
    "    \"\"\"Number of days ahead the agent sets prices.\"\"\"\n",
    "    memory_size: int = 10000\n",
    "    \"\"\"Size of the agent's replay memory.\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"Discount factor for future rewards.\"\"\"\n",
    "    epsilon: float = 1.0\n",
    "    \"\"\"Initial exploration rate.\"\"\"\n",
    "    epsilon_min: float = 0.01\n",
    "    \"\"\"Minimum exploration rate.\"\"\"\n",
    "    epsilon_decay: float = 0.998\n",
    "    \"\"\"Exploration rate decay factor.\"\"\"\n",
    "    learning_rate: float = 0.0005\n",
    "    \"\"\"Learning rate for the optimizer.\"\"\"\n",
    "    batch_size: int = 128\n",
    "    \"\"\"Batch size for training.\"\"\"\n",
    "    target_update_freq: int = 20\n",
    "    \"\"\"Frequency of target network updates.\"\"\"\n",
    "    patience: int = 20\n",
    "    \"\"\"Patience for early stopping.\"\"\"\n",
    "    n_episodes: int = 100\n",
    "    \"\"\"Number of training episodes.\"\"\"\n",
    "    data_generation_params: Dict[str, Any] = field(default_factory=lambda: {  # Use a dictionary\n",
    "        'min_price': 2000.0,\n",
    "        'max_price': 8000.0,\n",
    "        'min_demand': 50,\n",
    "        'max_demand': 150,\n",
    "        'min_temp': 0.0,\n",
    "        'max_temp': 35.0,\n",
    "        'min_fuel_price': 2.0,\n",
    "        'max_fuel_price': 4.0\n",
    "    })\n",
    "    \"\"\"Parameters for data generation (if needed).\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Argument parsing for config file and overrides\n",
    "parser = argparse.ArgumentParser(description=\"Train DQN agent for airline pricing.\")\n",
    "parser.add_argument('--config', type=str, default='config.yaml', help='Path to config file (YAML)')\n",
    "parser.add_argument('--n_episodes', type=int, help='Override number of training episodes')  # Example override\n",
    "# ... Add other argument overrides as needed ...\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load config from YAML or use defaults\n",
    "try:\n",
    "    with open(args.config, 'r') as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    config = Config(**config_dict)\n",
    "except FileNotFoundError:\n",
    "    logging.warning(f\"Config file not found: {args.config}. Using default configuration.\")\n",
    "    config = Config()\n",
    "\n",
    "# Override config parameters from command-line arguments\n",
    "if args.n_episodes:\n",
    "    config.n_episodes = args.n_episodes\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set seeds for reproducibility (using a single seed now)\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.backends.cudnn.deterministic = True  # Important for reproducibility with CUDA\n",
    "torch.backends.cudnn.benchmark = False     # For fair comparison across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.state_scaler = StandardScaler()\n",
    "        self.price_scaler = MinMaxScaler(feature_range=config.PRICE_SCALER_RANGE)\n",
    "        self.demand_scaler = StandardScaler()\n",
    "        self.route_encoder = {}\n",
    "        self.airline_encoder = {}\n",
    "        self.aircraft_encoder = {}\n",
    "        self.weather_encoder = {}\n",
    "        self.holiday_encoder = {}\n",
    "        self.fuel_price_mean = None  # To store the mean fuel price.\n",
    "\n",
    "    def fit(self, historical_data, fuel_prices, climate_data, holiday_data):\n",
    "        if historical_data.empty or climate_data.empty or holiday_data.empty:  # fuel_prices can now be empty.\n",
    "            raise ValueError(\"Input data cannot be empty (except fuel_prices, which is handled separately).\")\n",
    "\n",
    "        # Handle the case where fuel_prices is empty or None:\n",
    "        if fuel_prices is None or fuel_prices.empty:\n",
    "            logging.warning(\"fuel_prices DataFrame is empty.  Creating a placeholder with constant fuel price.\")\n",
    "            # Create a DataFrame with the same 'Date' range as historical_data and a constant 'FuelPrice'\n",
    "            dates = historical_data['Date'].unique()\n",
    "            self.fuel_price_mean = 2.5  # A reasonable default.  Could also be put in Config.\n",
    "            fuel_prices = pd.DataFrame({'Date': dates, 'FuelPrice': self.fuel_price_mean})\n",
    "            fuel_prices['Date'] = pd.to_datetime(fuel_prices['Date'])\n",
    "\n",
    "        else: #if file is not empty.\n",
    "            fuel_prices.loc[:, 'Date'] = pd.to_datetime(fuel_prices['Date'])\n",
    "            if 'FuelPrice' not in fuel_prices.columns:\n",
    "                raise ValueError(\"Missing FuelPrice column\")\n",
    "            self.fuel_price_mean = fuel_prices['FuelPrice'].mean()  # Use the actual mean\n",
    "            fuel_prices = fuel_prices.fillna(self.fuel_price_mean) # added to fill any remaining NaN\n",
    "\n",
    "        #The rest of your fit function is correct.\n",
    "\n",
    "        _ = self.transform(historical_data, fuel_prices, climate_data, holiday_data)  # Call transform (for side effects like outlier removal, etc.)\n",
    "        sample_state = [0.0, 1.0, 0.0, 2.5, 20.0, 0.0, 150.0, 300.0, 100.0, -0.5]\n",
    "        sample_state.extend([0.0] * 5)  # 5 categorical features (route, airline, aircraft, weather, season)\n",
    "        self.state_scaler.fit(np.array([sample_state]))\n",
    "        self.demand_scaler.fit(historical_data[['Demand']])\n",
    "\n",
    "        # Create the encoders\n",
    "        self.route_encoder = {route: i for i, route in enumerate(historical_data['Route'].unique())}\n",
    "        self.airline_encoder = {airline: i for i, airline in enumerate(historical_data['Airline'].unique())}\n",
    "        self.aircraft_encoder = {aircraft: i for i, aircraft in enumerate(historical_data['AircraftType'].unique())}\n",
    "        self.weather_encoder = {weather: i for i, weather in enumerate(climate_data['WeatherCondition'].unique())}\n",
    "        self.holiday_encoder = {holiday: i for i, holiday in enumerate(holiday_data['HolidayName'].unique())}\n",
    "\n",
    "        self.route_reference = list(self.route_encoder.keys())[0]\n",
    "        self.airline_reference = list(self.airline_encoder.keys())[0]\n",
    "        self.aircraft_reference = list(self.aircraft_encoder.keys())[0]\n",
    "        self.weather_reference = list(self.weather_encoder.keys())[0]\n",
    "        self.holiday_reference = list(self.holiday_encoder.keys())[0]\n",
    "        return self\n",
    "    def transform(self, historical_data, fuel_prices, climate_data, holiday_data):\n",
    "        required_columns = {\n",
    "            'historical_data': ['Date', 'Route', 'Airline', 'AircraftType', 'Price', 'Demand', 'Capacity'],\n",
    "            'fuel_prices': ['Date', 'FuelPrice'],\n",
    "            'climate_data': ['Date', 'Location', 'Temperature', 'WeatherCondition'],\n",
    "            'holiday_data': ['Date', 'Location', 'HolidayName']\n",
    "        }\n",
    "        for df_name, cols in required_columns.items():\n",
    "            df = locals()[df_name]\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in {df_name}: {', '.join(missing_cols)}\")\n",
    "\n",
    "\n",
    "        historical_data = historical_data.copy()\n",
    "        fuel_prices = fuel_prices.copy()\n",
    "        climate_data = climate_data.copy()\n",
    "        holiday_data = holiday_data.copy()\n",
    "\n",
    "        # Date conversions and feature additions (using .loc for clarity)\n",
    "        historical_data.loc[:, 'Date'] = pd.to_datetime(historical_data['Date'])\n",
    "        fuel_prices.loc[:, 'Date'] = pd.to_datetime(fuel_prices['Date'])\n",
    "        climate_data.loc[:, 'Date'] = pd.to_datetime(climate_data['Date'])\n",
    "        holiday_data.loc[:, 'Date'] = pd.to_datetime(holiday_data['Date'])\n",
    "\n",
    "        historical_data.loc[:, 'DayOfWeek'] = historical_data['Date'].dt.dayofweek\n",
    "        historical_data.loc[:, 'Month'] = historical_data['Date'].dt.month\n",
    "        historical_data.loc[:, 'IsWeekend'] = (historical_data['DayOfWeek'] >= 5).astype(int)\n",
    "        historical_data.loc[:, 'Season'] = historical_data['Month'].map(self._get_season)\n",
    "\n",
    "        historical_data = historical_data.fillna({\n",
    "            'Demand': historical_data['Demand'].mean(),\n",
    "            'Price': historical_data['Price'].mean(),\n",
    "            'Capacity': historical_data['Capacity'].mode()[0]\n",
    "        })\n",
    "        fuel_prices = fuel_prices.fillna(fuel_prices['FuelPrice'].mean())\n",
    "        climate_data = climate_data.ffill().bfill() # to fill the missing value.\n",
    "        holiday_data = holiday_data.fillna('None')\n",
    "\n",
    "        historical_data = self._remove_outliers(historical_data, ['Price', 'Demand'])\n",
    "        historical_data = self._merge_data(historical_data, fuel_prices, climate_data, holiday_data)\n",
    "        historical_data.loc[:, 'Demand_MA7'] = historical_data.groupby(['Route', 'Airline'])['Demand'].transform(\n",
    "            lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        def calculate_elasticity(group):\n",
    "            price_pct = group['Price'].pct_change()\n",
    "            demand_pct = group['Demand'].pct_change()\n",
    "            elasticity = np.where(price_pct != 0, demand_pct / price_pct, 0)\n",
    "            return pd.Series(elasticity, index=group.index)\n",
    "\n",
    "        historical_data = historical_data.sort_values(['Route', 'Airline', 'Date'])\n",
    "        historical_data.loc[:, 'PriceElasticity'] = historical_data.groupby(\n",
    "            ['Route', 'Airline'], group_keys=False).apply(calculate_elasticity, include_groups=False).fillna(0)\n",
    "\n",
    "        return historical_data\n",
    "\n",
    "\n",
    "    def _get_season(self, month):\n",
    "        if month in [12, 1, 2]: return 'Winter'\n",
    "        if month in [3, 4, 5]: return 'Spring'\n",
    "        if month in [6, 7, 8]: return 'Summer'\n",
    "        return 'Fall'\n",
    "\n",
    "    def _remove_outliers(self, df, columns, n_std=config.OUTLIER_STD_THRESHOLD):\n",
    "        for column in columns:\n",
    "            mean = df[column].mean()\n",
    "            std = df[column].std()\n",
    "            df = df[(df[column] <= mean + (n_std * std)) & (df[column] >= mean - (n_std * std))]\n",
    "        return df\n",
    "\n",
    "    def _merge_data(self, historical_data, fuel_prices, climate_data, holiday_data):\n",
    "        \"\"\"Merge all data sources.\"\"\"\n",
    "        try:\n",
    "            historical_data = pd.merge(historical_data, fuel_prices, on='Date', how='left')\n",
    "            if 'FuelPrice' not in historical_data.columns:\n",
    "                raise ValueError(\"Failed to merge fuel prices\")\n",
    "\n",
    "            historical_data['Origin'] = historical_data['Route'].apply(lambda x: x.split('-')[0])\n",
    "            historical_data = pd.merge(historical_data, climate_data,\n",
    "                                       left_on=['Date', 'Origin'],\n",
    "                                       right_on=['Date', 'Location'],\n",
    "                                       how='left')\n",
    "            if 'Temperature' not in historical_data.columns or 'WeatherCondition' not in historical_data.columns:\n",
    "                 raise ValueError(\"Failed to merge climate data\")\n",
    "            historical_data.drop('Location', axis=1, inplace=True)\n",
    "\n",
    "            historical_data = pd.merge(historical_data, holiday_data,\n",
    "                                       left_on=['Date', 'Origin'],\n",
    "                                       right_on=['Date', 'Location'],\n",
    "                                       how='left')\n",
    "            if 'HolidayName' not in historical_data.columns:\n",
    "                raise ValueError(\"Failed to merge holiday data\")\n",
    "\n",
    "            historical_data['IsHoliday'] = (historical_data['HolidayName'].notna()).astype(int)\n",
    "            historical_data.drop(['Location', 'HolidayName'], axis=1, inplace=True)\n",
    "            return historical_data\n",
    "\n",
    "        except KeyError as e:\n",
    "            logging.error(f\"KeyError during merge: {e}\")\n",
    "            raise\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"ValueError during merge: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during merge: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scale_prices(self, prices):\n",
    "        prices = prices.cpu().numpy() if isinstance(prices, torch.Tensor) else np.array(prices)\n",
    "        prices = np.clip(prices, 0, self.historical_price_max)\n",
    "        scaled = self.price_scaler.transform(prices.reshape(-1, 1)).flatten()\n",
    "\n",
    "        return torch.tensor(scaled, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def inverse_scale_prices(self, scaled_prices):\n",
    "        scaled_prices = scaled_prices.cpu().numpy() if isinstance(scaled_prices, torch.Tensor) else np.array(scaled_prices)\n",
    "        return self.price_scaler.inverse_transform(scaled_prices.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "    def k_encode(self, value, encoder, reference_value):\n",
    "      encoding = [0] * (len(encoder) - 1)\n",
    "      if value != reference_value:\n",
    "          index = encoder.get(value)\n",
    "          if index is not None:\n",
    "            adjusted_index = index - (index > list(encoder.values())[list(encoder.keys()).index(reference_value)])\n",
    "            if adjusted_index < len(encoding):\n",
    "                encoding[adjusted_index] = 1\n",
    "      return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirlinePricingEnv:\n",
    "    def __init__(self, historical_data, fuel_prices, climate_data, holiday_data, data_processor):\n",
    "        if historical_data.empty or fuel_prices.empty or climate_data.empty or holiday_data.empty:\n",
    "            raise ValueError(\"Input data cannot be empty\")\n",
    "\n",
    "        if not isinstance(data_processor, DataProcessor):\n",
    "            raise ValueError(\"data_processor must be an instance of DataProcessor\")\n",
    "\n",
    "        self.data_processor = data_processor\n",
    "        self.historical_data = self.data_processor.transform(historical_data, fuel_prices, climate_data, holiday_data)\n",
    "        self.routes = self.historical_data['Route'].unique()\n",
    "        self.airlines = self.historical_data['Airline'].unique()\n",
    "        self.aircraft_types = self.historical_data['AircraftType'].unique()\n",
    "        self.current_date = self.historical_data['Date'].min()\n",
    "        self.action_days_ahead = config.ACTION_DAYS_AHEAD  # Define how many days ahead the agent sets prices\n",
    "        self.simulation_length_days = config.SIMULATION_LENGTH_DAYS\n",
    "        self.seats_capacity = config.SEATS_CAPACITY\n",
    "        self.prices = {}  # (route, airline): price. Store ONLY current prices.\n",
    "        self.seats_sold = {}\n",
    "        self.data_processor.historical_price_max = self.historical_data['Price'].max()\n",
    "        self.data_processor.price_scaler.fit(self.historical_data['Price'].values.reshape(-1, 1))\n",
    "        self.seasonal_indices = self._calculate_seasonal_indices()\n",
    "        self.historical_price_mean = self.historical_data['Price'].mean()\n",
    "        self.preprocessed_data = self._preprocess_data()  # Preprocess for faster lookups\n",
    "        self.reset()\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"Pre-processes historical data for fast lookups in _get_demand.\"\"\"\n",
    "        data_dict = {}\n",
    "        for (route, airline), group in self.historical_data.groupby(['Route', 'Airline']):\n",
    "            data_dict[(route, airline)] = group.set_index('Date').to_dict('index')\n",
    "        return data_dict\n",
    "\n",
    "    def reset(self, historical_data=None):\n",
    "        if historical_data is None:\n",
    "            self.current_date = self.historical_data['Date'].min()\n",
    "            historical_data = self.historical_data\n",
    "        else:\n",
    "            self.current_date = historical_data['Date'].min()\n",
    "        self.current_step = 0\n",
    "        self.prices = {}  # Reset prices\n",
    "        self.seats_sold = {}\n",
    "        return self._get_state(historical_data)\n",
    "    \n",
    "\n",
    "    def _get_state(self, historical_data):\n",
    "       try:\n",
    "            flight_date = self.current_date\n",
    "            route = self.routes[0]\n",
    "            airline = self.airlines[0]\n",
    "\n",
    "            # Use preprocessed data for faster lookup\n",
    "            data_for_route_airline = self.preprocessed_data.get((route, airline))\n",
    "            if data_for_route_airline:\n",
    "                row = data_for_route_airline.get(flight_date)\n",
    "            else:\n",
    "                row = None\n",
    "\n",
    "            if row is None:\n",
    "                day_of_week, month, is_weekend, season = flight_date.weekday(), flight_date.month, 1 if flight_date.weekday() >= 5 else 0, self.data_processor._get_season(flight_date.month)\n",
    "                fuel_price, temperature, weather_condition, is_holiday = self.historical_data['FuelPrice'].mean(), 25, 'Sunny', 0\n",
    "                demand_ma7, price_elasticity, seats_sold, current_price = self.historical_data['Demand_MA7'].mean(), 0, 0, 0.0\n",
    "            else:\n",
    "                day_of_week, month, is_weekend, season = int(row['DayOfWeek']), int(row['Month']), int(row['IsWeekend']), row['Season']\n",
    "                fuel_price, temperature, weather_condition = float(row['FuelPrice']), float(row['Temperature']), row['WeatherCondition']\n",
    "                is_holiday, demand_ma7, price_elasticity = int(row['IsHoliday']), float(row['Demand_MA7']), float(row['PriceElasticity'])\n",
    "                seats_sold = self.seats_sold.get((route, airline, flight_date), 0)\n",
    "                current_price = self.prices.get((route, airline), 0.0)  # Get current price for the (route, airline)\n",
    "\n",
    "            route_encoded = self.data_processor.k_encode(route, self.data_processor.route_encoder, self.data_processor.route_reference)\n",
    "            airline_encoded = self.data_processor.k_encode(airline, self.data_processor.airline_encoder, self.data_processor.airline_reference)\n",
    "            aircraft_type = self.historical_data[(self.historical_data['Route'] == route) & (self.historical_data['Airline'] == airline)]['AircraftType'].mode()[0]\n",
    "            aircraft_encoded = self.data_processor.k_encode(aircraft_type, self.data_processor.aircraft_encoder, self.data_processor.aircraft_reference)\n",
    "            weather_encoded = self.data_processor.k_encode(weather_condition, self.data_processor.weather_encoder, self.data_processor.weather_reference)\n",
    "            season_encoded = self.data_processor.k_encode(season, {'Winter': 0, 'Spring': 1, 'Summer': 2, 'Fall': 3}, 'Winter')\n",
    "            remaining_capacity = self.seats_capacity - seats_sold\n",
    "\n",
    "            state = [float(day_of_week), float(month), float(is_weekend), float(fuel_price),\n",
    "                    float(temperature), float(is_holiday), float(remaining_capacity), float(current_price),\n",
    "                    float(demand_ma7), float(price_elasticity)]\n",
    "            state.extend([float(self.data_processor.route_encoder.get(route, 0.0)),\n",
    "                          float(self.data_processor.airline_encoder.get(airline, 0.0)),\n",
    "                          float(self.data_processor.aircraft_encoder.get(aircraft_type, 0.0)),\n",
    "                          float(self.data_processor.weather_encoder.get(weather_condition, 0.0)),\n",
    "                          float({'Winter': 0, 'Spring': 1, 'Summer': 2, 'Fall': 3}.get(season, 0.0))])\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            scaled_state = torch.from_numpy(self.data_processor.state_scaler.transform(state_tensor.numpy())).float()\n",
    "\n",
    "            if torch.isnan(scaled_state).any() or torch.isinf(scaled_state).any():\n",
    "                logging.warning(\"Scaled state contains NaN/inf values; using fallback.\")\n",
    "                scaled_state = torch.zeros_like(scaled_state)\n",
    "            return scaled_state.squeeze(0)\n",
    "\n",
    "       except Exception as e:\n",
    "            logging.error(f\"Error generating state: {str(e)}\")\n",
    "            state_size = 15  #  Adjust as needed\n",
    "            return torch.zeros(state_size, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def _calculate_seasonal_indices(self):\n",
    "        seasonal_indices = {}\n",
    "        for (route, airline), group in self.historical_data.groupby(['Route', 'Airline']):\n",
    "            seasonal_avg = group.groupby('Season')['Demand'].mean()\n",
    "            overall_avg = group['Demand'].mean()\n",
    "            seasonal_indices[(route, airline)] = seasonal_avg / overall_avg\n",
    "        return seasonal_indices\n",
    "\n",
    "    def _get_demand(self, route, airline, flight_date, price):\n",
    "        \"\"\"Gets demand using the preprocessed data dictionary.\"\"\"\n",
    "        try:\n",
    "            data_for_route_airline = self.preprocessed_data[(route, airline)]\n",
    "            row = data_for_route_airline.get(flight_date)\n",
    "\n",
    "            if row is None:\n",
    "                # Handle cases where there's no exact date match.\n",
    "                # Find closest date (using idxmin, as before, but on a smaller dataset)\n",
    "                dates = np.array(list(data_for_route_airline.keys()))\n",
    "                closest_date_idx = np.argmin(np.abs(dates - flight_date))\n",
    "                closest_date = dates[np.argmin(np.abs(dates - flight_date))]\n",
    "                row = data_for_route_airline[closest_date]\n",
    "                base_demand, price_elasticity = row['Demand_MA7'], row['PriceElasticity']\n",
    "            else:\n",
    "                base_demand, price_elasticity = row['Demand_MA7'], row['PriceElasticity']\n",
    "\n",
    "        except KeyError:\n",
    "            # Handle cases where the (route, airline) combination doesn't exist.\n",
    "            base_demand, price_elasticity = self.historical_data['Demand'].mean(), -0.5\n",
    "\n",
    "\n",
    "        seasonal_index = self.seasonal_indices.get((route, airline), {}).get(self.data_processor._get_season(flight_date.month), 1.0)\n",
    "        base_demand *= seasonal_index\n",
    "\n",
    "        # Check if the current date is holiday. No need to search entire historical data\n",
    "        is_holiday = int(row['IsHoliday']) if row and 'IsHoliday' in row else 0\n",
    "        base_demand *= 1.5 if is_holiday == 1 else 1\n",
    "\n",
    "        demand = int(base_demand * (1 + price_elasticity * ((price - self.historical_price_mean) / self.historical_price_mean)))\n",
    "        return max(0, int(demand))\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Executes one step (one day) in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (torch.Tensor): Pricing actions for all routes/airlines/dates\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains (next_state, reward, done, info) where:\n",
    "                next_state (torch.Tensor): Next state representation\n",
    "                reward (torch.Tensor): Total revenue from this step\n",
    "                done (torch.Tensor): Whether simulation is complete\n",
    "                info (dict): Additional information (currently empty)\n",
    "        \"\"\"\n",
    "        # print(\"Shape of action inside step:\", action.shape)\n",
    "        total_revenue = 0\n",
    "\n",
    "        # Convert action to a NumPy array if it's a tensor\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            action = action.detach().cpu().numpy()\n",
    "\n",
    "        action_idx = 0  # Initialize action index\n",
    "\n",
    "        for route in self.routes:\n",
    "            for airline in self.airlines:\n",
    "              # Agent sets price for the current day (or a small number of future days)\n",
    "                for days_ahead in range(self.action_days_ahead):\n",
    "                    flight_date = self.current_date + pd.Timedelta(days=days_ahead)\n",
    "\n",
    "                    # Scale the single price\n",
    "                    price = self.data_processor.inverse_scale_prices(np.array([action]))[0]\n",
    "                    # (route, airline) -> price. Setting price for the current day\n",
    "                    self.prices[(route, airline)] = price\n",
    "\n",
    "                    seats_sold_key = (route, airline, flight_date)\n",
    "                    seats_already_sold = self.seats_sold.get(seats_sold_key, 0)\n",
    "                    demand = self._get_demand(route, airline, flight_date, price)\n",
    "                    remaining_capacity = self.seats_capacity - seats_already_sold\n",
    "                    actual_demand = min(demand, remaining_capacity) # Actual demand\n",
    "                    self.seats_sold[seats_sold_key] = seats_already_sold + actual_demand\n",
    "                    total_revenue += actual_demand * price # Accumulate revenue\n",
    "                    action_idx +=1\n",
    "\n",
    "        # Advance the environment\n",
    "        self.current_date += pd.Timedelta(days=1)\n",
    "        self.current_step += 1\n",
    "        done = torch.tensor(self.current_step >= self.simulation_length_days, dtype=torch.bool)\n",
    "        next_state = self._get_state(self.historical_data)\n",
    "        return next_state, torch.tensor(total_revenue, dtype=torch.float32), done, {} # Return a float reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):  # Inherit from nn.Module\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        super(DQNAgent, self).__init__()  # Call superclass constructor\n",
    "        if state_size <= 0 or action_size <= 0:\n",
    "            raise ValueError(\"state_size and action_size must be positive integers\")\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=config.MEMORY_SIZE)\n",
    "        self.gamma = config.GAMMA\n",
    "        self.epsilon = config.EPSILON\n",
    "        self.epsilon_min = config.EPSILON_MIN\n",
    "        self.epsilon_decay = config.EPSILON_DECAY\n",
    "        self.learning_rate = config.LEARNING_RATE\n",
    "        self.batch_size = config.BATCH_SIZE\n",
    "        self.device = device\n",
    "        self.model = self._build_model().to(self.device)  # Move model to device\n",
    "        self.target_model = self._build_model().to(self.device)  # Move target model to device\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)  # Define optimizer\n",
    "        self.loss_fn = nn.HuberLoss()  # Define loss function\n",
    "        self.update_target_counter = 0\n",
    "        self.target_update_frequency = config.TARGET_UPDATE_FREQ\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the neural network model for Q-value approximation.\"\"\"\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in the replay buffer.\"\"\"\n",
    "        # No changes here; we're storing the action index directly.\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "             return torch.randint(0, self.action_size, (1,), device=self.device,dtype=torch.int64) #Return action index\n",
    "        state = state.unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "             q_values = self.model(state)\n",
    "        return torch.argmax(q_values, dim=1) # Return a LongTensor, shape [1]\n",
    "\n",
    "    def replay(self, batch_size: int) -> float:\n",
    "        if batch_size > len(self.memory):\n",
    "            raise ValueError(f\"Batch size {batch_size} exceeds memory size {len(self.memory)}\")\n",
    "\n",
    "        try:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "            states, actions, rewards, next_states, dones = map(torch.stack, zip(*minibatch))\n",
    "            states, actions, rewards, next_states, dones = (\n",
    "                states.to(self.device), actions.to(self.device,dtype=torch.int64), rewards.to(self.device),\n",
    "                next_states.to(self.device), dones.to(self.device)\n",
    "            )\n",
    "\n",
    "            current_q_values = self.model(states)  # [batch_size, action_size]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q_values = self.target_model(next_states)  # [batch_size, action_size]\n",
    "            max_next_q_values = next_q_values.max(1)[0]  # [batch_size]\n",
    "\n",
    "            # Correctly compute targets using gather and scatter_\n",
    "            targets = current_q_values.clone()  # [batch_size, action_size]\n",
    "            # actions = actions.long() # Convert actions to long, needed for gather and scatter. # No longer needed\n",
    "\n",
    "            # Use gather to get the Q-values corresponding to the SELECTED ACTIONS\n",
    "            selected_q_values = current_q_values.gather(1, actions)  # [batch_size, 1]\n",
    "\n",
    "            # Compute the target values\n",
    "            target_values = rewards + self.gamma * max_next_q_values * (~dones)  # [batch_size]\n",
    "\n",
    "            # Use scatter to update ONLY the Q-values corresponding to the selected actions\n",
    "\n",
    "            targets.scatter_(1, actions, target_values.unsqueeze(1)) #Corrected Line\n",
    "\n",
    "\n",
    "            loss = self.loss_fn(current_q_values, targets)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            logging.debug(f\"Training loss: {loss.item():.4f}, Epsilon: {self.epsilon:.4f}\")\n",
    "            return loss.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during replay: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))  # Load state_dict\n",
    "        self.target_model.load_state_dict(torch.load(name)) # Load into target_model too\n",
    "        self.model.to(self.device) # Make sure its on the correct device\n",
    "        self.target_model.to(self.device) # Make sure its on the correct device\n",
    "\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)  # Save state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"\n",
    "    A class to track and calculate performance metrics during training and evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.episode_rewards = []  # List to store total reward per episode\n",
    "        self.loss_history = []     # List to store average loss per episode\n",
    "        self.demand_predictions = [] # to store the demand predictions for one episode\n",
    "        self.actual_demands = []    # To store the actual demands to compare.\n",
    "        self.price_predictions = []  # To store the prices set by agent.\n",
    "        self.actual_prices = []      # To store the prices.\n",
    "\n",
    "    def log_episode(self, total_reward, avg_loss):\n",
    "        \"\"\"Log performance for each training episode.\"\"\"\n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.loss_history.append(avg_loss)\n",
    "\n",
    "    def log_predictions(self, predicted_demand, actual_demand, predicted_price, actual_price):\n",
    "        \"\"\"\n",
    "        Logs predictions and actual values for demand and price.\n",
    "\n",
    "        Args:\n",
    "            predicted_demand: The demand predicted by the agent.\n",
    "            actual_demand: The actual demand observed in the environment.\n",
    "            predicted_price: The price set by the agent.\n",
    "            actual_price: The actual/historical price.\n",
    "        \"\"\"\n",
    "        self.demand_predictions.append(predicted_demand)\n",
    "        self.actual_demands.append(actual_demand)\n",
    "        self.price_predictions.append(predicted_price)\n",
    "        self.actual_prices.append(actual_price)\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"Compute comprehensive performance metrics.\"\"\"\n",
    "        metrics = {\n",
    "            'Mean Episode Reward': np.mean(self.episode_rewards),\n",
    "            'Reward Std Deviation': np.std(self.episode_rewards),\n",
    "            'Mean Training Loss': np.mean(self.loss_history),\n",
    "        }\n",
    "\n",
    "        # Ensure that predictions exist before calculation\n",
    "        if self.demand_predictions and self.actual_demands:\n",
    "            metrics['Demand MAPE'] = mean_absolute_percentage_error(\n",
    "                self.actual_demands, self.demand_predictions\n",
    "            )\n",
    "            metrics['Demand RMSE'] = np.sqrt(mean_squared_error(\n",
    "                self.actual_demands, self.demand_predictions\n",
    "            ))\n",
    "        else:\n",
    "            metrics['Demand MAPE'] = None  # Or np.nan, or some other placeholder\n",
    "            metrics['Demand RMSE'] = None  # Or np.nan\n",
    "\n",
    "        if self.price_predictions and self.actual_prices:\n",
    "             metrics['Price MAPE'] = mean_absolute_percentage_error(\n",
    "                  self.actual_prices, self.price_predictions\n",
    "              )\n",
    "             metrics['Price RMSE'] = np.sqrt(mean_squared_error(\n",
    "                  self.actual_prices, self.price_predictions\n",
    "              ))\n",
    "        else:\n",
    "            metrics['Price MAPE'] = None\n",
    "            metrics['Price RMSE'] = None\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def plot_training_progress(self):\n",
    "      \"\"\"Visualize training progression.\"\"\"\n",
    "      fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "      # Episode Rewards\n",
    "      axs[0, 0].plot(self.episode_rewards)\n",
    "      axs[0, 0].set_title('Episode Rewards')\n",
    "      axs[0,0].set_xlabel('Episode')\n",
    "      axs[0,0].set_ylabel('Reward')\n",
    "\n",
    "\n",
    "      # Training Loss\n",
    "      axs[0, 1].plot(self.loss_history)\n",
    "      axs[0, 1].set_title('Training Loss')\n",
    "      axs[0,1].set_xlabel('Episode')\n",
    "      axs[0,1].set_ylabel('Loss')\n",
    "\n",
    "      # Demand Prediction Error\n",
    "      if self.demand_predictions and self.actual_demands: # Check if we have the necessary data.\n",
    "        demand_errors = np.abs(np.array(self.actual_demands) - np.array(self.demand_predictions)) / (np.array(self.actual_demands) + 1e-9)  # Adding a small constant to avoid division by zero\n",
    "        axs[1, 0].plot(demand_errors)\n",
    "        axs[1, 0].set_title('Demand Prediction Error (Absolute Percentage Error)')\n",
    "        axs[1,0].set_xlabel('Step')\n",
    "        axs[1,0].set_ylabel('Error')\n",
    "      else:\n",
    "          axs[1, 0].set_title('Demand Prediction Error (Not Available)')\n",
    "\n",
    "      # Price Prediction Error\n",
    "      if self.price_predictions and self.actual_prices:\n",
    "        price_errors = np.abs(np.array(self.actual_prices) - np.array(self.price_predictions)) / (np.array(self.actual_prices) + 1e-9)\n",
    "        axs[1, 1].plot(price_errors)\n",
    "        axs[1, 1].set_title('Price Prediction Error (Absolute Percentage Error)')\n",
    "        axs[1,1].set_xlabel('Step')\n",
    "        axs[1,1].set_ylabel('Error')\n",
    "      else:\n",
    "          axs[1, 1].set_title('Price Prediction Error (Not Available)')\n",
    "\n",
    "\n",
    "      plt.tight_layout()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env, agent, n_episodes, validation_data=None, writer=None):\n",
    "    \"\"\"\n",
    "    Trains the DQN agent.\n",
    "\n",
    "    Args:\n",
    "        env: The AirlinePricingEnv environment.\n",
    "        agent: The DQNAgent.\n",
    "        n_episodes: The number of training episodes.\n",
    "        validation_data:  Optional validation data (not used in this version).\n",
    "        writer:  The SummaryWriter for TensorBoard logging.\n",
    "\n",
    "    Returns:\n",
    "        The trained agent.\n",
    "    \"\"\"\n",
    "    best_reward = float('-inf')\n",
    "    patience_counter = 0\n",
    "    metrics_tracker = PerformanceMetrics()\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state = state.to(agent.device)\n",
    "        total_reward = 0\n",
    "        losses = []\n",
    "\n",
    "        with tqdm(total=env.simulation_length_days, desc=f\"Episode {episode + 1}/{n_episodes}\", unit=\"step\") as pbar:\n",
    "            for time in range(env.simulation_length_days):\n",
    "                action = agent.act(state)  # Get the action INDEX\n",
    "                start_time = time.time()\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                step_time = time.time() - start_time\n",
    "\n",
    "                next_state = next_state.to(agent.device)\n",
    "\n",
    "                # Get the actual demand and price for logging:\n",
    "                route = env.routes[0]  # Assuming single route for now\n",
    "                airline = env.airlines[0]  # Assuming single airline for now\n",
    "                flight_date = env.current_date - pd.Timedelta(days=1)  # Get *previous* day (step advances the date)\n",
    "                # Get demand using the function in environment\n",
    "                demand = env._get_demand(route, airline, flight_date, env.prices[(route, airline)])\n",
    "                # Get price using the environment price\n",
    "                price = env.prices[(route, airline)]\n",
    "                metrics_tracker.log_predictions(demand, env.historical_data[(env.historical_data['Date'] == flight_date)\n",
    "                                                                         & (env.historical_data['Route'] == route)\n",
    "                                                                         & (env.historical_data['Airline'] == airline)]['Demand'].iloc[0], price, env.historical_data[(env.historical_data['Date'] == flight_date)\n",
    "                                                                                                                & (env.historical_data['Route'] == route)\n",
    "                                                                                                                & (env.historical_data['Airline'] == airline)]['Price'].iloc[0]  )\n",
    "\n",
    "\n",
    "\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                if len(agent.memory) > agent.batch_size:\n",
    "                    loss = agent.replay(agent.batch_size)\n",
    "                    losses.append(loss)\n",
    "                    if writer:\n",
    "                        writer.add_scalar(\"Loss/train\", loss, episode * env.simulation_length_days + time)\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward  # Accumulate the reward\n",
    "                agent.update_target_counter += 1\n",
    "\n",
    "                if agent.update_target_counter >= agent.target_update_frequency:\n",
    "                    agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "                    agent.update_target_counter = 0\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    \"Reward\": f\"{total_reward:.2f}\",\n",
    "                    \"Avg Loss\": f\"{np.mean(losses) if losses else 0:.4f}\",\n",
    "                    \"Epsilon\": f\"{agent.epsilon:.4f}\",\n",
    "                    \"Step Time\": f\"{step_time:.2f}s\"\n",
    "                })\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        if validation_data is not None:\n",
    "          val_reward = evaluate_model(env, agent, validation_data)\n",
    "          if val_reward > best_reward:\n",
    "              best_reward = val_reward\n",
    "              patience_counter = 0\n",
    "              agent.save('best_model.pth')\n",
    "          else:\n",
    "              patience_counter += 1\n",
    "          if patience_counter >= config.PATIENCE:\n",
    "            logging.info(\"Early stopping triggered!\")\n",
    "            break\n",
    "          if writer:\n",
    "            writer.add_scalar(\"Reward/validation\", val_reward, episode)\n",
    "\n",
    "\n",
    "        avg_loss = np.mean(losses) if losses else 0.0\n",
    "        metrics_tracker.log_episode(total_reward, avg_loss, total_reward)\n",
    "        logging.info(f\"Episode: {episode + 1}/{n_episodes}, Total Reward: {total_reward}, Avg Loss: {avg_loss}, Epsilon: {agent.epsilon:.3f}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(\"Reward/train\", total_reward, episode)\n",
    "            writer.add_scalar(\"Epsilon\", agent.epsilon, episode)\n",
    "\n",
    "    # Calculate and plot metrics after training is complete\n",
    "    performance_metrics = metrics_tracker.calculate_metrics()\n",
    "    print(performance_metrics)\n",
    "    metrics_tracker.plot_training_progress()\n",
    "\n",
    "    return agent\n",
    "\n",
    "def evaluate_model(env, agent, validation_data):\n",
    "    \"\"\"Evaluate the trained agent.\"\"\"\n",
    "    total_reward = 0\n",
    "    state = env.reset(validation_data)\n",
    "    state = state.to(agent.device)  # Move state to device\n",
    "    for time in range(env.simulation_length_days):\n",
    "        action = agent.act(state, training=False)  # Get action (no exploration)\n",
    "        next_state, reward, done, _ = env.step(action)  # Step the environment\n",
    "        next_state = next_state.to(agent.device)\n",
    "        total_reward += reward  # Accumulate reward\n",
    "        state = next_state # update the state\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_holiday_data(historical_data):\n",
    "    \"\"\"Creates a sample holiday dataset.\"\"\"\n",
    "\n",
    "    # Get unique dates and locations\n",
    "    dates = historical_data['Date'].unique()\n",
    "    # Expecting Route to be in format 'Origin-Destination'\n",
    "    locations = np.unique([route.split('-')[0] for route in historical_data['Route'].unique()])\n",
    "\n",
    "    holiday_data = []\n",
    "    for location in locations:\n",
    "        # Create 2-4 random holidays per location\n",
    "        num_holidays = random.randint(2, 4)  \n",
    "        for _ in range(num_holidays):\n",
    "            # Randomly select a date\n",
    "            random_date = pd.to_datetime(random.choice(dates))\n",
    "\n",
    "            # Create a holiday name (you can customize this)\n",
    "            holiday_name = f\"{location} Holiday {_+1}\"\n",
    "\n",
    "            holiday_data.append({'Date': random_date, 'Location': location, 'HolidayName': holiday_name})\n",
    "\n",
    "    return pd.DataFrame(holiday_data)\n",
    "\n",
    "def setup_gpu(disable_gpu=False, memory_limit=None):\n",
    "    \"\"\"Configures GPU usage for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        disable_gpu (bool): Disable GPU use.\n",
    "        memory_limit (int):  (Not directly applicable to PyTorch in the same way as TF).\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The device to use (CPU or CUDA).\n",
    "    \"\"\"\n",
    "    if disable_gpu:\n",
    "        logging.info(\"GPU disabled by user request. Running on CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logging.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return device\n",
    "    else:\n",
    "        logging.info(\"No GPU found, using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def add_gpu_arguments(parser):\n",
    "    \"\"\"Adds GPU-related arguments to an ArgumentParser.\"\"\"\n",
    "    group = parser.add_argument_group('GPU Settings')\n",
    "    group.add_argument('--disable_gpu', action='store_true',\n",
    "                       help='Disable GPU usage, even if available.')\n",
    "    group.add_argument('--memory_limit', type=int, default=None,\n",
    "                       help='(Not directly used in PyTorch) Limit GPU memory usage (in MB).')  # Kept for compatibility, but won't be used\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main(n_episodes, batch_size, learning_rate, gamma, epsilon_decay, target_update_freq, device):\n",
    "    \"\"\"Main function to run the airline pricing simulation.\"\"\"\n",
    "\n",
    "    logging.info(\"Loading data from CSV...\")\n",
    "    # Load dataframes.  \n",
    "    historical_data = pd.read_csv('flight_data_BOM_BLR.csv')\n",
    "    historical_data2 = pd.read_csv('flight_data_DEL_BLR.csv')\n",
    "    historical_data3 = pd.read_csv('flight_data_DEL_CCU.csv')\n",
    "\n",
    "    historical_data = pd.concat([historical_data, historical_data2, historical_data3], ignore_index=True)\n",
    "\n",
    "    # Load other necessary data\n",
    "    climate_data = pd.read_csv('location_data.csv')\n",
    "\n",
    "    # Since holiday data is not provided in the uploaded CSV, use _create_holiday_data\n",
    "    holiday_data = create_holiday_data(historical_data)\n",
    "    # Create a constant fuel price DataFrame, as you don't have a separate file\n",
    "    fuel_prices = pd.DataFrame({'Date': historical_data['Date'].unique(), 'FuelPrice': 2.5})  # Example constant fuel price\n",
    "    fuel_prices['Date'] = pd.to_datetime(fuel_prices['Date'])\n",
    "\n",
    "    # Initialize data processor and fit it with the data\n",
    "    logging.info(\"Initializing data processor...\")\n",
    "    data_processor = DataProcessor()\n",
    "    data_processor.fit(historical_data, fuel_prices, climate_data, holiday_data)\n",
    "\n",
    "\n",
    "    logging.info(\"Creating environment...\")\n",
    "    env = AirlinePricingEnv(historical_data, fuel_prices, climate_data, holiday_data, data_processor)\n",
    "\n",
    "    state_size = env.reset().shape[0]  # Get state size directly from tensor\n",
    "    action_size = len(env.routes) * len(env.airlines) * config.ACTION_DAYS_AHEAD #As the action is one value now.\n",
    "\n",
    "\n",
    "    logging.info(\"Creating DQN agent...\")\n",
    "    agent = DQNAgent(state_size, action_size, device)  # Pass device to agent\n",
    "    agent.gamma = gamma\n",
    "    agent.learning_rate = learning_rate\n",
    "    agent.epsilon_decay = epsilon_decay\n",
    "    agent.target_update_frequency = target_update_freq\n",
    "    agent.batch_size = batch_size\n",
    "\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    logging.info(\"Starting training...\")\n",
    "    trained_agent, performance_metrics = train_model(env, agent, n_episodes,  writer)\n",
    "\n",
    "    logging.info(\"Evaluating final model...\")\n",
    "    final_reward = evaluate_model(env, trained_agent, historical_data)\n",
    "    logging.info(f\"Final reward: {final_reward}\")\n",
    "\n",
    "    logging.info(\"Saving model...\")\n",
    "    trained_agent.save('final_model.pth')  # Save as .pth\n",
    "    writer.close()\n",
    "    logging.info(\"Training complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train DQN agent for airline pricing.\")\n",
    "    parser = add_gpu_arguments(parser)\n",
    "    # Removed file path arguments\n",
    "    parser.add_argument('--n_episodes', type=int, default=config.N_EPISODES, help='Training episodes.')\n",
    "    parser.add_argument('--batch_size', type=int, default=config.BATCH_SIZE, help='Training batch size.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=config.LEARNING_RATE, help='Optimizer learning rate.')\n",
    "    parser.add_argument('--gamma', type=float, default=config.GAMMA, help='Discount factor.')\n",
    "    parser.add_argument('--epsilon_decay', type=float, default=config.EPSILON_DECAY, help='Epsilon decay rate.')\n",
    "    parser.add_argument('--target_update_freq', type=int, default=config.TARGET_UPDATE_FREQ, help='Target network update frequency.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = setup_gpu(args.disable_gpu) # Removed args.memory_limit\n",
    "    # Call main and pass config and device.\n",
    "    main(args.n_episodes,args.batch_size,args.learning_rate,args.gamma,args.epsilon_decay,args.target_update_freq, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
